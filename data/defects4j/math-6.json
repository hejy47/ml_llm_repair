{"org.apache.commons.math3.optim.BaseOptimizer.BaseOptimizer": {"buggy_content": "protected BaseOptimizer(ConvergenceChecker<PAIR> checker) {\n    this.checker = checker;\n    evaluations = new Incrementor(0, new MaxEvalCallback());\n    iterations = new Incrementor(0, new MaxIterCallback());\n}", "method_range": "47-52", "fault_locations": "51"}, "org.apache.commons.math3.optim.nonlinear.scalar.gradient.NonLinearConjugateGradientOptimizer.doOptimize": {"buggy_content": "@Override\nprotected PointValuePair doOptimize() {\n    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n    final double[] point = getStartPoint();\n    final GoalType goal = getGoalType();\n    final int n = point.length;\n    double[] r = computeObjectiveGradient(point);\n    if (goal == GoalType.MINIMIZE) {\n        for (int i = 0; i < n; i++) {\n            r[i] = -r[i];\n        }\n    }\n    // Initial search direction.\n    double[] steepestDescent = preconditioner.precondition(point, r);\n    double[] searchDirection = steepestDescent.clone();\n    double delta = 0;\n    for (int i = 0; i < n; ++i) {\n        delta += r[i] * searchDirection[i];\n    }\n    PointValuePair current = null;\n    int iter = 0;\n    int maxEval = getMaxEvaluations();\n    while (true) {\n        ++iter;\n        final double objective = computeObjectiveValue(point);\n        PointValuePair previous = current;\n        current = new PointValuePair(point, objective);\n        if (previous != null) {\n            if (checker.converged(iter, previous, current)) {\n                // We have found an optimum.\n                return current;\n            }\n        }\n        // Find the optimal step in the search direction.\n        final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n        final double uB = findUpperBound(lsf, 0, initialStep);\n        // XXX Last parameters is set to a value close to zero in order to\n        // work around the divergence problem in the \"testCircleFitting\"\n        // unit test (see MATH-439).\n        final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n        // Subtract used up evaluations.\n        maxEval -= solver.getEvaluations();\n        // Validate new point.\n        for (int i = 0; i < point.length; ++i) {\n            point[i] += step * searchDirection[i];\n        }\n        r = computeObjectiveGradient(point);\n        if (goal == GoalType.MINIMIZE) {\n            for (int i = 0; i < n; ++i) {\n                r[i] = -r[i];\n            }\n        }\n        // Compute beta.\n        final double deltaOld = delta;\n        final double[] newSteepestDescent = preconditioner.precondition(point, r);\n        delta = 0;\n        for (int i = 0; i < n; ++i) {\n            delta += r[i] * newSteepestDescent[i];\n        }\n        final double beta;\n        switch(updateFormula) {\n            case FLETCHER_REEVES:\n                beta = delta / deltaOld;\n                break;\n            case POLAK_RIBIERE:\n                double deltaMid = 0;\n                for (int i = 0; i < r.length; ++i) {\n                    deltaMid += r[i] * steepestDescent[i];\n                }\n                beta = (delta - deltaMid) / deltaOld;\n                break;\n            default:\n                // Should never happen.\n                throw new MathInternalError();\n        }\n        steepestDescent = newSteepestDescent;\n        // Compute conjugate search direction.\n        if (iter % n == 0 || beta < 0) {\n            // Break conjugation: reset search direction.\n            searchDirection = steepestDescent.clone();\n        } else {\n            // Compute new conjugate search direction.\n            for (int i = 0; i < n; ++i) {\n                searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n            }\n        }\n    }\n}", "method_range": "191-288", "fault_locations": "214,215,216,217,220,221,223,273,274,277"}, "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizer.doOptimize": {"buggy_content": "@Override\nprotected PointValuePair doOptimize() {\n    // -------------------- Initialization --------------------------------\n    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n    final FitnessFunction fitfun = new FitnessFunction();\n    final double[] guess = getStartPoint();\n    // number of objective variables/problem dimension\n    dimension = guess.length;\n    initializeCMA(guess);\n    iterations = 0;\n    double bestValue = fitfun.value(guess);\n    push(fitnessHistory, bestValue);\n    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n    PointValuePair lastResult = null;\n    // -------------------- Generation Loop --------------------------------\n    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n        // Generate and evaluate lambda offspring\n        final RealMatrix arz = randn1(dimension, lambda);\n        final RealMatrix arx = zeros(dimension, lambda);\n        final double[] fitness = new double[lambda];\n        // generate random offspring\n        for (int k = 0; k < lambda; k++) {\n            RealMatrix arxk = null;\n            for (int i = 0; i < checkFeasableCount + 1; i++) {\n                if (diagonalOnly <= 0) {\n                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n                    sigma));\n                } else {\n                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                }\n                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n                    break;\n                }\n                // regenerate random arguments for row\n                arz.setColumn(k, randn(dimension));\n            }\n            copyColumn(arxk, 0, arx, k);\n            try {\n                // compute fitness\n                fitness[k] = fitfun.value(arx.getColumn(k));\n            } catch (TooManyEvaluationsException e) {\n                break generationLoop;\n            }\n        }\n        // Sort by fitness and compute weighted mean into xmean\n        final int[] arindex = sortedIndices(fitness);\n        // Calculate new xmean, this is selection and recombination\n        // for speed up of Eq. (2) and (3)\n        final RealMatrix xold = xmean;\n        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n        xmean = bestArx.multiply(weights);\n        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n        final RealMatrix zmean = bestArz.multiply(weights);\n        final boolean hsig = updateEvolutionPaths(zmean, xold);\n        if (diagonalOnly <= 0) {\n            updateCovariance(hsig, bestArx, arz, arindex, xold);\n        } else {\n            updateCovarianceDiagonalOnly(hsig, bestArz);\n        }\n        // Adapt step size sigma - Eq. (5)\n        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n        final double bestFitness = fitness[arindex[0]];\n        final double worstFitness = fitness[arindex[arindex.length - 1]];\n        if (bestValue > bestFitness) {\n            bestValue = bestFitness;\n            lastResult = optimum;\n            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n            if (getConvergenceChecker() != null && lastResult != null) {\n                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                    break generationLoop;\n                }\n            }\n        }\n        // handle termination criteria\n        // Break, if fitness is good enough\n        if (stopFitness != 0) {\n            // only if stopFitness is defined\n            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                break generationLoop;\n            }\n        }\n        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n        final double[] pcCol = pc.getColumn(0);\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                break;\n            }\n            if (i >= dimension - 1) {\n                break generationLoop;\n            }\n        }\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                break generationLoop;\n            }\n        }\n        final double historyBest = min(fitnessHistory);\n        final double historyWorst = max(fitnessHistory);\n        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n            break generationLoop;\n        }\n        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n            break generationLoop;\n        }\n        // condition number of the covariance matrix exceeds 1e14\n        if (max(diagD) / min(diagD) > 1e7) {\n            break generationLoop;\n        }\n        // user defined termination\n        if (getConvergenceChecker() != null) {\n            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n                break generationLoop;\n            }\n            lastResult = current;\n        }\n        // Adjust step size in case of equal function values (flat fitness)\n        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        // store best in history\n        push(fitnessHistory, bestFitness);\n        fitfun.setValueRange(worstFitness - bestFitness);\n        if (generateStatistics) {\n            statisticsSigmaHistory.add(sigma);\n            statisticsFitnessHistory.add(bestFitness);\n            statisticsMeanHistory.add(xmean.transpose());\n            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n        }\n    }\n    return optimum;\n}", "method_range": "367-515", "fault_locations": "387,388"}, "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.PowellOptimizer.doOptimize": {"buggy_content": "@Override\nprotected PointValuePair doOptimize() {\n    checkParameters();\n    final GoalType goal = getGoalType();\n    final double[] guess = getStartPoint();\n    final int n = guess.length;\n    final double[][] direc = new double[n][n];\n    for (int i = 0; i < n; i++) {\n        direc[i][i] = 1;\n    }\n    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n    double[] x = guess;\n    double fVal = computeObjectiveValue(x);\n    double[] x1 = x.clone();\n    int iter = 0;\n    while (true) {\n        ++iter;\n        double fX = fVal;\n        double fX2 = 0;\n        double delta = 0;\n        int bigInd = 0;\n        double alphaMin = 0;\n        for (int i = 0; i < n; i++) {\n            final double[] d = MathArrays.copyOf(direc[i]);\n            fX2 = fVal;\n            final UnivariatePointValuePair optimum = line.search(x, d);\n            fVal = optimum.getValue();\n            alphaMin = optimum.getPoint();\n            final double[][] result = newPointAndDirection(x, d, alphaMin);\n            x = result[0];\n            if ((fX2 - fVal) > delta) {\n                delta = fX2 - fVal;\n                bigInd = i;\n            }\n        }\n        // Default convergence check.\n        boolean stop = 2 * (fX - fVal) <= (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) + absoluteThreshold);\n        final PointValuePair previous = new PointValuePair(x1, fX);\n        final PointValuePair current = new PointValuePair(x, fVal);\n        if (!stop) {\n            // User-defined stopping criteria.\n            if (checker != null) {\n                stop = checker.converged(iter, previous, current);\n            }\n        }\n        if (stop) {\n            if (goal == GoalType.MINIMIZE) {\n                return (fVal < fX) ? current : previous;\n            } else {\n                return (fVal > fX) ? current : previous;\n            }\n        }\n        final double[] d = new double[n];\n        final double[] x2 = new double[n];\n        for (int i = 0; i < n; i++) {\n            d[i] = x[i] - x1[i];\n            x2[i] = 2 * x[i] - x1[i];\n        }\n        x1 = x.clone();\n        fX2 = computeObjectiveValue(x2);\n        if (fX > fX2) {\n            double t = 2 * (fX + fX2 - 2 * fVal);\n            double temp = fX - fVal - delta;\n            t *= temp * temp;\n            temp = fX - fX2;\n            t -= delta * temp * temp;\n            if (t < 0.0) {\n                final UnivariatePointValuePair optimum = line.search(x, d);\n                fVal = optimum.getValue();\n                alphaMin = optimum.getPoint();\n                final double[][] result = newPointAndDirection(x, d, alphaMin);\n                x = result[0];\n                final int lastInd = n - 1;\n                direc[bigInd] = direc[lastInd];\n                direc[lastInd] = result[1];\n            }\n        }\n    }\n}", "method_range": "172-268", "fault_locations": "191,192,193,224,225,227"}, "org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizer.doOptimize": {"buggy_content": "@Override\nprotected PointValuePair doOptimize() {\n    checkParameters();\n    // Indirect call to \"computeObjectiveValue\" in order to update the\n    // evaluations counter.\n    final MultivariateFunction evalFunc = new MultivariateFunction() {\n\n        public double value(double[] point) {\n            return computeObjectiveValue(point);\n        }\n    };\n    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n    final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() {\n\n        public int compare(final PointValuePair o1, final PointValuePair o2) {\n            final double v1 = o1.getValue();\n            final double v2 = o2.getValue();\n            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n        }\n    };\n    // Initialize search.\n    simplex.build(getStartPoint());\n    simplex.evaluate(evalFunc, comparator);\n    PointValuePair[] previous = null;\n    int iteration = 0;\n    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n    while (true) {\n        if (iteration > 0) {\n            boolean converged = true;\n            for (int i = 0; i < simplex.getSize(); i++) {\n                PointValuePair prev = previous[i];\n                converged = converged && checker.converged(iteration, prev, simplex.getPoint(i));\n            }\n            if (converged) {\n                // We have found an optimum.\n                return simplex.getPoint(0);\n            }\n        }\n        // We still need to search.\n        previous = simplex.getPoints();\n        simplex.iterate(evalFunc, comparator);\n        ++iteration;\n    }\n}", "method_range": "126-177", "fault_locations": "158,173,174,175"}, "org.apache.commons.math3.optim.nonlinear.vector.jacobian.GaussNewtonOptimizer.doOptimize": {"buggy_content": "@Override\npublic PointVectorValuePair doOptimize() {\n    checkParameters();\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n    // Computation will be useless without a checker (see \"for-loop\").\n    if (checker == null) {\n        throw new NullArgumentException();\n    }\n    final double[] targetValues = getTarget();\n    // Number of observed data.\n    final int nR = targetValues.length;\n    final RealMatrix weightMatrix = getWeight();\n    // Diagonal of the weight matrix.\n    final double[] residualsWeights = new double[nR];\n    for (int i = 0; i < nR; i++) {\n        residualsWeights[i] = weightMatrix.getEntry(i, i);\n    }\n    final double[] currentPoint = getStartPoint();\n    final int nC = currentPoint.length;\n    // iterate until convergence is reached\n    PointVectorValuePair current = null;\n    int iter = 0;\n    for (boolean converged = false; !converged; ) {\n        ++iter;\n        // evaluate the objective function and its jacobian\n        PointVectorValuePair previous = current;\n        // Value of the objective function at \"currentPoint\".\n        final double[] currentObjective = computeObjectiveValue(currentPoint);\n        final double[] currentResiduals = computeResiduals(currentObjective);\n        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n        current = new PointVectorValuePair(currentPoint, currentObjective);\n        // build the linear problem\n        final double[] b = new double[nC];\n        final double[][] a = new double[nC][nC];\n        for (int i = 0; i < nR; ++i) {\n            final double[] grad = weightedJacobian.getRow(i);\n            final double weight = residualsWeights[i];\n            final double residual = currentResiduals[i];\n            // compute the normal equation\n            final double wr = weight * residual;\n            for (int j = 0; j < nC; ++j) {\n                b[j] += wr * grad[j];\n            }\n            // build the contribution matrix for measurement i\n            for (int k = 0; k < nC; ++k) {\n                double[] ak = a[k];\n                double wgk = weight * grad[k];\n                for (int l = 0; l < nC; ++l) {\n                    ak[l] += wgk * grad[l];\n                }\n            }\n        }\n        try {\n            // solve the linearized least squares problem\n            RealMatrix mA = new BlockRealMatrix(a);\n            DecompositionSolver solver = useLU ? new LUDecomposition(mA).getSolver() : new QRDecomposition(mA).getSolver();\n            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n            // update the estimated parameters\n            for (int i = 0; i < nC; ++i) {\n                currentPoint[i] += dX[i];\n            }\n        } catch (SingularMatrixException e) {\n            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n        }\n        // Check convergence.\n        if (previous != null) {\n            converged = checker.converged(iter, previous, current);\n            if (converged) {\n                setCost(computeCost(currentResiduals));\n                return current;\n            }\n        }\n    }\n    // Must never happen.\n    throw new MathInternalError();\n}", "method_range": "79-169", "fault_locations": "106,107,108,157,158,160"}, "org.apache.commons.math3.optim.nonlinear.vector.jacobian.LevenbergMarquardtOptimizer.doOptimize": {"buggy_content": "@Override\nprotected PointVectorValuePair doOptimize() {\n    checkParameters();\n    // Number of observed data.\n    final int nR = getTarget().length;\n    final double[] currentPoint = getStartPoint();\n    // Number of parameters.\n    final int nC = currentPoint.length;\n    // arrays shared with the other private methods\n    solvedCols = FastMath.min(nR, nC);\n    diagR = new double[nC];\n    jacNorm = new double[nC];\n    beta = new double[nC];\n    permutation = new int[nC];\n    lmDir = new double[nC];\n    // local point\n    double delta = 0;\n    double xNorm = 0;\n    double[] diag = new double[nC];\n    double[] oldX = new double[nC];\n    double[] oldRes = new double[nR];\n    double[] oldObj = new double[nR];\n    double[] qtf = new double[nR];\n    double[] work1 = new double[nC];\n    double[] work2 = new double[nC];\n    double[] work3 = new double[nC];\n    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n    // Evaluate the function at the starting point and calculate its norm.\n    double[] currentObjective = computeObjectiveValue(currentPoint);\n    double[] currentResiduals = computeResiduals(currentObjective);\n    PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n    double currentCost = computeCost(currentResiduals);\n    // Outer loop.\n    lmPar = 0;\n    boolean firstIteration = true;\n    int iter = 0;\n    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n    while (true) {\n        ++iter;\n        final PointVectorValuePair previous = current;\n        // QR decomposition of the jacobian matrix\n        qrDecomposition(computeWeightedJacobian(currentPoint));\n        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n        for (int i = 0; i < nR; i++) {\n            qtf[i] = weightedResidual[i];\n        }\n        // compute Qt.res\n        qTy(qtf);\n        // now we don't need Q anymore,\n        // so let jacobian contain the R matrix with its diagonal elements\n        for (int k = 0; k < solvedCols; ++k) {\n            int pk = permutation[k];\n            weightedJacobian[k][pk] = diagR[pk];\n        }\n        if (firstIteration) {\n            // scale the point according to the norms of the columns\n            // of the initial jacobian\n            xNorm = 0;\n            for (int k = 0; k < nC; ++k) {\n                double dk = jacNorm[k];\n                if (dk == 0) {\n                    dk = 1.0;\n                }\n                double xk = dk * currentPoint[k];\n                xNorm += xk * xk;\n                diag[k] = dk;\n            }\n            xNorm = FastMath.sqrt(xNorm);\n            // initialize the step bound delta\n            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n        }\n        // check orthogonality between function vector and jacobian columns\n        double maxCosine = 0;\n        if (currentCost != 0) {\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double s = jacNorm[pj];\n                if (s != 0) {\n                    double sum = 0;\n                    for (int i = 0; i <= j; ++i) {\n                        sum += weightedJacobian[i][pj] * qtf[i];\n                    }\n                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n                }\n            }\n        }\n        if (maxCosine <= orthoTolerance) {\n            // Convergence has been reached.\n            setCost(currentCost);\n            return current;\n        }\n        // rescale if necessary\n        for (int j = 0; j < nC; ++j) {\n            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n        }\n        // Inner loop.\n        for (double ratio = 0; ratio < 1.0e-4; ) {\n            // save the state\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                oldX[pj] = currentPoint[pj];\n            }\n            final double previousCost = currentCost;\n            double[] tmpVec = weightedResidual;\n            weightedResidual = oldRes;\n            oldRes = tmpVec;\n            tmpVec = currentObjective;\n            currentObjective = oldObj;\n            oldObj = tmpVec;\n            // determine the Levenberg-Marquardt parameter\n            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n            // compute the new point and the norm of the evolution direction\n            double lmNorm = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                lmDir[pj] = -lmDir[pj];\n                currentPoint[pj] = oldX[pj] + lmDir[pj];\n                double s = diag[pj] * lmDir[pj];\n                lmNorm += s * s;\n            }\n            lmNorm = FastMath.sqrt(lmNorm);\n            // on the first iteration, adjust the initial step bound.\n            if (firstIteration) {\n                delta = FastMath.min(delta, lmNorm);\n            }\n            // Evaluate the function at x + p and calculate its norm.\n            currentObjective = computeObjectiveValue(currentPoint);\n            currentResiduals = computeResiduals(currentObjective);\n            current = new PointVectorValuePair(currentPoint, currentObjective);\n            currentCost = computeCost(currentResiduals);\n            // compute the scaled actual reduction\n            double actRed = -1.0;\n            if (0.1 * currentCost < previousCost) {\n                double r = currentCost / previousCost;\n                actRed = 1.0 - r * r;\n            }\n            // compute the scaled predicted reduction\n            // and the scaled directional derivative\n            for (int j = 0; j < solvedCols; ++j) {\n                int pj = permutation[j];\n                double dirJ = lmDir[pj];\n                work1[j] = 0;\n                for (int i = 0; i <= j; ++i) {\n                    work1[i] += weightedJacobian[i][pj] * dirJ;\n                }\n            }\n            double coeff1 = 0;\n            for (int j = 0; j < solvedCols; ++j) {\n                coeff1 += work1[j] * work1[j];\n            }\n            double pc2 = previousCost * previousCost;\n            coeff1 = coeff1 / pc2;\n            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n            double preRed = coeff1 + 2 * coeff2;\n            double dirDer = -(coeff1 + coeff2);\n            // ratio of the actual to the predicted reduction\n            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n            // update the step bound\n            if (ratio <= 0.25) {\n                double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n                if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n                    tmp = 0.1;\n                }\n                delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n                lmPar /= tmp;\n            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n                delta = 2 * lmNorm;\n                lmPar *= 0.5;\n            }\n            // test for successful iteration.\n            if (ratio >= 1.0e-4) {\n                // successful iteration, update the norm\n                firstIteration = false;\n                xNorm = 0;\n                for (int k = 0; k < nC; ++k) {\n                    double xK = diag[k] * currentPoint[k];\n                    xNorm += xK * xK;\n                }\n                xNorm = FastMath.sqrt(xNorm);\n                // tests for convergence.\n                if (checker != null) {\n                    // we use the vectorial convergence checker\n                    if (checker.converged(iter, previous, current)) {\n                        setCost(currentCost);\n                        return current;\n                    }\n                }\n            } else {\n                // failed iteration, reset the previous values\n                currentCost = previousCost;\n                for (int j = 0; j < solvedCols; ++j) {\n                    int pj = permutation[j];\n                    currentPoint[pj] = oldX[pj];\n                }\n                tmpVec = weightedResidual;\n                weightedResidual = oldRes;\n                oldRes = tmpVec;\n                tmpVec = currentObjective;\n                currentObjective = oldObj;\n                oldObj = tmpVec;\n                // Reset \"current\" to previous values.\n                current = new PointVectorValuePair(currentPoint, currentObjective);\n            }\n            // Default convergence criteria.\n            if ((FastMath.abs(actRed) <= costRelativeTolerance && preRed <= costRelativeTolerance && ratio <= 2.0) || delta <= parRelativeTolerance * xNorm) {\n                setCost(currentCost);\n                return current;\n            }\n            // tests for termination and stringent tolerances\n            // (2.2204e-16 is the machine epsilon for IEEE754)\n            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE, costRelativeTolerance);\n            } else if (delta <= 2.2204e-16 * xNorm) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE, parRelativeTolerance);\n            } else if (maxCosine <= 2.2204e-16) {\n                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE, orthoTolerance);\n            }\n        }\n    }\n}", "method_range": "283-534", "fault_locations": "322,323,324,325,486,487,489"}}