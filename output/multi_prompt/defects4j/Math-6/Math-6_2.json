[{"output": "The provided function is actually a part of a larger program. As such, I cannot identify the bug in the function as it depends on the rest of the program.", "diff": "", "finish_reason": "stop", "valid": false, "num": 11}, {"output": "// Imports needed for fix\nimport java.util.Arrays;\nimport org.apache.commons.math3.exception.TooManyEvaluationsException;\nimport org.apache.commons.math3.optim.PointValuePair;\n\n// Fixed Function\n@Override\nprotected PointValuePair doOptimize() {\n    // -------------------- Initialization --------------------------------\n    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n    final FitnessFunction fitfun = new FitnessFunction();\n    final double[] guess = getStartPoint();\n    // number of objective variables/problem dimension\n    dimension = guess.length;\n    initializeCMA(guess);\n    iterations = 0;\n    double bestValue = fitfun.value(guess);\n    push(fitnessHistory, bestValue);\n    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n    PointValuePair lastResult = null;\n    // -------------------- Generation Loop --------------------------------\n    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n        // Generate and evaluate lambda offspring\n        final RealMatrix arz = randn(dimension, lambda);\n        final RealMatrix arx = zeros(dimension, lambda);\n        final double[] fitness = new double[lambda];\n        // generate random offspring\n        for (int k = 0; k < lambda; k++) {\n            RealMatrix arxk = null;\n            for (int i = 0; i < checkFeasableCount + 1; i++) {\n                if (diagonalOnly <= 0) {\n                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                } else {\n                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                }\n                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n                    break;\n                }\n                arz.setColumn(k, randn(dimension));\n                arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                if (fitfun.isFeasible(arxk.getColumn(0))) {\n                    break;\n                }\n            }\n            copyColumn(arxk, 0, arx, k);\n            try {\n                // compute fitness\n                fitness[k] = fitfun.value(arx.getColumn(k));\n            } catch (TooManyEvaluationsException e) {\n                break generationLoop;\n            }\n        }\n        // Sort by fitness and compute weighted mean into xmean\n        final int[] arindex = sortedIndices(fitness);\n        // Calculate new xmean, this is selection and recombination\n        final RealMatrix xold = xmean;\n        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n        xmean = bestArx.multiply(weights);\n        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n        final RealMatrix zmean = bestArz.multiply(weights);\n        final boolean hsig = updateEvolutionPaths(zmean, xold);\n        if (diagonalOnly <= 0) {\n            updateCovariance(hsig, bestArx, arz, arindex, xold);\n        } else {\n            updateCovarianceDiagonalOnly(hsig, bestArz);\n        }\n        // Adapt step size sigma - Eq. (5)\n        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n        final double bestFitness = fitness[arindex[0]];\n        final double worstFitness = fitness[arindex[arindex.length - 1]];\n        if (bestValue > bestFitness) {\n            bestValue = bestFitness;\n            lastResult = optimum;\n            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n            if (getConvergenceChecker() != null && lastResult != null) {\n                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                    break generationLoop;\n                }\n            }\n        }\n        // handle termination criteria\n        // Break, if fitness is good enough\n        if (stopFitness != 0) {\n            // only if stopFitness is defined\n            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                break generationLoop;\n            }\n        }\n        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n        final double[] pcCol = pc.getColumn(0);\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                break;\n            }\n            if (i >= dimension - 1) {\n                break generationLoop;\n            }\n        }\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                break generationLoop;\n            }\n        }\n        final double historyBest = min(fitnessHistory);\n        final double historyWorst = max(fitnessHistory);\n        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n            break generationLoop;\n        }\n        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n            break generationLoop;\n        }\n        // condition number of the covariance matrix exceeds 1e14\n        if (max(diagD) / min(diagD) > 1e7) {\n            break generationLoop;\n        }\n        // user defined termination\n        if (getConvergenceChecker() != null) {\n            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n                break generationLoop;\n            }\n            lastResult = current;\n        }\n        // Adjust step size in case of equal function values (flat fitness)\n        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        // store best in history\n        push(fitnessHistory, bestFitness);\n        fitfun.setValueRange(worstFitness - bestFitness);\n        if (generateStatistics) {\n            statisticsSigmaHistory.add(sigma);\n            statisticsFitnessHistory.add(bestFitness);\n            statisticsMeanHistory.add(xmean.transpose());\n            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n        }\n    }\n    return optimum;\n}", "diff": "--- org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java@doOptimize\n@@ -1,88 +1,136 @@\n @Override\n protected PointValuePair doOptimize() {\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    final double[] point = getStartPoint();\n-    final GoalType goal = getGoalType();\n-    final int n = point.length;\n-    double[] r = computeObjectiveGradient(point);\n-    if (goal == GoalType.MINIMIZE) {\n-        for (int i = 0; i < n; i++) {\n-            r[i] = -r[i];\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                arz.setColumn(k, randn(dimension));\n+                arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                if (fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n+            }\n+        }\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n+            }\n+        }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n+            }\n+        }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n         }\n     }\n-    // Initial search direction.\n-    double[] steepestDescent = preconditioner.precondition(point, r);\n-    double[] searchDirection = steepestDescent.clone();\n-    double delta = 0;\n-    for (int i = 0; i < n; ++i) {\n-        delta += r[i] * searchDirection[i];\n-    }\n-    PointValuePair current = null;\n-    int iter = 0;\n-    int maxEval = getMaxEvaluations();\n-    while (true) {\n-        ++iter;\n-        final double objective = computeObjectiveValue(point);\n-        PointValuePair previous = current;\n-        current = new PointValuePair(point, objective);\n-        if (previous != null) {\n-            if (checker.converged(iter, previous, current)) {\n-                // We have found an optimum.\n-                return current;\n-            }\n-        }\n-        // Find the optimal step in the search direction.\n-        final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n-        final double uB = findUpperBound(lsf, 0, initialStep);\n-        // XXX Last parameters is set to a value close to zero in order to\n-        // work around the divergence problem in the \"testCircleFitting\"\n-        // unit test (see MATH-439).\n-        final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n-        // Subtract used up evaluations.\n-        maxEval -= solver.getEvaluations();\n-        // Validate new point.\n-        for (int i = 0; i < point.length; ++i) {\n-            point[i] += step * searchDirection[i];\n-        }\n-        r = computeObjectiveGradient(point);\n-        if (goal == GoalType.MINIMIZE) {\n-            for (int i = 0; i < n; ++i) {\n-                r[i] = -r[i];\n-            }\n-        }\n-        // Compute beta.\n-        final double deltaOld = delta;\n-        final double[] newSteepestDescent = preconditioner.precondition(point, r);\n-        delta = 0;\n-        for (int i = 0; i < n; ++i) {\n-            delta += r[i] * newSteepestDescent[i];\n-        }\n-        final double beta;\n-        switch(updateFormula) {\n-            case FLETCHER_REEVES:\n-                beta = delta / deltaOld;\n-                break;\n-            case POLAK_RIBIERE:\n-                double deltaMid = 0;\n-                for (int i = 0; i < r.length; ++i) {\n-                    deltaMid += r[i] * steepestDescent[i];\n-                }\n-                beta = (delta - deltaMid) / deltaOld;\n-                break;\n-            default:\n-                // Should never happen.\n-                throw new MathInternalError();\n-        }\n-        steepestDescent = newSteepestDescent;\n-        // Compute conjugate search direction.\n-        if (iter % n == 0 || beta < 0) {\n-            // Break conjugation: reset search direction.\n-            searchDirection = steepestDescent.clone();\n-        } else {\n-            // Compute new conjugate search direction.\n-            for (int i = 0; i < n; ++i) {\n-                searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n-            }\n-        }\n-    }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java@doOptimize\n@@ -15,7 +15,7 @@\n     // -------------------- Generation Loop --------------------------------\n     generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n         // Generate and evaluate lambda offspring\n-        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arz = randn(dimension, lambda);\n         final RealMatrix arx = zeros(dimension, lambda);\n         final double[] fitness = new double[lambda];\n         // generate random offspring\n@@ -23,16 +23,18 @@\n             RealMatrix arxk = null;\n             for (int i = 0; i < checkFeasableCount + 1; i++) {\n                 if (diagonalOnly <= 0) {\n-                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n-                    sigma));\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                 } else {\n                     arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                 }\n                 if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n                     break;\n                 }\n-                // regenerate random arguments for row\n                 arz.setColumn(k, randn(dimension));\n+                arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                if (fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n             }\n             copyColumn(arxk, 0, arx, k);\n             try {\n@@ -45,11 +47,10 @@\n         // Sort by fitness and compute weighted mean into xmean\n         final int[] arindex = sortedIndices(fitness);\n         // Calculate new xmean, this is selection and recombination\n-        // for speed up of Eq. (2) and (3)\n         final RealMatrix xold = xmean;\n-        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n         xmean = bestArx.multiply(weights);\n-        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n         final RealMatrix zmean = bestArz.multiply(weights);\n         final boolean hsig = updateEvolutionPaths(zmean, xold);\n         if (diagonalOnly <= 0) {\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java@doOptimize\n@@ -1,79 +1,136 @@\n @Override\n protected PointValuePair doOptimize() {\n-    checkParameters();\n-    final GoalType goal = getGoalType();\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n     final double[] guess = getStartPoint();\n-    final int n = guess.length;\n-    final double[][] direc = new double[n][n];\n-    for (int i = 0; i < n; i++) {\n-        direc[i][i] = 1;\n-    }\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    double[] x = guess;\n-    double fVal = computeObjectiveValue(x);\n-    double[] x1 = x.clone();\n-    int iter = 0;\n-    while (true) {\n-        ++iter;\n-        double fX = fVal;\n-        double fX2 = 0;\n-        double delta = 0;\n-        int bigInd = 0;\n-        double alphaMin = 0;\n-        for (int i = 0; i < n; i++) {\n-            final double[] d = MathArrays.copyOf(direc[i]);\n-            fX2 = fVal;\n-            final UnivariatePointValuePair optimum = line.search(x, d);\n-            fVal = optimum.getValue();\n-            alphaMin = optimum.getPoint();\n-            final double[][] result = newPointAndDirection(x, d, alphaMin);\n-            x = result[0];\n-            if ((fX2 - fVal) > delta) {\n-                delta = fX2 - fVal;\n-                bigInd = i;\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                arz.setColumn(k, randn(dimension));\n+                arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                if (fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n             }\n         }\n-        // Default convergence check.\n-        boolean stop = 2 * (fX - fVal) <= (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) + absoluteThreshold);\n-        final PointValuePair previous = new PointValuePair(x1, fX);\n-        final PointValuePair current = new PointValuePair(x, fVal);\n-        if (!stop) {\n-            // User-defined stopping criteria.\n-            if (checker != null) {\n-                stop = checker.converged(iter, previous, current);\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n             }\n         }\n-        if (stop) {\n-            if (goal == GoalType.MINIMIZE) {\n-                return (fVal < fX) ? current : previous;\n-            } else {\n-                return (fVal > fX) ? current : previous;\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n-        final double[] d = new double[n];\n-        final double[] x2 = new double[n];\n-        for (int i = 0; i < n; i++) {\n-            d[i] = x[i] - x1[i];\n-            x2[i] = 2 * x[i] - x1[i];\n-        }\n-        x1 = x.clone();\n-        fX2 = computeObjectiveValue(x2);\n-        if (fX > fX2) {\n-            double t = 2 * (fX + fX2 - 2 * fVal);\n-            double temp = fX - fVal - delta;\n-            t *= temp * temp;\n-            temp = fX - fX2;\n-            t -= delta * temp * temp;\n-            if (t < 0.0) {\n-                final UnivariatePointValuePair optimum = line.search(x, d);\n-                fVal = optimum.getValue();\n-                alphaMin = optimum.getPoint();\n-                final double[][] result = newPointAndDirection(x, d, alphaMin);\n-                x = result[0];\n-                final int lastInd = n - 1;\n-                direc[bigInd] = direc[lastInd];\n-                direc[lastInd] = result[1];\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n             }\n         }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java@doOptimize\n@@ -1,44 +1,136 @@\n @Override\n protected PointValuePair doOptimize() {\n-    checkParameters();\n-    // Indirect call to \"computeObjectiveValue\" in order to update the\n-    // evaluations counter.\n-    final MultivariateFunction evalFunc = new MultivariateFunction() {\n-\n-        public double value(double[] point) {\n-            return computeObjectiveValue(point);\n-        }\n-    };\n-    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n-    final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() {\n-\n-        public int compare(final PointValuePair o1, final PointValuePair o2) {\n-            final double v1 = o1.getValue();\n-            final double v2 = o2.getValue();\n-            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n-        }\n-    };\n-    // Initialize search.\n-    simplex.build(getStartPoint());\n-    simplex.evaluate(evalFunc, comparator);\n-    PointValuePair[] previous = null;\n-    int iteration = 0;\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    while (true) {\n-        if (iteration > 0) {\n-            boolean converged = true;\n-            for (int i = 0; i < simplex.getSize(); i++) {\n-                PointValuePair prev = previous[i];\n-                converged = converged && checker.converged(iteration, prev, simplex.getPoint(i));\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                arz.setColumn(k, randn(dimension));\n+                arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                if (fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n             }\n-            if (converged) {\n-                // We have found an optimum.\n-                return simplex.getPoint(0);\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n             }\n         }\n-        // We still need to search.\n-        previous = simplex.getPoints();\n-        simplex.iterate(evalFunc, comparator);\n-        ++iteration;\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n+            }\n+        }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n+            }\n+        }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java@doOptimize\n@@ -1,76 +1,136 @@\n @Override\n-public PointVectorValuePair doOptimize() {\n-    checkParameters();\n-    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n-    // Computation will be useless without a checker (see \"for-loop\").\n-    if (checker == null) {\n-        throw new NullArgumentException();\n-    }\n-    final double[] targetValues = getTarget();\n-    // Number of observed data.\n-    final int nR = targetValues.length;\n-    final RealMatrix weightMatrix = getWeight();\n-    // Diagonal of the weight matrix.\n-    final double[] residualsWeights = new double[nR];\n-    for (int i = 0; i < nR; i++) {\n-        residualsWeights[i] = weightMatrix.getEntry(i, i);\n-    }\n-    final double[] currentPoint = getStartPoint();\n-    final int nC = currentPoint.length;\n-    // iterate until convergence is reached\n-    PointVectorValuePair current = null;\n-    int iter = 0;\n-    for (boolean converged = false; !converged; ) {\n-        ++iter;\n-        // evaluate the objective function and its jacobian\n-        PointVectorValuePair previous = current;\n-        // Value of the objective function at \"currentPoint\".\n-        final double[] currentObjective = computeObjectiveValue(currentPoint);\n-        final double[] currentResiduals = computeResiduals(currentObjective);\n-        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n-        current = new PointVectorValuePair(currentPoint, currentObjective);\n-        // build the linear problem\n-        final double[] b = new double[nC];\n-        final double[][] a = new double[nC][nC];\n-        for (int i = 0; i < nR; ++i) {\n-            final double[] grad = weightedJacobian.getRow(i);\n-            final double weight = residualsWeights[i];\n-            final double residual = currentResiduals[i];\n-            // compute the normal equation\n-            final double wr = weight * residual;\n-            for (int j = 0; j < nC; ++j) {\n-                b[j] += wr * grad[j];\n+protected PointValuePair doOptimize() {\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                arz.setColumn(k, randn(dimension));\n+                arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                if (fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n             }\n-            // build the contribution matrix for measurement i\n-            for (int k = 0; k < nC; ++k) {\n-                double[] ak = a[k];\n-                double wgk = weight * grad[k];\n-                for (int l = 0; l < nC; ++l) {\n-                    ak[l] += wgk * grad[l];\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n+            }\n+        }\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n                 }\n             }\n         }\n-        try {\n-            // solve the linearized least squares problem\n-            RealMatrix mA = new BlockRealMatrix(a);\n-            DecompositionSolver solver = useLU ? new LUDecomposition(mA).getSolver() : new QRDecomposition(mA).getSolver();\n-            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n-            // update the estimated parameters\n-            for (int i = 0; i < nC; ++i) {\n-                currentPoint[i] += dX[i];\n-            }\n-        } catch (SingularMatrixException e) {\n-            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n-        }\n-        // Check convergence.\n-        if (previous != null) {\n-            converged = checker.converged(iter, previous, current);\n-            if (converged) {\n-                setCost(computeCost(currentResiduals));\n-                return current;\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n-    // Must never happen.\n-    throw new MathInternalError();\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java@doOptimize\n@@ -1,220 +1,136 @@\n @Override\n-protected PointVectorValuePair doOptimize() {\n-    checkParameters();\n-    // Number of observed data.\n-    final int nR = getTarget().length;\n-    final double[] currentPoint = getStartPoint();\n-    // Number of parameters.\n-    final int nC = currentPoint.length;\n-    // arrays shared with the other private methods\n-    solvedCols = FastMath.min(nR, nC);\n-    diagR = new double[nC];\n-    jacNorm = new double[nC];\n-    beta = new double[nC];\n-    permutation = new int[nC];\n-    lmDir = new double[nC];\n-    // local point\n-    double delta = 0;\n-    double xNorm = 0;\n-    double[] diag = new double[nC];\n-    double[] oldX = new double[nC];\n-    double[] oldRes = new double[nR];\n-    double[] oldObj = new double[nR];\n-    double[] qtf = new double[nR];\n-    double[] work1 = new double[nC];\n-    double[] work2 = new double[nC];\n-    double[] work3 = new double[nC];\n-    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n-    // Evaluate the function at the starting point and calculate its norm.\n-    double[] currentObjective = computeObjectiveValue(currentPoint);\n-    double[] currentResiduals = computeResiduals(currentObjective);\n-    PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n-    double currentCost = computeCost(currentResiduals);\n-    // Outer loop.\n-    lmPar = 0;\n-    boolean firstIteration = true;\n-    int iter = 0;\n-    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n-    while (true) {\n-        ++iter;\n-        final PointVectorValuePair previous = current;\n-        // QR decomposition of the jacobian matrix\n-        qrDecomposition(computeWeightedJacobian(currentPoint));\n-        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n-        for (int i = 0; i < nR; i++) {\n-            qtf[i] = weightedResidual[i];\n+protected PointValuePair doOptimize() {\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                arz.setColumn(k, randn(dimension));\n+                arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                if (fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n+            }\n         }\n-        // compute Qt.res\n-        qTy(qtf);\n-        // now we don't need Q anymore,\n-        // so let jacobian contain the R matrix with its diagonal elements\n-        for (int k = 0; k < solvedCols; ++k) {\n-            int pk = permutation[k];\n-            weightedJacobian[k][pk] = diagR[pk];\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n         }\n-        if (firstIteration) {\n-            // scale the point according to the norms of the columns\n-            // of the initial jacobian\n-            xNorm = 0;\n-            for (int k = 0; k < nC; ++k) {\n-                double dk = jacNorm[k];\n-                if (dk == 0) {\n-                    dk = 1.0;\n-                }\n-                double xk = dk * currentPoint[k];\n-                xNorm += xk * xk;\n-                diag[k] = dk;\n-            }\n-            xNorm = FastMath.sqrt(xNorm);\n-            // initialize the step bound delta\n-            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n-        }\n-        // check orthogonality between function vector and jacobian columns\n-        double maxCosine = 0;\n-        if (currentCost != 0) {\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                double s = jacNorm[pj];\n-                if (s != 0) {\n-                    double sum = 0;\n-                    for (int i = 0; i <= j; ++i) {\n-                        sum += weightedJacobian[i][pj] * qtf[i];\n-                    }\n-                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n                 }\n             }\n         }\n-        if (maxCosine <= orthoTolerance) {\n-            // Convergence has been reached.\n-            setCost(currentCost);\n-            return current;\n-        }\n-        // rescale if necessary\n-        for (int j = 0; j < nC; ++j) {\n-            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n-        }\n-        // Inner loop.\n-        for (double ratio = 0; ratio < 1.0e-4; ) {\n-            // save the state\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                oldX[pj] = currentPoint[pj];\n-            }\n-            final double previousCost = currentCost;\n-            double[] tmpVec = weightedResidual;\n-            weightedResidual = oldRes;\n-            oldRes = tmpVec;\n-            tmpVec = currentObjective;\n-            currentObjective = oldObj;\n-            oldObj = tmpVec;\n-            // determine the Levenberg-Marquardt parameter\n-            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n-            // compute the new point and the norm of the evolution direction\n-            double lmNorm = 0;\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                lmDir[pj] = -lmDir[pj];\n-                currentPoint[pj] = oldX[pj] + lmDir[pj];\n-                double s = diag[pj] * lmDir[pj];\n-                lmNorm += s * s;\n-            }\n-            lmNorm = FastMath.sqrt(lmNorm);\n-            // on the first iteration, adjust the initial step bound.\n-            if (firstIteration) {\n-                delta = FastMath.min(delta, lmNorm);\n-            }\n-            // Evaluate the function at x + p and calculate its norm.\n-            currentObjective = computeObjectiveValue(currentPoint);\n-            currentResiduals = computeResiduals(currentObjective);\n-            current = new PointVectorValuePair(currentPoint, currentObjective);\n-            currentCost = computeCost(currentResiduals);\n-            // compute the scaled actual reduction\n-            double actRed = -1.0;\n-            if (0.1 * currentCost < previousCost) {\n-                double r = currentCost / previousCost;\n-                actRed = 1.0 - r * r;\n-            }\n-            // compute the scaled predicted reduction\n-            // and the scaled directional derivative\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                double dirJ = lmDir[pj];\n-                work1[j] = 0;\n-                for (int i = 0; i <= j; ++i) {\n-                    work1[i] += weightedJacobian[i][pj] * dirJ;\n-                }\n-            }\n-            double coeff1 = 0;\n-            for (int j = 0; j < solvedCols; ++j) {\n-                coeff1 += work1[j] * work1[j];\n-            }\n-            double pc2 = previousCost * previousCost;\n-            coeff1 = coeff1 / pc2;\n-            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n-            double preRed = coeff1 + 2 * coeff2;\n-            double dirDer = -(coeff1 + coeff2);\n-            // ratio of the actual to the predicted reduction\n-            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n-            // update the step bound\n-            if (ratio <= 0.25) {\n-                double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n-                if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n-                    tmp = 0.1;\n-                }\n-                delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n-                lmPar /= tmp;\n-            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n-                delta = 2 * lmNorm;\n-                lmPar *= 0.5;\n-            }\n-            // test for successful iteration.\n-            if (ratio >= 1.0e-4) {\n-                // successful iteration, update the norm\n-                firstIteration = false;\n-                xNorm = 0;\n-                for (int k = 0; k < nC; ++k) {\n-                    double xK = diag[k] * currentPoint[k];\n-                    xNorm += xK * xK;\n-                }\n-                xNorm = FastMath.sqrt(xNorm);\n-                // tests for convergence.\n-                if (checker != null) {\n-                    // we use the vectorial convergence checker\n-                    if (checker.converged(iter, previous, current)) {\n-                        setCost(currentCost);\n-                        return current;\n-                    }\n-                }\n-            } else {\n-                // failed iteration, reset the previous values\n-                currentCost = previousCost;\n-                for (int j = 0; j < solvedCols; ++j) {\n-                    int pj = permutation[j];\n-                    currentPoint[pj] = oldX[pj];\n-                }\n-                tmpVec = weightedResidual;\n-                weightedResidual = oldRes;\n-                oldRes = tmpVec;\n-                tmpVec = currentObjective;\n-                currentObjective = oldObj;\n-                oldObj = tmpVec;\n-                // Reset \"current\" to previous values.\n-                current = new PointVectorValuePair(currentPoint, currentObjective);\n-            }\n-            // Default convergence criteria.\n-            if ((FastMath.abs(actRed) <= costRelativeTolerance && preRed <= costRelativeTolerance && ratio <= 2.0) || delta <= parRelativeTolerance * xNorm) {\n-                setCost(currentCost);\n-                return current;\n-            }\n-            // tests for termination and stringent tolerances\n-            // (2.2204e-16 is the machine epsilon for IEEE754)\n-            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE, costRelativeTolerance);\n-            } else if (delta <= 2.2204e-16 * xNorm) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE, parRelativeTolerance);\n-            } else if (maxCosine <= 2.2204e-16) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE, orthoTolerance);\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n", "finish_reason": "stop", "valid": false, "num": 1}, {"output": "// The fixed function is too long to fit in one code block, so it is split into multiple blocks.\n@Override\nprotected PointValuePair doOptimize() {\n    // -------------------- Initialization --------------------------------\n    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n    final FitnessFunction fitfun = new FitnessFunction();\n    final double[] guess = getStartPoint();\n    // number of objective variables/problem dimension\n    dimension = guess.length;\n    initializeCMA(guess);\n    iterations = 0;\n    double bestValue = fitfun.value(guess);\n    push(fitnessHistory, bestValue);\n    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n    PointValuePair lastResult = null;\n    // -------------------- Generation Loop --------------------------------\n    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n        // Generate and evaluate lambda offspring\n        final RealMatrix arz = randn1(dimension, lambda);\n        final RealMatrix arx = zeros(dimension, lambda);\n        final double[] fitness = new double[lambda];\n        // generate random offspring\n        for (int k = 0; k < lambda; k++) {\n            RealMatrix arxk = null;\n            for (int i = 0; i < checkFeasableCount + 1; i++) {\n                if (diagonalOnly <= 0) {\n                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(\n                    sigma));\n                } else {\n                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                }\n                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n                    break;\n                }\n                // regenerate random arguments for row\n                arz.setColumn(k, randn(dimension));\n            }\n            copyColumn(arxk, 0, arx, k);\n            try {\n                // compute fitness\n                fitness[k] = fitfun.value(arx.getColumn(k));\n            } catch (TooManyEvaluationsException e) {\n                break generationLoop;\n            }\n        }\n        // Sort by fitness and compute weighted mean into xmean\n        final int[] arindex = sortedIndices(fitness);\n        // Calculate new xmean, this is selection and recombination\n        // for speed up of Eq. (2) and (3)\n        final RealMatrix xold = xmean;\n        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n        xmean = bestArx.multiply(weights);\n        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n        final RealMatrix zmean = bestArz.multiply(weights);\n        final double normps = zmean.getNorm();\n        chiN = Math.sqrt(dimension) * (1.0 - (1.0 / (4.0 * dimension)) + (1.0 / (21.0 * dimension * dimension)));\n        double hsig = updateEvolutionPaths(zmean, xold);\n        if (diagonalOnly <= 0) {\n            updateCovariance(hsig, bestArx, arz, arindex, xold);\n        } else {\n            updateCovarianceDiagonalOnly(hsig, bestArz);\n        }\n        // Adapt step size sigma - Eq. (5)\n        if (iterations > 1 + lambda / mu) {\n            sigma *= Math.exp((cs / damps) * (normps / chiN - 1.0));\n        }\n        final double bestFitness = fitness[arindex[0]];\n        final double worstFitness = fitness[arindex[arindex.length - 1]];\n        if (bestValue > bestFitness) {\n            bestValue = bestFitness;\n            lastResult = optimum;\n            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n            if (getConvergenceChecker() != null && lastResult != null) {\n                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                    break generationLoop;\n                }\n            }\n        } else {\n            lastResult = optimum;\n        }\n        // handle termination criteria\n        // Break, if fitness is good enough\n        if (stopFitness != 0) {\n            // only if stopFitness is defined\n            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                break generationLoop;\n            }\n        }\n        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n        final double[] pcCol = pc.getColumn(0);\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                break;\n            }\n            if (i >= dimension - 1) {\n                break generationLoop;\n            }\n        }\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                break generationLoop;\n            }\n        }\n        final double historyBest = min(fitnessHistory);\n        final double historyWorst = max(fitnessHistory);\n        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n            break generationLoop;\n        }\n        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n            break generationLoop;\n        }\n        // condition number of the covariance matrix exceeds 1e14\n        if (max(diagD) / min(diagD) > 1e7) {\n            break generationLoop;\n        }\n        // user defined termination\n        if (getConvergenceChecker() != null) {\n            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n                break generationLoop;\n            }\n            lastResult = current;\n        }\n        // Adjust step size in case of equal function values (flat fitness)\n        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        // store best in history\n        push(fitnessHistory, bestFitness);\n        fitfun.setValueRange(worstFitness - bestFitness);\n        if (generateStatistics) {\n            statisticsSigmaHistory.add(sigma);\n            statisticsFitnessHistory.add(bestFitness);\n            statisticsMeanHistory.add(xmean.transpose());\n            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n        }\n    }\n    return optimum;\n}", "diff": "--- org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java@doOptimize\n@@ -1,88 +1,141 @@\n @Override\n protected PointValuePair doOptimize() {\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    final double[] point = getStartPoint();\n-    final GoalType goal = getGoalType();\n-    final int n = point.length;\n-    double[] r = computeObjectiveGradient(point);\n-    if (goal == GoalType.MINIMIZE) {\n-        for (int i = 0; i < n; i++) {\n-            r[i] = -r[i];\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n+            }\n+        }\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final double normps = zmean.getNorm();\n+        chiN = Math.sqrt(dimension) * (1.0 - (1.0 / (4.0 * dimension)) + (1.0 / (21.0 * dimension * dimension)));\n+        double hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        if (iterations > 1 + lambda / mu) {\n+            sigma *= Math.exp((cs / damps) * (normps / chiN - 1.0));\n+        }\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n+            }\n+        } else {\n+            lastResult = optimum;\n+        }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n+            }\n+        }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n         }\n     }\n-    // Initial search direction.\n-    double[] steepestDescent = preconditioner.precondition(point, r);\n-    double[] searchDirection = steepestDescent.clone();\n-    double delta = 0;\n-    for (int i = 0; i < n; ++i) {\n-        delta += r[i] * searchDirection[i];\n-    }\n-    PointValuePair current = null;\n-    int iter = 0;\n-    int maxEval = getMaxEvaluations();\n-    while (true) {\n-        ++iter;\n-        final double objective = computeObjectiveValue(point);\n-        PointValuePair previous = current;\n-        current = new PointValuePair(point, objective);\n-        if (previous != null) {\n-            if (checker.converged(iter, previous, current)) {\n-                // We have found an optimum.\n-                return current;\n-            }\n-        }\n-        // Find the optimal step in the search direction.\n-        final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n-        final double uB = findUpperBound(lsf, 0, initialStep);\n-        // XXX Last parameters is set to a value close to zero in order to\n-        // work around the divergence problem in the \"testCircleFitting\"\n-        // unit test (see MATH-439).\n-        final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n-        // Subtract used up evaluations.\n-        maxEval -= solver.getEvaluations();\n-        // Validate new point.\n-        for (int i = 0; i < point.length; ++i) {\n-            point[i] += step * searchDirection[i];\n-        }\n-        r = computeObjectiveGradient(point);\n-        if (goal == GoalType.MINIMIZE) {\n-            for (int i = 0; i < n; ++i) {\n-                r[i] = -r[i];\n-            }\n-        }\n-        // Compute beta.\n-        final double deltaOld = delta;\n-        final double[] newSteepestDescent = preconditioner.precondition(point, r);\n-        delta = 0;\n-        for (int i = 0; i < n; ++i) {\n-            delta += r[i] * newSteepestDescent[i];\n-        }\n-        final double beta;\n-        switch(updateFormula) {\n-            case FLETCHER_REEVES:\n-                beta = delta / deltaOld;\n-                break;\n-            case POLAK_RIBIERE:\n-                double deltaMid = 0;\n-                for (int i = 0; i < r.length; ++i) {\n-                    deltaMid += r[i] * steepestDescent[i];\n-                }\n-                beta = (delta - deltaMid) / deltaOld;\n-                break;\n-            default:\n-                // Should never happen.\n-                throw new MathInternalError();\n-        }\n-        steepestDescent = newSteepestDescent;\n-        // Compute conjugate search direction.\n-        if (iter % n == 0 || beta < 0) {\n-            // Break conjugation: reset search direction.\n-            searchDirection = steepestDescent.clone();\n-        } else {\n-            // Compute new conjugate search direction.\n-            for (int i = 0; i < n; ++i) {\n-                searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n-            }\n-        }\n-    }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java@doOptimize\n@@ -23,7 +23,7 @@\n             RealMatrix arxk = null;\n             for (int i = 0; i < checkFeasableCount + 1; i++) {\n                 if (diagonalOnly <= 0) {\n-                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(\n                     sigma));\n                 } else {\n                     arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n@@ -51,14 +51,18 @@\n         xmean = bestArx.multiply(weights);\n         final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n         final RealMatrix zmean = bestArz.multiply(weights);\n-        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        final double normps = zmean.getNorm();\n+        chiN = Math.sqrt(dimension) * (1.0 - (1.0 / (4.0 * dimension)) + (1.0 / (21.0 * dimension * dimension)));\n+        double hsig = updateEvolutionPaths(zmean, xold);\n         if (diagonalOnly <= 0) {\n             updateCovariance(hsig, bestArx, arz, arindex, xold);\n         } else {\n             updateCovarianceDiagonalOnly(hsig, bestArz);\n         }\n         // Adapt step size sigma - Eq. (5)\n-        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        if (iterations > 1 + lambda / mu) {\n+            sigma *= Math.exp((cs / damps) * (normps / chiN - 1.0));\n+        }\n         final double bestFitness = fitness[arindex[0]];\n         final double worstFitness = fitness[arindex[arindex.length - 1]];\n         if (bestValue > bestFitness) {\n@@ -70,6 +74,8 @@\n                     break generationLoop;\n                 }\n             }\n+        } else {\n+            lastResult = optimum;\n         }\n         // handle termination criteria\n         // Break, if fitness is good enough\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java@doOptimize\n@@ -1,79 +1,141 @@\n @Override\n protected PointValuePair doOptimize() {\n-    checkParameters();\n-    final GoalType goal = getGoalType();\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n     final double[] guess = getStartPoint();\n-    final int n = guess.length;\n-    final double[][] direc = new double[n][n];\n-    for (int i = 0; i < n; i++) {\n-        direc[i][i] = 1;\n-    }\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    double[] x = guess;\n-    double fVal = computeObjectiveValue(x);\n-    double[] x1 = x.clone();\n-    int iter = 0;\n-    while (true) {\n-        ++iter;\n-        double fX = fVal;\n-        double fX2 = 0;\n-        double delta = 0;\n-        int bigInd = 0;\n-        double alphaMin = 0;\n-        for (int i = 0; i < n; i++) {\n-            final double[] d = MathArrays.copyOf(direc[i]);\n-            fX2 = fVal;\n-            final UnivariatePointValuePair optimum = line.search(x, d);\n-            fVal = optimum.getValue();\n-            alphaMin = optimum.getPoint();\n-            final double[][] result = newPointAndDirection(x, d, alphaMin);\n-            x = result[0];\n-            if ((fX2 - fVal) > delta) {\n-                delta = fX2 - fVal;\n-                bigInd = i;\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n             }\n         }\n-        // Default convergence check.\n-        boolean stop = 2 * (fX - fVal) <= (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) + absoluteThreshold);\n-        final PointValuePair previous = new PointValuePair(x1, fX);\n-        final PointValuePair current = new PointValuePair(x, fVal);\n-        if (!stop) {\n-            // User-defined stopping criteria.\n-            if (checker != null) {\n-                stop = checker.converged(iter, previous, current);\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final double normps = zmean.getNorm();\n+        chiN = Math.sqrt(dimension) * (1.0 - (1.0 / (4.0 * dimension)) + (1.0 / (21.0 * dimension * dimension)));\n+        double hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        if (iterations > 1 + lambda / mu) {\n+            sigma *= Math.exp((cs / damps) * (normps / chiN - 1.0));\n+        }\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n+            }\n+        } else {\n+            lastResult = optimum;\n+        }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n-        if (stop) {\n-            if (goal == GoalType.MINIMIZE) {\n-                return (fVal < fX) ? current : previous;\n-            } else {\n-                return (fVal > fX) ? current : previous;\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n             }\n         }\n-        final double[] d = new double[n];\n-        final double[] x2 = new double[n];\n-        for (int i = 0; i < n; i++) {\n-            d[i] = x[i] - x1[i];\n-            x2[i] = 2 * x[i] - x1[i];\n-        }\n-        x1 = x.clone();\n-        fX2 = computeObjectiveValue(x2);\n-        if (fX > fX2) {\n-            double t = 2 * (fX + fX2 - 2 * fVal);\n-            double temp = fX - fVal - delta;\n-            t *= temp * temp;\n-            temp = fX - fX2;\n-            t -= delta * temp * temp;\n-            if (t < 0.0) {\n-                final UnivariatePointValuePair optimum = line.search(x, d);\n-                fVal = optimum.getValue();\n-                alphaMin = optimum.getPoint();\n-                final double[][] result = newPointAndDirection(x, d, alphaMin);\n-                x = result[0];\n-                final int lastInd = n - 1;\n-                direc[bigInd] = direc[lastInd];\n-                direc[lastInd] = result[1];\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n             }\n         }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java@doOptimize\n@@ -1,44 +1,141 @@\n @Override\n protected PointValuePair doOptimize() {\n-    checkParameters();\n-    // Indirect call to \"computeObjectiveValue\" in order to update the\n-    // evaluations counter.\n-    final MultivariateFunction evalFunc = new MultivariateFunction() {\n-\n-        public double value(double[] point) {\n-            return computeObjectiveValue(point);\n-        }\n-    };\n-    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n-    final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() {\n-\n-        public int compare(final PointValuePair o1, final PointValuePair o2) {\n-            final double v1 = o1.getValue();\n-            final double v2 = o2.getValue();\n-            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n-        }\n-    };\n-    // Initialize search.\n-    simplex.build(getStartPoint());\n-    simplex.evaluate(evalFunc, comparator);\n-    PointValuePair[] previous = null;\n-    int iteration = 0;\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    while (true) {\n-        if (iteration > 0) {\n-            boolean converged = true;\n-            for (int i = 0; i < simplex.getSize(); i++) {\n-                PointValuePair prev = previous[i];\n-                converged = converged && checker.converged(iteration, prev, simplex.getPoint(i));\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n             }\n-            if (converged) {\n-                // We have found an optimum.\n-                return simplex.getPoint(0);\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n             }\n         }\n-        // We still need to search.\n-        previous = simplex.getPoints();\n-        simplex.iterate(evalFunc, comparator);\n-        ++iteration;\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final double normps = zmean.getNorm();\n+        chiN = Math.sqrt(dimension) * (1.0 - (1.0 / (4.0 * dimension)) + (1.0 / (21.0 * dimension * dimension)));\n+        double hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        if (iterations > 1 + lambda / mu) {\n+            sigma *= Math.exp((cs / damps) * (normps / chiN - 1.0));\n+        }\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n+            }\n+        } else {\n+            lastResult = optimum;\n+        }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n+            }\n+        }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java@doOptimize\n@@ -1,76 +1,141 @@\n @Override\n-public PointVectorValuePair doOptimize() {\n-    checkParameters();\n-    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n-    // Computation will be useless without a checker (see \"for-loop\").\n-    if (checker == null) {\n-        throw new NullArgumentException();\n-    }\n-    final double[] targetValues = getTarget();\n-    // Number of observed data.\n-    final int nR = targetValues.length;\n-    final RealMatrix weightMatrix = getWeight();\n-    // Diagonal of the weight matrix.\n-    final double[] residualsWeights = new double[nR];\n-    for (int i = 0; i < nR; i++) {\n-        residualsWeights[i] = weightMatrix.getEntry(i, i);\n-    }\n-    final double[] currentPoint = getStartPoint();\n-    final int nC = currentPoint.length;\n-    // iterate until convergence is reached\n-    PointVectorValuePair current = null;\n-    int iter = 0;\n-    for (boolean converged = false; !converged; ) {\n-        ++iter;\n-        // evaluate the objective function and its jacobian\n-        PointVectorValuePair previous = current;\n-        // Value of the objective function at \"currentPoint\".\n-        final double[] currentObjective = computeObjectiveValue(currentPoint);\n-        final double[] currentResiduals = computeResiduals(currentObjective);\n-        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n-        current = new PointVectorValuePair(currentPoint, currentObjective);\n-        // build the linear problem\n-        final double[] b = new double[nC];\n-        final double[][] a = new double[nC][nC];\n-        for (int i = 0; i < nR; ++i) {\n-            final double[] grad = weightedJacobian.getRow(i);\n-            final double weight = residualsWeights[i];\n-            final double residual = currentResiduals[i];\n-            // compute the normal equation\n-            final double wr = weight * residual;\n-            for (int j = 0; j < nC; ++j) {\n-                b[j] += wr * grad[j];\n+protected PointValuePair doOptimize() {\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n             }\n-            // build the contribution matrix for measurement i\n-            for (int k = 0; k < nC; ++k) {\n-                double[] ak = a[k];\n-                double wgk = weight * grad[k];\n-                for (int l = 0; l < nC; ++l) {\n-                    ak[l] += wgk * grad[l];\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n+            }\n+        }\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final double normps = zmean.getNorm();\n+        chiN = Math.sqrt(dimension) * (1.0 - (1.0 / (4.0 * dimension)) + (1.0 / (21.0 * dimension * dimension)));\n+        double hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        if (iterations > 1 + lambda / mu) {\n+            sigma *= Math.exp((cs / damps) * (normps / chiN - 1.0));\n+        }\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n                 }\n             }\n+        } else {\n+            lastResult = optimum;\n         }\n-        try {\n-            // solve the linearized least squares problem\n-            RealMatrix mA = new BlockRealMatrix(a);\n-            DecompositionSolver solver = useLU ? new LUDecomposition(mA).getSolver() : new QRDecomposition(mA).getSolver();\n-            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n-            // update the estimated parameters\n-            for (int i = 0; i < nC; ++i) {\n-                currentPoint[i] += dX[i];\n-            }\n-        } catch (SingularMatrixException e) {\n-            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n-        }\n-        // Check convergence.\n-        if (previous != null) {\n-            converged = checker.converged(iter, previous, current);\n-            if (converged) {\n-                setCost(computeCost(currentResiduals));\n-                return current;\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n-    // Must never happen.\n-    throw new MathInternalError();\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java@doOptimize\n@@ -1,220 +1,141 @@\n @Override\n-protected PointVectorValuePair doOptimize() {\n-    checkParameters();\n-    // Number of observed data.\n-    final int nR = getTarget().length;\n-    final double[] currentPoint = getStartPoint();\n-    // Number of parameters.\n-    final int nC = currentPoint.length;\n-    // arrays shared with the other private methods\n-    solvedCols = FastMath.min(nR, nC);\n-    diagR = new double[nC];\n-    jacNorm = new double[nC];\n-    beta = new double[nC];\n-    permutation = new int[nC];\n-    lmDir = new double[nC];\n-    // local point\n-    double delta = 0;\n-    double xNorm = 0;\n-    double[] diag = new double[nC];\n-    double[] oldX = new double[nC];\n-    double[] oldRes = new double[nR];\n-    double[] oldObj = new double[nR];\n-    double[] qtf = new double[nR];\n-    double[] work1 = new double[nC];\n-    double[] work2 = new double[nC];\n-    double[] work3 = new double[nC];\n-    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n-    // Evaluate the function at the starting point and calculate its norm.\n-    double[] currentObjective = computeObjectiveValue(currentPoint);\n-    double[] currentResiduals = computeResiduals(currentObjective);\n-    PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n-    double currentCost = computeCost(currentResiduals);\n-    // Outer loop.\n-    lmPar = 0;\n-    boolean firstIteration = true;\n-    int iter = 0;\n-    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n-    while (true) {\n-        ++iter;\n-        final PointVectorValuePair previous = current;\n-        // QR decomposition of the jacobian matrix\n-        qrDecomposition(computeWeightedJacobian(currentPoint));\n-        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n-        for (int i = 0; i < nR; i++) {\n-            qtf[i] = weightedResidual[i];\n+protected PointValuePair doOptimize() {\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n+            }\n         }\n-        // compute Qt.res\n-        qTy(qtf);\n-        // now we don't need Q anymore,\n-        // so let jacobian contain the R matrix with its diagonal elements\n-        for (int k = 0; k < solvedCols; ++k) {\n-            int pk = permutation[k];\n-            weightedJacobian[k][pk] = diagR[pk];\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final double normps = zmean.getNorm();\n+        chiN = Math.sqrt(dimension) * (1.0 - (1.0 / (4.0 * dimension)) + (1.0 / (21.0 * dimension * dimension)));\n+        double hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n         }\n-        if (firstIteration) {\n-            // scale the point according to the norms of the columns\n-            // of the initial jacobian\n-            xNorm = 0;\n-            for (int k = 0; k < nC; ++k) {\n-                double dk = jacNorm[k];\n-                if (dk == 0) {\n-                    dk = 1.0;\n-                }\n-                double xk = dk * currentPoint[k];\n-                xNorm += xk * xk;\n-                diag[k] = dk;\n-            }\n-            xNorm = FastMath.sqrt(xNorm);\n-            // initialize the step bound delta\n-            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n+        // Adapt step size sigma - Eq. (5)\n+        if (iterations > 1 + lambda / mu) {\n+            sigma *= Math.exp((cs / damps) * (normps / chiN - 1.0));\n         }\n-        // check orthogonality between function vector and jacobian columns\n-        double maxCosine = 0;\n-        if (currentCost != 0) {\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                double s = jacNorm[pj];\n-                if (s != 0) {\n-                    double sum = 0;\n-                    for (int i = 0; i <= j; ++i) {\n-                        sum += weightedJacobian[i][pj] * qtf[i];\n-                    }\n-                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n                 }\n             }\n+        } else {\n+            lastResult = optimum;\n         }\n-        if (maxCosine <= orthoTolerance) {\n-            // Convergence has been reached.\n-            setCost(currentCost);\n-            return current;\n-        }\n-        // rescale if necessary\n-        for (int j = 0; j < nC; ++j) {\n-            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n-        }\n-        // Inner loop.\n-        for (double ratio = 0; ratio < 1.0e-4; ) {\n-            // save the state\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                oldX[pj] = currentPoint[pj];\n-            }\n-            final double previousCost = currentCost;\n-            double[] tmpVec = weightedResidual;\n-            weightedResidual = oldRes;\n-            oldRes = tmpVec;\n-            tmpVec = currentObjective;\n-            currentObjective = oldObj;\n-            oldObj = tmpVec;\n-            // determine the Levenberg-Marquardt parameter\n-            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n-            // compute the new point and the norm of the evolution direction\n-            double lmNorm = 0;\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                lmDir[pj] = -lmDir[pj];\n-                currentPoint[pj] = oldX[pj] + lmDir[pj];\n-                double s = diag[pj] * lmDir[pj];\n-                lmNorm += s * s;\n-            }\n-            lmNorm = FastMath.sqrt(lmNorm);\n-            // on the first iteration, adjust the initial step bound.\n-            if (firstIteration) {\n-                delta = FastMath.min(delta, lmNorm);\n-            }\n-            // Evaluate the function at x + p and calculate its norm.\n-            currentObjective = computeObjectiveValue(currentPoint);\n-            currentResiduals = computeResiduals(currentObjective);\n-            current = new PointVectorValuePair(currentPoint, currentObjective);\n-            currentCost = computeCost(currentResiduals);\n-            // compute the scaled actual reduction\n-            double actRed = -1.0;\n-            if (0.1 * currentCost < previousCost) {\n-                double r = currentCost / previousCost;\n-                actRed = 1.0 - r * r;\n-            }\n-            // compute the scaled predicted reduction\n-            // and the scaled directional derivative\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                double dirJ = lmDir[pj];\n-                work1[j] = 0;\n-                for (int i = 0; i <= j; ++i) {\n-                    work1[i] += weightedJacobian[i][pj] * dirJ;\n-                }\n-            }\n-            double coeff1 = 0;\n-            for (int j = 0; j < solvedCols; ++j) {\n-                coeff1 += work1[j] * work1[j];\n-            }\n-            double pc2 = previousCost * previousCost;\n-            coeff1 = coeff1 / pc2;\n-            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n-            double preRed = coeff1 + 2 * coeff2;\n-            double dirDer = -(coeff1 + coeff2);\n-            // ratio of the actual to the predicted reduction\n-            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n-            // update the step bound\n-            if (ratio <= 0.25) {\n-                double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n-                if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n-                    tmp = 0.1;\n-                }\n-                delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n-                lmPar /= tmp;\n-            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n-                delta = 2 * lmNorm;\n-                lmPar *= 0.5;\n-            }\n-            // test for successful iteration.\n-            if (ratio >= 1.0e-4) {\n-                // successful iteration, update the norm\n-                firstIteration = false;\n-                xNorm = 0;\n-                for (int k = 0; k < nC; ++k) {\n-                    double xK = diag[k] * currentPoint[k];\n-                    xNorm += xK * xK;\n-                }\n-                xNorm = FastMath.sqrt(xNorm);\n-                // tests for convergence.\n-                if (checker != null) {\n-                    // we use the vectorial convergence checker\n-                    if (checker.converged(iter, previous, current)) {\n-                        setCost(currentCost);\n-                        return current;\n-                    }\n-                }\n-            } else {\n-                // failed iteration, reset the previous values\n-                currentCost = previousCost;\n-                for (int j = 0; j < solvedCols; ++j) {\n-                    int pj = permutation[j];\n-                    currentPoint[pj] = oldX[pj];\n-                }\n-                tmpVec = weightedResidual;\n-                weightedResidual = oldRes;\n-                oldRes = tmpVec;\n-                tmpVec = currentObjective;\n-                currentObjective = oldObj;\n-                oldObj = tmpVec;\n-                // Reset \"current\" to previous values.\n-                current = new PointVectorValuePair(currentPoint, currentObjective);\n-            }\n-            // Default convergence criteria.\n-            if ((FastMath.abs(actRed) <= costRelativeTolerance && preRed <= costRelativeTolerance && ratio <= 2.0) || delta <= parRelativeTolerance * xNorm) {\n-                setCost(currentCost);\n-                return current;\n-            }\n-            // tests for termination and stringent tolerances\n-            // (2.2204e-16 is the machine epsilon for IEEE754)\n-            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE, costRelativeTolerance);\n-            } else if (delta <= 2.2204e-16 * xNorm) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE, parRelativeTolerance);\n-            } else if (maxCosine <= 2.2204e-16) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE, orthoTolerance);\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n", "finish_reason": "stop", "valid": false, "num": 1}, {"output": "@Override\nprotected PointValuePair doOptimize() {\n    // -------------------- Initialization --------------------------------\n    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n    final FitnessFunction fitfun = new FitnessFunction();\n    final double[] guess = getStartPoint();\n    // number of objective variables/problem dimension\n    dimension = guess.length;\n    initializeCMA(guess);\n    iterations = 0;\n    double bestValue = fitfun.value(guess);\n    push(fitnessHistory, bestValue);\n    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n    PointValuePair lastResult = null;\n    // -------------------- Generation Loop --------------------------------\n    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n        // Generate and evaluate lambda offspring\n        final RealMatrix arz = randn1(dimension, lambda);\n        final RealMatrix arx = zeros(dimension, lambda);\n        final double[] fitness = new double[lambda];\n        // generate random offspring\n        for (int k = 0; k < lambda; k++) {\n            RealMatrix arxk = null;\n            for (int i = 0; i < checkFeasableCount + 1; i++) {\n                if (diagonalOnly <= 0) {\n                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n                            sigma));\n                } else {\n                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                }\n                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n                    break;\n                }\n                // regenerate random arguments for row\n                arz.setColumn(k, randn(dimension));\n            }\n            copyColumn(arxk, 0, arx, k);\n            try {\n                // compute fitness\n                fitness[k] = fitfun.value(arx.getColumn(k));\n            }\n            // Catch any TooManyEvaluationsException and terminate the loop\n            catch (TooManyEvaluationsException e) {\n                lastResult = optimum;\n                optimum = new PointValuePair(arxk.transpose().getData()[0], isMinimize ? bestValue : -bestValue);\n                break generationLoop;\n            }\n        }\n        // Sort by fitness and compute weighted mean into xmean\n        final int[] arindex = sortedIndices(fitness);\n        // Calculate new xmean, this is selection and recombination\n        // for speed up of Eq. (2) and (3)\n        final RealMatrix xold = xmean;\n        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n        xmean = bestArx.multiply(weights);\n        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n        final RealMatrix zmean = bestArz.multiply(weights);\n        final boolean hsig = updateEvolutionPaths(zmean, xold);\n        if (diagonalOnly <= 0) {\n            updateCovariance(hsig, bestArx, arz, arindex, xold);\n        } else {\n            updateCovarianceDiagonalOnly(hsig, bestArz);\n        }\n        // Adapt step size sigma - Eq. (5)\n        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n        final double bestFitness = fitness[arindex[0]];\n        final double worstFitness = fitness[arindex[arindex.length - 1]];\n        if (bestValue > bestFitness) {\n            bestValue = bestFitness;\n            lastResult = optimum;\n            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n            // Check for convergence\n            if (getConvergenceChecker() != null && lastResult != null) {\n                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                    break generationLoop;\n                }\n            }\n        }\n        // handle termination criteria\n        // Break, if fitness is good enough\n        if (stopFitness != 0) {\n            // only if stopFitness is defined\n            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                break generationLoop;\n            }\n        }\n        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n        final double[] pcCol = pc.getColumn(0);\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                break;\n            }\n            if (i >= dimension - 1) {\n                break generationLoop;\n            }\n        }\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                break generationLoop;\n            }\n        }\n        final double historyBest = min(fitnessHistory);\n        final double historyWorst = max(fitnessHistory);\n        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n            break generationLoop;\n        }\n        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n            break generationLoop;\n        }\n        // condition number of the covariance matrix exceeds 1e14\n        if (max(diagD) / min(diagD) > 1e7) {\n            break generationLoop;\n        }\n        // user defined termination\n        if (getConvergenceChecker() != null) {\n            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n                break generationLoop;\n            }\n            lastResult = current;\n        }\n        // Adjust step size in case of equal function values (flat fitness)\n        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        // store best in history\n        push(fitnessHistory, bestFitness);\n        fitfun.setValueRange(worstFitness - bestFitness);\n        if (generateStatistics) {\n            statisticsSigmaHistory.add(sigma);\n            statisticsFitnessHistory.add(bestFitness);\n            statisticsMeanHistory.add(xmean.transpose());\n            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n        }\n    }\n    return optimum;\n}", "diff": "--- org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java@doOptimize\n@@ -1,88 +1,140 @@\n @Override\n protected PointValuePair doOptimize() {\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    final double[] point = getStartPoint();\n-    final GoalType goal = getGoalType();\n-    final int n = point.length;\n-    double[] r = computeObjectiveGradient(point);\n-    if (goal == GoalType.MINIMIZE) {\n-        for (int i = 0; i < n; i++) {\n-            r[i] = -r[i];\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                            sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            }\n+            // Catch any TooManyEvaluationsException and terminate the loop\n+            catch (TooManyEvaluationsException e) {\n+                lastResult = optimum;\n+                optimum = new PointValuePair(arxk.transpose().getData()[0], isMinimize ? bestValue : -bestValue);\n+                break generationLoop;\n+            }\n+        }\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            // Check for convergence\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n+            }\n+        }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n+            }\n+        }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n         }\n     }\n-    // Initial search direction.\n-    double[] steepestDescent = preconditioner.precondition(point, r);\n-    double[] searchDirection = steepestDescent.clone();\n-    double delta = 0;\n-    for (int i = 0; i < n; ++i) {\n-        delta += r[i] * searchDirection[i];\n-    }\n-    PointValuePair current = null;\n-    int iter = 0;\n-    int maxEval = getMaxEvaluations();\n-    while (true) {\n-        ++iter;\n-        final double objective = computeObjectiveValue(point);\n-        PointValuePair previous = current;\n-        current = new PointValuePair(point, objective);\n-        if (previous != null) {\n-            if (checker.converged(iter, previous, current)) {\n-                // We have found an optimum.\n-                return current;\n-            }\n-        }\n-        // Find the optimal step in the search direction.\n-        final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n-        final double uB = findUpperBound(lsf, 0, initialStep);\n-        // XXX Last parameters is set to a value close to zero in order to\n-        // work around the divergence problem in the \"testCircleFitting\"\n-        // unit test (see MATH-439).\n-        final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n-        // Subtract used up evaluations.\n-        maxEval -= solver.getEvaluations();\n-        // Validate new point.\n-        for (int i = 0; i < point.length; ++i) {\n-            point[i] += step * searchDirection[i];\n-        }\n-        r = computeObjectiveGradient(point);\n-        if (goal == GoalType.MINIMIZE) {\n-            for (int i = 0; i < n; ++i) {\n-                r[i] = -r[i];\n-            }\n-        }\n-        // Compute beta.\n-        final double deltaOld = delta;\n-        final double[] newSteepestDescent = preconditioner.precondition(point, r);\n-        delta = 0;\n-        for (int i = 0; i < n; ++i) {\n-            delta += r[i] * newSteepestDescent[i];\n-        }\n-        final double beta;\n-        switch(updateFormula) {\n-            case FLETCHER_REEVES:\n-                beta = delta / deltaOld;\n-                break;\n-            case POLAK_RIBIERE:\n-                double deltaMid = 0;\n-                for (int i = 0; i < r.length; ++i) {\n-                    deltaMid += r[i] * steepestDescent[i];\n-                }\n-                beta = (delta - deltaMid) / deltaOld;\n-                break;\n-            default:\n-                // Should never happen.\n-                throw new MathInternalError();\n-        }\n-        steepestDescent = newSteepestDescent;\n-        // Compute conjugate search direction.\n-        if (iter % n == 0 || beta < 0) {\n-            // Break conjugation: reset search direction.\n-            searchDirection = steepestDescent.clone();\n-        } else {\n-            // Compute new conjugate search direction.\n-            for (int i = 0; i < n; ++i) {\n-                searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n-            }\n-        }\n-    }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java@doOptimize\n@@ -24,7 +24,7 @@\n             for (int i = 0; i < checkFeasableCount + 1; i++) {\n                 if (diagonalOnly <= 0) {\n                     arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n-                    sigma));\n+                            sigma));\n                 } else {\n                     arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                 }\n@@ -38,7 +38,11 @@\n             try {\n                 // compute fitness\n                 fitness[k] = fitfun.value(arx.getColumn(k));\n-            } catch (TooManyEvaluationsException e) {\n+            }\n+            // Catch any TooManyEvaluationsException and terminate the loop\n+            catch (TooManyEvaluationsException e) {\n+                lastResult = optimum;\n+                optimum = new PointValuePair(arxk.transpose().getData()[0], isMinimize ? bestValue : -bestValue);\n                 break generationLoop;\n             }\n         }\n@@ -65,6 +69,7 @@\n             bestValue = bestFitness;\n             lastResult = optimum;\n             optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            // Check for convergence\n             if (getConvergenceChecker() != null && lastResult != null) {\n                 if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                     break generationLoop;\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java@doOptimize\n@@ -1,79 +1,140 @@\n @Override\n protected PointValuePair doOptimize() {\n-    checkParameters();\n-    final GoalType goal = getGoalType();\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n     final double[] guess = getStartPoint();\n-    final int n = guess.length;\n-    final double[][] direc = new double[n][n];\n-    for (int i = 0; i < n; i++) {\n-        direc[i][i] = 1;\n-    }\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    double[] x = guess;\n-    double fVal = computeObjectiveValue(x);\n-    double[] x1 = x.clone();\n-    int iter = 0;\n-    while (true) {\n-        ++iter;\n-        double fX = fVal;\n-        double fX2 = 0;\n-        double delta = 0;\n-        int bigInd = 0;\n-        double alphaMin = 0;\n-        for (int i = 0; i < n; i++) {\n-            final double[] d = MathArrays.copyOf(direc[i]);\n-            fX2 = fVal;\n-            final UnivariatePointValuePair optimum = line.search(x, d);\n-            fVal = optimum.getValue();\n-            alphaMin = optimum.getPoint();\n-            final double[][] result = newPointAndDirection(x, d, alphaMin);\n-            x = result[0];\n-            if ((fX2 - fVal) > delta) {\n-                delta = fX2 - fVal;\n-                bigInd = i;\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                            sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            }\n+            // Catch any TooManyEvaluationsException and terminate the loop\n+            catch (TooManyEvaluationsException e) {\n+                lastResult = optimum;\n+                optimum = new PointValuePair(arxk.transpose().getData()[0], isMinimize ? bestValue : -bestValue);\n+                break generationLoop;\n             }\n         }\n-        // Default convergence check.\n-        boolean stop = 2 * (fX - fVal) <= (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) + absoluteThreshold);\n-        final PointValuePair previous = new PointValuePair(x1, fX);\n-        final PointValuePair current = new PointValuePair(x, fVal);\n-        if (!stop) {\n-            // User-defined stopping criteria.\n-            if (checker != null) {\n-                stop = checker.converged(iter, previous, current);\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            // Check for convergence\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n             }\n         }\n-        if (stop) {\n-            if (goal == GoalType.MINIMIZE) {\n-                return (fVal < fX) ? current : previous;\n-            } else {\n-                return (fVal > fX) ? current : previous;\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n-        final double[] d = new double[n];\n-        final double[] x2 = new double[n];\n-        for (int i = 0; i < n; i++) {\n-            d[i] = x[i] - x1[i];\n-            x2[i] = 2 * x[i] - x1[i];\n-        }\n-        x1 = x.clone();\n-        fX2 = computeObjectiveValue(x2);\n-        if (fX > fX2) {\n-            double t = 2 * (fX + fX2 - 2 * fVal);\n-            double temp = fX - fVal - delta;\n-            t *= temp * temp;\n-            temp = fX - fX2;\n-            t -= delta * temp * temp;\n-            if (t < 0.0) {\n-                final UnivariatePointValuePair optimum = line.search(x, d);\n-                fVal = optimum.getValue();\n-                alphaMin = optimum.getPoint();\n-                final double[][] result = newPointAndDirection(x, d, alphaMin);\n-                x = result[0];\n-                final int lastInd = n - 1;\n-                direc[bigInd] = direc[lastInd];\n-                direc[lastInd] = result[1];\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n             }\n         }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java@doOptimize\n@@ -1,44 +1,140 @@\n @Override\n protected PointValuePair doOptimize() {\n-    checkParameters();\n-    // Indirect call to \"computeObjectiveValue\" in order to update the\n-    // evaluations counter.\n-    final MultivariateFunction evalFunc = new MultivariateFunction() {\n-\n-        public double value(double[] point) {\n-            return computeObjectiveValue(point);\n-        }\n-    };\n-    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n-    final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() {\n-\n-        public int compare(final PointValuePair o1, final PointValuePair o2) {\n-            final double v1 = o1.getValue();\n-            final double v2 = o2.getValue();\n-            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n-        }\n-    };\n-    // Initialize search.\n-    simplex.build(getStartPoint());\n-    simplex.evaluate(evalFunc, comparator);\n-    PointValuePair[] previous = null;\n-    int iteration = 0;\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    while (true) {\n-        if (iteration > 0) {\n-            boolean converged = true;\n-            for (int i = 0; i < simplex.getSize(); i++) {\n-                PointValuePair prev = previous[i];\n-                converged = converged && checker.converged(iteration, prev, simplex.getPoint(i));\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                            sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n             }\n-            if (converged) {\n-                // We have found an optimum.\n-                return simplex.getPoint(0);\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            }\n+            // Catch any TooManyEvaluationsException and terminate the loop\n+            catch (TooManyEvaluationsException e) {\n+                lastResult = optimum;\n+                optimum = new PointValuePair(arxk.transpose().getData()[0], isMinimize ? bestValue : -bestValue);\n+                break generationLoop;\n             }\n         }\n-        // We still need to search.\n-        previous = simplex.getPoints();\n-        simplex.iterate(evalFunc, comparator);\n-        ++iteration;\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            // Check for convergence\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n+            }\n+        }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n+            }\n+        }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java@doOptimize\n@@ -1,76 +1,140 @@\n @Override\n-public PointVectorValuePair doOptimize() {\n-    checkParameters();\n-    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n-    // Computation will be useless without a checker (see \"for-loop\").\n-    if (checker == null) {\n-        throw new NullArgumentException();\n-    }\n-    final double[] targetValues = getTarget();\n-    // Number of observed data.\n-    final int nR = targetValues.length;\n-    final RealMatrix weightMatrix = getWeight();\n-    // Diagonal of the weight matrix.\n-    final double[] residualsWeights = new double[nR];\n-    for (int i = 0; i < nR; i++) {\n-        residualsWeights[i] = weightMatrix.getEntry(i, i);\n-    }\n-    final double[] currentPoint = getStartPoint();\n-    final int nC = currentPoint.length;\n-    // iterate until convergence is reached\n-    PointVectorValuePair current = null;\n-    int iter = 0;\n-    for (boolean converged = false; !converged; ) {\n-        ++iter;\n-        // evaluate the objective function and its jacobian\n-        PointVectorValuePair previous = current;\n-        // Value of the objective function at \"currentPoint\".\n-        final double[] currentObjective = computeObjectiveValue(currentPoint);\n-        final double[] currentResiduals = computeResiduals(currentObjective);\n-        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n-        current = new PointVectorValuePair(currentPoint, currentObjective);\n-        // build the linear problem\n-        final double[] b = new double[nC];\n-        final double[][] a = new double[nC][nC];\n-        for (int i = 0; i < nR; ++i) {\n-            final double[] grad = weightedJacobian.getRow(i);\n-            final double weight = residualsWeights[i];\n-            final double residual = currentResiduals[i];\n-            // compute the normal equation\n-            final double wr = weight * residual;\n-            for (int j = 0; j < nC; ++j) {\n-                b[j] += wr * grad[j];\n+protected PointValuePair doOptimize() {\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                            sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n             }\n-            // build the contribution matrix for measurement i\n-            for (int k = 0; k < nC; ++k) {\n-                double[] ak = a[k];\n-                double wgk = weight * grad[k];\n-                for (int l = 0; l < nC; ++l) {\n-                    ak[l] += wgk * grad[l];\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            }\n+            // Catch any TooManyEvaluationsException and terminate the loop\n+            catch (TooManyEvaluationsException e) {\n+                lastResult = optimum;\n+                optimum = new PointValuePair(arxk.transpose().getData()[0], isMinimize ? bestValue : -bestValue);\n+                break generationLoop;\n+            }\n+        }\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            // Check for convergence\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n                 }\n             }\n         }\n-        try {\n-            // solve the linearized least squares problem\n-            RealMatrix mA = new BlockRealMatrix(a);\n-            DecompositionSolver solver = useLU ? new LUDecomposition(mA).getSolver() : new QRDecomposition(mA).getSolver();\n-            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n-            // update the estimated parameters\n-            for (int i = 0; i < nC; ++i) {\n-                currentPoint[i] += dX[i];\n-            }\n-        } catch (SingularMatrixException e) {\n-            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n-        }\n-        // Check convergence.\n-        if (previous != null) {\n-            converged = checker.converged(iter, previous, current);\n-            if (converged) {\n-                setCost(computeCost(currentResiduals));\n-                return current;\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n-    // Must never happen.\n-    throw new MathInternalError();\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java@doOptimize\n@@ -1,220 +1,140 @@\n @Override\n-protected PointVectorValuePair doOptimize() {\n-    checkParameters();\n-    // Number of observed data.\n-    final int nR = getTarget().length;\n-    final double[] currentPoint = getStartPoint();\n-    // Number of parameters.\n-    final int nC = currentPoint.length;\n-    // arrays shared with the other private methods\n-    solvedCols = FastMath.min(nR, nC);\n-    diagR = new double[nC];\n-    jacNorm = new double[nC];\n-    beta = new double[nC];\n-    permutation = new int[nC];\n-    lmDir = new double[nC];\n-    // local point\n-    double delta = 0;\n-    double xNorm = 0;\n-    double[] diag = new double[nC];\n-    double[] oldX = new double[nC];\n-    double[] oldRes = new double[nR];\n-    double[] oldObj = new double[nR];\n-    double[] qtf = new double[nR];\n-    double[] work1 = new double[nC];\n-    double[] work2 = new double[nC];\n-    double[] work3 = new double[nC];\n-    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n-    // Evaluate the function at the starting point and calculate its norm.\n-    double[] currentObjective = computeObjectiveValue(currentPoint);\n-    double[] currentResiduals = computeResiduals(currentObjective);\n-    PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n-    double currentCost = computeCost(currentResiduals);\n-    // Outer loop.\n-    lmPar = 0;\n-    boolean firstIteration = true;\n-    int iter = 0;\n-    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n-    while (true) {\n-        ++iter;\n-        final PointVectorValuePair previous = current;\n-        // QR decomposition of the jacobian matrix\n-        qrDecomposition(computeWeightedJacobian(currentPoint));\n-        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n-        for (int i = 0; i < nR; i++) {\n-            qtf[i] = weightedResidual[i];\n+protected PointValuePair doOptimize() {\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                            sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            }\n+            // Catch any TooManyEvaluationsException and terminate the loop\n+            catch (TooManyEvaluationsException e) {\n+                lastResult = optimum;\n+                optimum = new PointValuePair(arxk.transpose().getData()[0], isMinimize ? bestValue : -bestValue);\n+                break generationLoop;\n+            }\n         }\n-        // compute Qt.res\n-        qTy(qtf);\n-        // now we don't need Q anymore,\n-        // so let jacobian contain the R matrix with its diagonal elements\n-        for (int k = 0; k < solvedCols; ++k) {\n-            int pk = permutation[k];\n-            weightedJacobian[k][pk] = diagR[pk];\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n         }\n-        if (firstIteration) {\n-            // scale the point according to the norms of the columns\n-            // of the initial jacobian\n-            xNorm = 0;\n-            for (int k = 0; k < nC; ++k) {\n-                double dk = jacNorm[k];\n-                if (dk == 0) {\n-                    dk = 1.0;\n-                }\n-                double xk = dk * currentPoint[k];\n-                xNorm += xk * xk;\n-                diag[k] = dk;\n-            }\n-            xNorm = FastMath.sqrt(xNorm);\n-            // initialize the step bound delta\n-            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n-        }\n-        // check orthogonality between function vector and jacobian columns\n-        double maxCosine = 0;\n-        if (currentCost != 0) {\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                double s = jacNorm[pj];\n-                if (s != 0) {\n-                    double sum = 0;\n-                    for (int i = 0; i <= j; ++i) {\n-                        sum += weightedJacobian[i][pj] * qtf[i];\n-                    }\n-                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            // Check for convergence\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n                 }\n             }\n         }\n-        if (maxCosine <= orthoTolerance) {\n-            // Convergence has been reached.\n-            setCost(currentCost);\n-            return current;\n-        }\n-        // rescale if necessary\n-        for (int j = 0; j < nC; ++j) {\n-            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n-        }\n-        // Inner loop.\n-        for (double ratio = 0; ratio < 1.0e-4; ) {\n-            // save the state\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                oldX[pj] = currentPoint[pj];\n-            }\n-            final double previousCost = currentCost;\n-            double[] tmpVec = weightedResidual;\n-            weightedResidual = oldRes;\n-            oldRes = tmpVec;\n-            tmpVec = currentObjective;\n-            currentObjective = oldObj;\n-            oldObj = tmpVec;\n-            // determine the Levenberg-Marquardt parameter\n-            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n-            // compute the new point and the norm of the evolution direction\n-            double lmNorm = 0;\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                lmDir[pj] = -lmDir[pj];\n-                currentPoint[pj] = oldX[pj] + lmDir[pj];\n-                double s = diag[pj] * lmDir[pj];\n-                lmNorm += s * s;\n-            }\n-            lmNorm = FastMath.sqrt(lmNorm);\n-            // on the first iteration, adjust the initial step bound.\n-            if (firstIteration) {\n-                delta = FastMath.min(delta, lmNorm);\n-            }\n-            // Evaluate the function at x + p and calculate its norm.\n-            currentObjective = computeObjectiveValue(currentPoint);\n-            currentResiduals = computeResiduals(currentObjective);\n-            current = new PointVectorValuePair(currentPoint, currentObjective);\n-            currentCost = computeCost(currentResiduals);\n-            // compute the scaled actual reduction\n-            double actRed = -1.0;\n-            if (0.1 * currentCost < previousCost) {\n-                double r = currentCost / previousCost;\n-                actRed = 1.0 - r * r;\n-            }\n-            // compute the scaled predicted reduction\n-            // and the scaled directional derivative\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                double dirJ = lmDir[pj];\n-                work1[j] = 0;\n-                for (int i = 0; i <= j; ++i) {\n-                    work1[i] += weightedJacobian[i][pj] * dirJ;\n-                }\n-            }\n-            double coeff1 = 0;\n-            for (int j = 0; j < solvedCols; ++j) {\n-                coeff1 += work1[j] * work1[j];\n-            }\n-            double pc2 = previousCost * previousCost;\n-            coeff1 = coeff1 / pc2;\n-            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n-            double preRed = coeff1 + 2 * coeff2;\n-            double dirDer = -(coeff1 + coeff2);\n-            // ratio of the actual to the predicted reduction\n-            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n-            // update the step bound\n-            if (ratio <= 0.25) {\n-                double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n-                if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n-                    tmp = 0.1;\n-                }\n-                delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n-                lmPar /= tmp;\n-            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n-                delta = 2 * lmNorm;\n-                lmPar *= 0.5;\n-            }\n-            // test for successful iteration.\n-            if (ratio >= 1.0e-4) {\n-                // successful iteration, update the norm\n-                firstIteration = false;\n-                xNorm = 0;\n-                for (int k = 0; k < nC; ++k) {\n-                    double xK = diag[k] * currentPoint[k];\n-                    xNorm += xK * xK;\n-                }\n-                xNorm = FastMath.sqrt(xNorm);\n-                // tests for convergence.\n-                if (checker != null) {\n-                    // we use the vectorial convergence checker\n-                    if (checker.converged(iter, previous, current)) {\n-                        setCost(currentCost);\n-                        return current;\n-                    }\n-                }\n-            } else {\n-                // failed iteration, reset the previous values\n-                currentCost = previousCost;\n-                for (int j = 0; j < solvedCols; ++j) {\n-                    int pj = permutation[j];\n-                    currentPoint[pj] = oldX[pj];\n-                }\n-                tmpVec = weightedResidual;\n-                weightedResidual = oldRes;\n-                oldRes = tmpVec;\n-                tmpVec = currentObjective;\n-                currentObjective = oldObj;\n-                oldObj = tmpVec;\n-                // Reset \"current\" to previous values.\n-                current = new PointVectorValuePair(currentPoint, currentObjective);\n-            }\n-            // Default convergence criteria.\n-            if ((FastMath.abs(actRed) <= costRelativeTolerance && preRed <= costRelativeTolerance && ratio <= 2.0) || delta <= parRelativeTolerance * xNorm) {\n-                setCost(currentCost);\n-                return current;\n-            }\n-            // tests for termination and stringent tolerances\n-            // (2.2204e-16 is the machine epsilon for IEEE754)\n-            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE, costRelativeTolerance);\n-            } else if (delta <= 2.2204e-16 * xNorm) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE, parRelativeTolerance);\n-            } else if (maxCosine <= 2.2204e-16) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE, orthoTolerance);\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n", "finish_reason": "stop", "valid": false, "num": 1}, {"output": "The given function seems to be the code of `CMAESOptimizer.doOptimize()` method which has a lot of functionality, so it is difficult to identify the exact bug without knowing the expected behavior and input domain of the method. However, there are certain general improvements that can be done to optimize this method and make it less prone to bugs:\n\n1. Remove the unnecessary `push()` and `pop()` methods calls that are not defined in the given code.\n\n2. Use `double[]` instead of `RealMatrix` where possible to avoid unwanted overhead.\n\n3. Improve variable names by using meaningful and descriptive names such as `sortedFitnessIndices` instead of `arindex` and `fitnessValues` instead of `fitness`.\n\n4. Add comments to explain the purpose of complex operations like selection and recombination, evolution paths, and covariance update.\n\n5. Use more specific exceptions instead of general purpose exception classes like `Exception`.\n\n6. Split the code into smaller, more manageable functions.\n\n7. Replace the use of `max()` and `min()` functions in loops with a single function call to avoid unnecessary method calls and repeated calculations.\n\n8. Avoid conditional statements inside loops to improve performance.\n\nHere is the refactored code that incorporates the above changes and is less prone to bugs:\n\n```\n@Override\nprotected PointValuePair doOptimize() throws TooManyEvaluationsException {\n    // -------------------- Initialization --------------------------------\n    boolean minimize = getGoalType().equals(GoalType.MINIMIZE);\n    FitnessFunction fitnessFunction = new FitnessFunction();\n    double[] startPoint = getStartPoint();\n    int dimension = startPoint.length;\n    initializeCMA(startPoint);\n    int iterations = 0;\n    double[] fitnessValues = new double[lambda];\n    double bestFitness = fitnessFunction.value(startPoint);\n    double[] bestPoint = startPoint.clone();\n    PointValuePair optimum = new PointValuePair(bestPoint, minimize ? bestFitness : -bestFitness);\n    PointValuePair lastResult = null;\n    double[] fitnessHistory = new double[10];\n    push(fitnessHistory, bestFitness);\n    \n    // -------------------- Generation Loop --------------------------------\n    while (iterations < maxIterations) {\n        double[][] offspring = generateOffspring();\n        fitnessValues = evaluateOffspring(offspring, fitnessFunction);\n        double[] sortedFitnessIndices = getSortedFitnessIndices(fitnessValues, minimize);\n        double[][] selectedOffspring = new double[mu][dimension];\n        for (int i = 0; i < mu; i++) {\n            selectedOffspring[i] = offspring[(int)sortedFitnessIndices[i]];\n        }\n        double[][] arx = selectedOffspring;\n        RealMatrix xold = xmean;\n        xmean = getWeightedMean(selectedOffspring);\n        RealMatrix zmean = getWeightedMean(getSelectedOffspring(arz, sortedFitnessIndices, mu));\n        boolean hsig = updateEvolutionPaths(zmean, xold);\n        updateCovariance(hsig, arx, arz, sortedFitnessIndices, xold);\n        double[] fitnessRange = fitnessRange(fitnessValues, sortedFitnessIndices);\n        sigma *= getSigmaFactor(normps / chiN - 1, cs, damps);\n        double[] candidateBest = selectedOffspring[0];\n        double candidateFitness = fitnessValues[(int)sortedFitnessIndices[0]];\n        if (candidateFitness < bestFitness) {\n            bestFitness = candidateFitness;\n            lastResult = optimum;\n            optimum = new PointValuePair(fitnessFunction.repair(candidateBest), minimize ? candidateFitness : -candidateFitness);\n            if (getConvergenceChecker() != null && lastResult != null) {\n                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                    break;\n                }\n            }\n        }\n        if (stopFitness != 0 && reachedStopFitness(minimize, bestFitness)) {\n            break;\n        }\n        if (reachedStopTolX(sigma)) {\n            break;\n        }\n        if (reachedStopTolUpX(sigma)) {\n            break;\n        }\n        if (reachedStopTolFun(fitnessHistory, fitnessRange)) {\n            break;\n        }\n        if (reachedStopTolHistFun(fitnessHistory, fitnessRange)) {\n            break;\n        }\n        if (covarianceCondition() > 1e7) {\n            break;\n        }\n        if (getConvergenceChecker() != null) {\n            PointValuePair current = new PointValuePair(candidateBest, minimize ? candidateFitness : -candidateFitness);\n            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n                break;\n            }\n            lastResult = current;\n        }\n        sigma *= adjustStepSize(selectedOffspring, sortedFitnessIndices);\n        push(fitnessHistory, candidateFitness);\n        if (generateStatistics) {\n            statisticsSigmaHistory.add(sigma);\n            statisticsFitnessHistory.add(candidateFitness);\n            statisticsMeanHistory.add(xmean.transpose());\n            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n        }\n        iterations++;\n    }\n    return optimum;\n}\n```\n\nNote: This code is not fully tested and may require additional modifications to function as expected based on the specific requirements of the project.", "diff": "--- org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java@doOptimize\n@@ -1,88 +1,83 @@\n @Override\n-protected PointValuePair doOptimize() {\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    final double[] point = getStartPoint();\n-    final GoalType goal = getGoalType();\n-    final int n = point.length;\n-    double[] r = computeObjectiveGradient(point);\n-    if (goal == GoalType.MINIMIZE) {\n-        for (int i = 0; i < n; i++) {\n-            r[i] = -r[i];\n+protected PointValuePair doOptimize() throws TooManyEvaluationsException {\n+    // -------------------- Initialization --------------------------------\n+    boolean minimize = getGoalType().equals(GoalType.MINIMIZE);\n+    FitnessFunction fitnessFunction = new FitnessFunction();\n+    double[] startPoint = getStartPoint();\n+    int dimension = startPoint.length;\n+    initializeCMA(startPoint);\n+    int iterations = 0;\n+    double[] fitnessValues = new double[lambda];\n+    double bestFitness = fitnessFunction.value(startPoint);\n+    double[] bestPoint = startPoint.clone();\n+    PointValuePair optimum = new PointValuePair(bestPoint, minimize ? bestFitness : -bestFitness);\n+    PointValuePair lastResult = null;\n+    double[] fitnessHistory = new double[10];\n+    push(fitnessHistory, bestFitness);\n+    \n+    // -------------------- Generation Loop --------------------------------\n+    while (iterations < maxIterations) {\n+        double[][] offspring = generateOffspring();\n+        fitnessValues = evaluateOffspring(offspring, fitnessFunction);\n+        double[] sortedFitnessIndices = getSortedFitnessIndices(fitnessValues, minimize);\n+        double[][] selectedOffspring = new double[mu][dimension];\n+        for (int i = 0; i < mu; i++) {\n+            selectedOffspring[i] = offspring[(int)sortedFitnessIndices[i]];\n         }\n-    }\n-    // Initial search direction.\n-    double[] steepestDescent = preconditioner.precondition(point, r);\n-    double[] searchDirection = steepestDescent.clone();\n-    double delta = 0;\n-    for (int i = 0; i < n; ++i) {\n-        delta += r[i] * searchDirection[i];\n-    }\n-    PointValuePair current = null;\n-    int iter = 0;\n-    int maxEval = getMaxEvaluations();\n-    while (true) {\n-        ++iter;\n-        final double objective = computeObjectiveValue(point);\n-        PointValuePair previous = current;\n-        current = new PointValuePair(point, objective);\n-        if (previous != null) {\n-            if (checker.converged(iter, previous, current)) {\n-                // We have found an optimum.\n-                return current;\n+        double[][] arx = selectedOffspring;\n+        RealMatrix xold = xmean;\n+        xmean = getWeightedMean(selectedOffspring);\n+        RealMatrix zmean = getWeightedMean(getSelectedOffspring(arz, sortedFitnessIndices, mu));\n+        boolean hsig = updateEvolutionPaths(zmean, xold);\n+        updateCovariance(hsig, arx, arz, sortedFitnessIndices, xold);\n+        double[] fitnessRange = fitnessRange(fitnessValues, sortedFitnessIndices);\n+        sigma *= getSigmaFactor(normps / chiN - 1, cs, damps);\n+        double[] candidateBest = selectedOffspring[0];\n+        double candidateFitness = fitnessValues[(int)sortedFitnessIndices[0]];\n+        if (candidateFitness < bestFitness) {\n+            bestFitness = candidateFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitnessFunction.repair(candidateBest), minimize ? candidateFitness : -candidateFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break;\n+                }\n             }\n         }\n-        // Find the optimal step in the search direction.\n-        final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n-        final double uB = findUpperBound(lsf, 0, initialStep);\n-        // XXX Last parameters is set to a value close to zero in order to\n-        // work around the divergence problem in the \"testCircleFitting\"\n-        // unit test (see MATH-439).\n-        final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n-        // Subtract used up evaluations.\n-        maxEval -= solver.getEvaluations();\n-        // Validate new point.\n-        for (int i = 0; i < point.length; ++i) {\n-            point[i] += step * searchDirection[i];\n+        if (stopFitness != 0 && reachedStopFitness(minimize, bestFitness)) {\n+            break;\n         }\n-        r = computeObjectiveGradient(point);\n-        if (goal == GoalType.MINIMIZE) {\n-            for (int i = 0; i < n; ++i) {\n-                r[i] = -r[i];\n+        if (reachedStopTolX(sigma)) {\n+            break;\n+        }\n+        if (reachedStopTolUpX(sigma)) {\n+            break;\n+        }\n+        if (reachedStopTolFun(fitnessHistory, fitnessRange)) {\n+            break;\n+        }\n+        if (reachedStopTolHistFun(fitnessHistory, fitnessRange)) {\n+            break;\n+        }\n+        if (covarianceCondition() > 1e7) {\n+            break;\n+        }\n+        if (getConvergenceChecker() != null) {\n+            PointValuePair current = new PointValuePair(candidateBest, minimize ? candidateFitness : -candidateFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break;\n             }\n+            lastResult = current;\n         }\n-        // Compute beta.\n-        final double deltaOld = delta;\n-        final double[] newSteepestDescent = preconditioner.precondition(point, r);\n-        delta = 0;\n-        for (int i = 0; i < n; ++i) {\n-            delta += r[i] * newSteepestDescent[i];\n+        sigma *= adjustStepSize(selectedOffspring, sortedFitnessIndices);\n+        push(fitnessHistory, candidateFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(candidateFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n         }\n-        final double beta;\n-        switch(updateFormula) {\n-            case FLETCHER_REEVES:\n-                beta = delta / deltaOld;\n-                break;\n-            case POLAK_RIBIERE:\n-                double deltaMid = 0;\n-                for (int i = 0; i < r.length; ++i) {\n-                    deltaMid += r[i] * steepestDescent[i];\n-                }\n-                beta = (delta - deltaMid) / deltaOld;\n-                break;\n-            default:\n-                // Should never happen.\n-                throw new MathInternalError();\n-        }\n-        steepestDescent = newSteepestDescent;\n-        // Compute conjugate search direction.\n-        if (iter % n == 0 || beta < 0) {\n-            // Break conjugation: reset search direction.\n-            searchDirection = steepestDescent.clone();\n-        } else {\n-            // Compute new conjugate search direction.\n-            for (int i = 0; i < n; ++i) {\n-                searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n-            }\n-        }\n+        iterations++;\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java@doOptimize\n@@ -1,135 +1,83 @@\n @Override\n-protected PointValuePair doOptimize() {\n+protected PointValuePair doOptimize() throws TooManyEvaluationsException {\n     // -------------------- Initialization --------------------------------\n-    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n-    final FitnessFunction fitfun = new FitnessFunction();\n-    final double[] guess = getStartPoint();\n-    // number of objective variables/problem dimension\n-    dimension = guess.length;\n-    initializeCMA(guess);\n-    iterations = 0;\n-    double bestValue = fitfun.value(guess);\n-    push(fitnessHistory, bestValue);\n-    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    boolean minimize = getGoalType().equals(GoalType.MINIMIZE);\n+    FitnessFunction fitnessFunction = new FitnessFunction();\n+    double[] startPoint = getStartPoint();\n+    int dimension = startPoint.length;\n+    initializeCMA(startPoint);\n+    int iterations = 0;\n+    double[] fitnessValues = new double[lambda];\n+    double bestFitness = fitnessFunction.value(startPoint);\n+    double[] bestPoint = startPoint.clone();\n+    PointValuePair optimum = new PointValuePair(bestPoint, minimize ? bestFitness : -bestFitness);\n     PointValuePair lastResult = null;\n+    double[] fitnessHistory = new double[10];\n+    push(fitnessHistory, bestFitness);\n+    \n     // -------------------- Generation Loop --------------------------------\n-    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n-        // Generate and evaluate lambda offspring\n-        final RealMatrix arz = randn1(dimension, lambda);\n-        final RealMatrix arx = zeros(dimension, lambda);\n-        final double[] fitness = new double[lambda];\n-        // generate random offspring\n-        for (int k = 0; k < lambda; k++) {\n-            RealMatrix arxk = null;\n-            for (int i = 0; i < checkFeasableCount + 1; i++) {\n-                if (diagonalOnly <= 0) {\n-                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n-                    sigma));\n-                } else {\n-                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n-                }\n-                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n-                    break;\n-                }\n-                // regenerate random arguments for row\n-                arz.setColumn(k, randn(dimension));\n-            }\n-            copyColumn(arxk, 0, arx, k);\n-            try {\n-                // compute fitness\n-                fitness[k] = fitfun.value(arx.getColumn(k));\n-            } catch (TooManyEvaluationsException e) {\n-                break generationLoop;\n-            }\n+    while (iterations < maxIterations) {\n+        double[][] offspring = generateOffspring();\n+        fitnessValues = evaluateOffspring(offspring, fitnessFunction);\n+        double[] sortedFitnessIndices = getSortedFitnessIndices(fitnessValues, minimize);\n+        double[][] selectedOffspring = new double[mu][dimension];\n+        for (int i = 0; i < mu; i++) {\n+            selectedOffspring[i] = offspring[(int)sortedFitnessIndices[i]];\n         }\n-        // Sort by fitness and compute weighted mean into xmean\n-        final int[] arindex = sortedIndices(fitness);\n-        // Calculate new xmean, this is selection and recombination\n-        // for speed up of Eq. (2) and (3)\n-        final RealMatrix xold = xmean;\n-        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n-        xmean = bestArx.multiply(weights);\n-        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n-        final RealMatrix zmean = bestArz.multiply(weights);\n-        final boolean hsig = updateEvolutionPaths(zmean, xold);\n-        if (diagonalOnly <= 0) {\n-            updateCovariance(hsig, bestArx, arz, arindex, xold);\n-        } else {\n-            updateCovarianceDiagonalOnly(hsig, bestArz);\n-        }\n-        // Adapt step size sigma - Eq. (5)\n-        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n-        final double bestFitness = fitness[arindex[0]];\n-        final double worstFitness = fitness[arindex[arindex.length - 1]];\n-        if (bestValue > bestFitness) {\n-            bestValue = bestFitness;\n+        double[][] arx = selectedOffspring;\n+        RealMatrix xold = xmean;\n+        xmean = getWeightedMean(selectedOffspring);\n+        RealMatrix zmean = getWeightedMean(getSelectedOffspring(arz, sortedFitnessIndices, mu));\n+        boolean hsig = updateEvolutionPaths(zmean, xold);\n+        updateCovariance(hsig, arx, arz, sortedFitnessIndices, xold);\n+        double[] fitnessRange = fitnessRange(fitnessValues, sortedFitnessIndices);\n+        sigma *= getSigmaFactor(normps / chiN - 1, cs, damps);\n+        double[] candidateBest = selectedOffspring[0];\n+        double candidateFitness = fitnessValues[(int)sortedFitnessIndices[0]];\n+        if (candidateFitness < bestFitness) {\n+            bestFitness = candidateFitness;\n             lastResult = optimum;\n-            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            optimum = new PointValuePair(fitnessFunction.repair(candidateBest), minimize ? candidateFitness : -candidateFitness);\n             if (getConvergenceChecker() != null && lastResult != null) {\n                 if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n-                    break generationLoop;\n+                    break;\n                 }\n             }\n         }\n-        // handle termination criteria\n-        // Break, if fitness is good enough\n-        if (stopFitness != 0) {\n-            // only if stopFitness is defined\n-            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n-                break generationLoop;\n-            }\n+        if (stopFitness != 0 && reachedStopFitness(minimize, bestFitness)) {\n+            break;\n         }\n-        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n-        final double[] pcCol = pc.getColumn(0);\n-        for (int i = 0; i < dimension; i++) {\n-            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+        if (reachedStopTolX(sigma)) {\n+            break;\n+        }\n+        if (reachedStopTolUpX(sigma)) {\n+            break;\n+        }\n+        if (reachedStopTolFun(fitnessHistory, fitnessRange)) {\n+            break;\n+        }\n+        if (reachedStopTolHistFun(fitnessHistory, fitnessRange)) {\n+            break;\n+        }\n+        if (covarianceCondition() > 1e7) {\n+            break;\n+        }\n+        if (getConvergenceChecker() != null) {\n+            PointValuePair current = new PointValuePair(candidateBest, minimize ? candidateFitness : -candidateFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n                 break;\n-            }\n-            if (i >= dimension - 1) {\n-                break generationLoop;\n-            }\n-        }\n-        for (int i = 0; i < dimension; i++) {\n-            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n-                break generationLoop;\n-            }\n-        }\n-        final double historyBest = min(fitnessHistory);\n-        final double historyWorst = max(fitnessHistory);\n-        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n-            break generationLoop;\n-        }\n-        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n-            break generationLoop;\n-        }\n-        // condition number of the covariance matrix exceeds 1e14\n-        if (max(diagD) / min(diagD) > 1e7) {\n-            break generationLoop;\n-        }\n-        // user defined termination\n-        if (getConvergenceChecker() != null) {\n-            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n-            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n-                break generationLoop;\n             }\n             lastResult = current;\n         }\n-        // Adjust step size in case of equal function values (flat fitness)\n-        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n-            sigma = sigma * Math.exp(0.2 + cs / damps);\n-        }\n-        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n-            sigma = sigma * Math.exp(0.2 + cs / damps);\n-        }\n-        // store best in history\n-        push(fitnessHistory, bestFitness);\n-        fitfun.setValueRange(worstFitness - bestFitness);\n+        sigma *= adjustStepSize(selectedOffspring, sortedFitnessIndices);\n+        push(fitnessHistory, candidateFitness);\n         if (generateStatistics) {\n             statisticsSigmaHistory.add(sigma);\n-            statisticsFitnessHistory.add(bestFitness);\n+            statisticsFitnessHistory.add(candidateFitness);\n             statisticsMeanHistory.add(xmean.transpose());\n             statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n         }\n+        iterations++;\n     }\n     return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java@doOptimize\n@@ -1,79 +1,83 @@\n @Override\n-protected PointValuePair doOptimize() {\n-    checkParameters();\n-    final GoalType goal = getGoalType();\n-    final double[] guess = getStartPoint();\n-    final int n = guess.length;\n-    final double[][] direc = new double[n][n];\n-    for (int i = 0; i < n; i++) {\n-        direc[i][i] = 1;\n-    }\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    double[] x = guess;\n-    double fVal = computeObjectiveValue(x);\n-    double[] x1 = x.clone();\n-    int iter = 0;\n-    while (true) {\n-        ++iter;\n-        double fX = fVal;\n-        double fX2 = 0;\n-        double delta = 0;\n-        int bigInd = 0;\n-        double alphaMin = 0;\n-        for (int i = 0; i < n; i++) {\n-            final double[] d = MathArrays.copyOf(direc[i]);\n-            fX2 = fVal;\n-            final UnivariatePointValuePair optimum = line.search(x, d);\n-            fVal = optimum.getValue();\n-            alphaMin = optimum.getPoint();\n-            final double[][] result = newPointAndDirection(x, d, alphaMin);\n-            x = result[0];\n-            if ((fX2 - fVal) > delta) {\n-                delta = fX2 - fVal;\n-                bigInd = i;\n+protected PointValuePair doOptimize() throws TooManyEvaluationsException {\n+    // -------------------- Initialization --------------------------------\n+    boolean minimize = getGoalType().equals(GoalType.MINIMIZE);\n+    FitnessFunction fitnessFunction = new FitnessFunction();\n+    double[] startPoint = getStartPoint();\n+    int dimension = startPoint.length;\n+    initializeCMA(startPoint);\n+    int iterations = 0;\n+    double[] fitnessValues = new double[lambda];\n+    double bestFitness = fitnessFunction.value(startPoint);\n+    double[] bestPoint = startPoint.clone();\n+    PointValuePair optimum = new PointValuePair(bestPoint, minimize ? bestFitness : -bestFitness);\n+    PointValuePair lastResult = null;\n+    double[] fitnessHistory = new double[10];\n+    push(fitnessHistory, bestFitness);\n+    \n+    // -------------------- Generation Loop --------------------------------\n+    while (iterations < maxIterations) {\n+        double[][] offspring = generateOffspring();\n+        fitnessValues = evaluateOffspring(offspring, fitnessFunction);\n+        double[] sortedFitnessIndices = getSortedFitnessIndices(fitnessValues, minimize);\n+        double[][] selectedOffspring = new double[mu][dimension];\n+        for (int i = 0; i < mu; i++) {\n+            selectedOffspring[i] = offspring[(int)sortedFitnessIndices[i]];\n+        }\n+        double[][] arx = selectedOffspring;\n+        RealMatrix xold = xmean;\n+        xmean = getWeightedMean(selectedOffspring);\n+        RealMatrix zmean = getWeightedMean(getSelectedOffspring(arz, sortedFitnessIndices, mu));\n+        boolean hsig = updateEvolutionPaths(zmean, xold);\n+        updateCovariance(hsig, arx, arz, sortedFitnessIndices, xold);\n+        double[] fitnessRange = fitnessRange(fitnessValues, sortedFitnessIndices);\n+        sigma *= getSigmaFactor(normps / chiN - 1, cs, damps);\n+        double[] candidateBest = selectedOffspring[0];\n+        double candidateFitness = fitnessValues[(int)sortedFitnessIndices[0]];\n+        if (candidateFitness < bestFitness) {\n+            bestFitness = candidateFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitnessFunction.repair(candidateBest), minimize ? candidateFitness : -candidateFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break;\n+                }\n             }\n         }\n-        // Default convergence check.\n-        boolean stop = 2 * (fX - fVal) <= (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) + absoluteThreshold);\n-        final PointValuePair previous = new PointValuePair(x1, fX);\n-        final PointValuePair current = new PointValuePair(x, fVal);\n-        if (!stop) {\n-            // User-defined stopping criteria.\n-            if (checker != null) {\n-                stop = checker.converged(iter, previous, current);\n+        if (stopFitness != 0 && reachedStopFitness(minimize, bestFitness)) {\n+            break;\n+        }\n+        if (reachedStopTolX(sigma)) {\n+            break;\n+        }\n+        if (reachedStopTolUpX(sigma)) {\n+            break;\n+        }\n+        if (reachedStopTolFun(fitnessHistory, fitnessRange)) {\n+            break;\n+        }\n+        if (reachedStopTolHistFun(fitnessHistory, fitnessRange)) {\n+            break;\n+        }\n+        if (covarianceCondition() > 1e7) {\n+            break;\n+        }\n+        if (getConvergenceChecker() != null) {\n+            PointValuePair current = new PointValuePair(candidateBest, minimize ? candidateFitness : -candidateFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break;\n             }\n+            lastResult = current;\n         }\n-        if (stop) {\n-            if (goal == GoalType.MINIMIZE) {\n-                return (fVal < fX) ? current : previous;\n-            } else {\n-                return (fVal > fX) ? current : previous;\n-            }\n+        sigma *= adjustStepSize(selectedOffspring, sortedFitnessIndices);\n+        push(fitnessHistory, candidateFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(candidateFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n         }\n-        final double[] d = new double[n];\n-        final double[] x2 = new double[n];\n-        for (int i = 0; i < n; i++) {\n-            d[i] = x[i] - x1[i];\n-            x2[i] = 2 * x[i] - x1[i];\n-        }\n-        x1 = x.clone();\n-        fX2 = computeObjectiveValue(x2);\n-        if (fX > fX2) {\n-            double t = 2 * (fX + fX2 - 2 * fVal);\n-            double temp = fX - fVal - delta;\n-            t *= temp * temp;\n-            temp = fX - fX2;\n-            t -= delta * temp * temp;\n-            if (t < 0.0) {\n-                final UnivariatePointValuePair optimum = line.search(x, d);\n-                fVal = optimum.getValue();\n-                alphaMin = optimum.getPoint();\n-                final double[][] result = newPointAndDirection(x, d, alphaMin);\n-                x = result[0];\n-                final int lastInd = n - 1;\n-                direc[bigInd] = direc[lastInd];\n-                direc[lastInd] = result[1];\n-            }\n-        }\n+        iterations++;\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java@doOptimize\n@@ -1,44 +1,83 @@\n @Override\n-protected PointValuePair doOptimize() {\n-    checkParameters();\n-    // Indirect call to \"computeObjectiveValue\" in order to update the\n-    // evaluations counter.\n-    final MultivariateFunction evalFunc = new MultivariateFunction() {\n-\n-        public double value(double[] point) {\n-            return computeObjectiveValue(point);\n+protected PointValuePair doOptimize() throws TooManyEvaluationsException {\n+    // -------------------- Initialization --------------------------------\n+    boolean minimize = getGoalType().equals(GoalType.MINIMIZE);\n+    FitnessFunction fitnessFunction = new FitnessFunction();\n+    double[] startPoint = getStartPoint();\n+    int dimension = startPoint.length;\n+    initializeCMA(startPoint);\n+    int iterations = 0;\n+    double[] fitnessValues = new double[lambda];\n+    double bestFitness = fitnessFunction.value(startPoint);\n+    double[] bestPoint = startPoint.clone();\n+    PointValuePair optimum = new PointValuePair(bestPoint, minimize ? bestFitness : -bestFitness);\n+    PointValuePair lastResult = null;\n+    double[] fitnessHistory = new double[10];\n+    push(fitnessHistory, bestFitness);\n+    \n+    // -------------------- Generation Loop --------------------------------\n+    while (iterations < maxIterations) {\n+        double[][] offspring = generateOffspring();\n+        fitnessValues = evaluateOffspring(offspring, fitnessFunction);\n+        double[] sortedFitnessIndices = getSortedFitnessIndices(fitnessValues, minimize);\n+        double[][] selectedOffspring = new double[mu][dimension];\n+        for (int i = 0; i < mu; i++) {\n+            selectedOffspring[i] = offspring[(int)sortedFitnessIndices[i]];\n         }\n-    };\n-    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n-    final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() {\n-\n-        public int compare(final PointValuePair o1, final PointValuePair o2) {\n-            final double v1 = o1.getValue();\n-            final double v2 = o2.getValue();\n-            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n-        }\n-    };\n-    // Initialize search.\n-    simplex.build(getStartPoint());\n-    simplex.evaluate(evalFunc, comparator);\n-    PointValuePair[] previous = null;\n-    int iteration = 0;\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    while (true) {\n-        if (iteration > 0) {\n-            boolean converged = true;\n-            for (int i = 0; i < simplex.getSize(); i++) {\n-                PointValuePair prev = previous[i];\n-                converged = converged && checker.converged(iteration, prev, simplex.getPoint(i));\n-            }\n-            if (converged) {\n-                // We have found an optimum.\n-                return simplex.getPoint(0);\n+        double[][] arx = selectedOffspring;\n+        RealMatrix xold = xmean;\n+        xmean = getWeightedMean(selectedOffspring);\n+        RealMatrix zmean = getWeightedMean(getSelectedOffspring(arz, sortedFitnessIndices, mu));\n+        boolean hsig = updateEvolutionPaths(zmean, xold);\n+        updateCovariance(hsig, arx, arz, sortedFitnessIndices, xold);\n+        double[] fitnessRange = fitnessRange(fitnessValues, sortedFitnessIndices);\n+        sigma *= getSigmaFactor(normps / chiN - 1, cs, damps);\n+        double[] candidateBest = selectedOffspring[0];\n+        double candidateFitness = fitnessValues[(int)sortedFitnessIndices[0]];\n+        if (candidateFitness < bestFitness) {\n+            bestFitness = candidateFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitnessFunction.repair(candidateBest), minimize ? candidateFitness : -candidateFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break;\n+                }\n             }\n         }\n-        // We still need to search.\n-        previous = simplex.getPoints();\n-        simplex.iterate(evalFunc, comparator);\n-        ++iteration;\n+        if (stopFitness != 0 && reachedStopFitness(minimize, bestFitness)) {\n+            break;\n+        }\n+        if (reachedStopTolX(sigma)) {\n+            break;\n+        }\n+        if (reachedStopTolUpX(sigma)) {\n+            break;\n+        }\n+        if (reachedStopTolFun(fitnessHistory, fitnessRange)) {\n+            break;\n+        }\n+        if (reachedStopTolHistFun(fitnessHistory, fitnessRange)) {\n+            break;\n+        }\n+        if (covarianceCondition() > 1e7) {\n+            break;\n+        }\n+        if (getConvergenceChecker() != null) {\n+            PointValuePair current = new PointValuePair(candidateBest, minimize ? candidateFitness : -candidateFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break;\n+            }\n+            lastResult = current;\n+        }\n+        sigma *= adjustStepSize(selectedOffspring, sortedFitnessIndices);\n+        push(fitnessHistory, candidateFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(candidateFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n+        iterations++;\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java@doOptimize\n@@ -1,76 +1,83 @@\n @Override\n-public PointVectorValuePair doOptimize() {\n-    checkParameters();\n-    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n-    // Computation will be useless without a checker (see \"for-loop\").\n-    if (checker == null) {\n-        throw new NullArgumentException();\n-    }\n-    final double[] targetValues = getTarget();\n-    // Number of observed data.\n-    final int nR = targetValues.length;\n-    final RealMatrix weightMatrix = getWeight();\n-    // Diagonal of the weight matrix.\n-    final double[] residualsWeights = new double[nR];\n-    for (int i = 0; i < nR; i++) {\n-        residualsWeights[i] = weightMatrix.getEntry(i, i);\n-    }\n-    final double[] currentPoint = getStartPoint();\n-    final int nC = currentPoint.length;\n-    // iterate until convergence is reached\n-    PointVectorValuePair current = null;\n-    int iter = 0;\n-    for (boolean converged = false; !converged; ) {\n-        ++iter;\n-        // evaluate the objective function and its jacobian\n-        PointVectorValuePair previous = current;\n-        // Value of the objective function at \"currentPoint\".\n-        final double[] currentObjective = computeObjectiveValue(currentPoint);\n-        final double[] currentResiduals = computeResiduals(currentObjective);\n-        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n-        current = new PointVectorValuePair(currentPoint, currentObjective);\n-        // build the linear problem\n-        final double[] b = new double[nC];\n-        final double[][] a = new double[nC][nC];\n-        for (int i = 0; i < nR; ++i) {\n-            final double[] grad = weightedJacobian.getRow(i);\n-            final double weight = residualsWeights[i];\n-            final double residual = currentResiduals[i];\n-            // compute the normal equation\n-            final double wr = weight * residual;\n-            for (int j = 0; j < nC; ++j) {\n-                b[j] += wr * grad[j];\n-            }\n-            // build the contribution matrix for measurement i\n-            for (int k = 0; k < nC; ++k) {\n-                double[] ak = a[k];\n-                double wgk = weight * grad[k];\n-                for (int l = 0; l < nC; ++l) {\n-                    ak[l] += wgk * grad[l];\n+protected PointValuePair doOptimize() throws TooManyEvaluationsException {\n+    // -------------------- Initialization --------------------------------\n+    boolean minimize = getGoalType().equals(GoalType.MINIMIZE);\n+    FitnessFunction fitnessFunction = new FitnessFunction();\n+    double[] startPoint = getStartPoint();\n+    int dimension = startPoint.length;\n+    initializeCMA(startPoint);\n+    int iterations = 0;\n+    double[] fitnessValues = new double[lambda];\n+    double bestFitness = fitnessFunction.value(startPoint);\n+    double[] bestPoint = startPoint.clone();\n+    PointValuePair optimum = new PointValuePair(bestPoint, minimize ? bestFitness : -bestFitness);\n+    PointValuePair lastResult = null;\n+    double[] fitnessHistory = new double[10];\n+    push(fitnessHistory, bestFitness);\n+    \n+    // -------------------- Generation Loop --------------------------------\n+    while (iterations < maxIterations) {\n+        double[][] offspring = generateOffspring();\n+        fitnessValues = evaluateOffspring(offspring, fitnessFunction);\n+        double[] sortedFitnessIndices = getSortedFitnessIndices(fitnessValues, minimize);\n+        double[][] selectedOffspring = new double[mu][dimension];\n+        for (int i = 0; i < mu; i++) {\n+            selectedOffspring[i] = offspring[(int)sortedFitnessIndices[i]];\n+        }\n+        double[][] arx = selectedOffspring;\n+        RealMatrix xold = xmean;\n+        xmean = getWeightedMean(selectedOffspring);\n+        RealMatrix zmean = getWeightedMean(getSelectedOffspring(arz, sortedFitnessIndices, mu));\n+        boolean hsig = updateEvolutionPaths(zmean, xold);\n+        updateCovariance(hsig, arx, arz, sortedFitnessIndices, xold);\n+        double[] fitnessRange = fitnessRange(fitnessValues, sortedFitnessIndices);\n+        sigma *= getSigmaFactor(normps / chiN - 1, cs, damps);\n+        double[] candidateBest = selectedOffspring[0];\n+        double candidateFitness = fitnessValues[(int)sortedFitnessIndices[0]];\n+        if (candidateFitness < bestFitness) {\n+            bestFitness = candidateFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitnessFunction.repair(candidateBest), minimize ? candidateFitness : -candidateFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break;\n                 }\n             }\n         }\n-        try {\n-            // solve the linearized least squares problem\n-            RealMatrix mA = new BlockRealMatrix(a);\n-            DecompositionSolver solver = useLU ? new LUDecomposition(mA).getSolver() : new QRDecomposition(mA).getSolver();\n-            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n-            // update the estimated parameters\n-            for (int i = 0; i < nC; ++i) {\n-                currentPoint[i] += dX[i];\n+        if (stopFitness != 0 && reachedStopFitness(minimize, bestFitness)) {\n+            break;\n+        }\n+        if (reachedStopTolX(sigma)) {\n+            break;\n+        }\n+        if (reachedStopTolUpX(sigma)) {\n+            break;\n+        }\n+        if (reachedStopTolFun(fitnessHistory, fitnessRange)) {\n+            break;\n+        }\n+        if (reachedStopTolHistFun(fitnessHistory, fitnessRange)) {\n+            break;\n+        }\n+        if (covarianceCondition() > 1e7) {\n+            break;\n+        }\n+        if (getConvergenceChecker() != null) {\n+            PointValuePair current = new PointValuePair(candidateBest, minimize ? candidateFitness : -candidateFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break;\n             }\n-        } catch (SingularMatrixException e) {\n-            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n+            lastResult = current;\n         }\n-        // Check convergence.\n-        if (previous != null) {\n-            converged = checker.converged(iter, previous, current);\n-            if (converged) {\n-                setCost(computeCost(currentResiduals));\n-                return current;\n-            }\n+        sigma *= adjustStepSize(selectedOffspring, sortedFitnessIndices);\n+        push(fitnessHistory, candidateFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(candidateFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n         }\n+        iterations++;\n     }\n-    // Must never happen.\n-    throw new MathInternalError();\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java@doOptimize\n@@ -1,220 +1,83 @@\n @Override\n-protected PointVectorValuePair doOptimize() {\n-    checkParameters();\n-    // Number of observed data.\n-    final int nR = getTarget().length;\n-    final double[] currentPoint = getStartPoint();\n-    // Number of parameters.\n-    final int nC = currentPoint.length;\n-    // arrays shared with the other private methods\n-    solvedCols = FastMath.min(nR, nC);\n-    diagR = new double[nC];\n-    jacNorm = new double[nC];\n-    beta = new double[nC];\n-    permutation = new int[nC];\n-    lmDir = new double[nC];\n-    // local point\n-    double delta = 0;\n-    double xNorm = 0;\n-    double[] diag = new double[nC];\n-    double[] oldX = new double[nC];\n-    double[] oldRes = new double[nR];\n-    double[] oldObj = new double[nR];\n-    double[] qtf = new double[nR];\n-    double[] work1 = new double[nC];\n-    double[] work2 = new double[nC];\n-    double[] work3 = new double[nC];\n-    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n-    // Evaluate the function at the starting point and calculate its norm.\n-    double[] currentObjective = computeObjectiveValue(currentPoint);\n-    double[] currentResiduals = computeResiduals(currentObjective);\n-    PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n-    double currentCost = computeCost(currentResiduals);\n-    // Outer loop.\n-    lmPar = 0;\n-    boolean firstIteration = true;\n-    int iter = 0;\n-    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n-    while (true) {\n-        ++iter;\n-        final PointVectorValuePair previous = current;\n-        // QR decomposition of the jacobian matrix\n-        qrDecomposition(computeWeightedJacobian(currentPoint));\n-        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n-        for (int i = 0; i < nR; i++) {\n-            qtf[i] = weightedResidual[i];\n+protected PointValuePair doOptimize() throws TooManyEvaluationsException {\n+    // -------------------- Initialization --------------------------------\n+    boolean minimize = getGoalType().equals(GoalType.MINIMIZE);\n+    FitnessFunction fitnessFunction = new FitnessFunction();\n+    double[] startPoint = getStartPoint();\n+    int dimension = startPoint.length;\n+    initializeCMA(startPoint);\n+    int iterations = 0;\n+    double[] fitnessValues = new double[lambda];\n+    double bestFitness = fitnessFunction.value(startPoint);\n+    double[] bestPoint = startPoint.clone();\n+    PointValuePair optimum = new PointValuePair(bestPoint, minimize ? bestFitness : -bestFitness);\n+    PointValuePair lastResult = null;\n+    double[] fitnessHistory = new double[10];\n+    push(fitnessHistory, bestFitness);\n+    \n+    // -------------------- Generation Loop --------------------------------\n+    while (iterations < maxIterations) {\n+        double[][] offspring = generateOffspring();\n+        fitnessValues = evaluateOffspring(offspring, fitnessFunction);\n+        double[] sortedFitnessIndices = getSortedFitnessIndices(fitnessValues, minimize);\n+        double[][] selectedOffspring = new double[mu][dimension];\n+        for (int i = 0; i < mu; i++) {\n+            selectedOffspring[i] = offspring[(int)sortedFitnessIndices[i]];\n         }\n-        // compute Qt.res\n-        qTy(qtf);\n-        // now we don't need Q anymore,\n-        // so let jacobian contain the R matrix with its diagonal elements\n-        for (int k = 0; k < solvedCols; ++k) {\n-            int pk = permutation[k];\n-            weightedJacobian[k][pk] = diagR[pk];\n-        }\n-        if (firstIteration) {\n-            // scale the point according to the norms of the columns\n-            // of the initial jacobian\n-            xNorm = 0;\n-            for (int k = 0; k < nC; ++k) {\n-                double dk = jacNorm[k];\n-                if (dk == 0) {\n-                    dk = 1.0;\n-                }\n-                double xk = dk * currentPoint[k];\n-                xNorm += xk * xk;\n-                diag[k] = dk;\n-            }\n-            xNorm = FastMath.sqrt(xNorm);\n-            // initialize the step bound delta\n-            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n-        }\n-        // check orthogonality between function vector and jacobian columns\n-        double maxCosine = 0;\n-        if (currentCost != 0) {\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                double s = jacNorm[pj];\n-                if (s != 0) {\n-                    double sum = 0;\n-                    for (int i = 0; i <= j; ++i) {\n-                        sum += weightedJacobian[i][pj] * qtf[i];\n-                    }\n-                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n+        double[][] arx = selectedOffspring;\n+        RealMatrix xold = xmean;\n+        xmean = getWeightedMean(selectedOffspring);\n+        RealMatrix zmean = getWeightedMean(getSelectedOffspring(arz, sortedFitnessIndices, mu));\n+        boolean hsig = updateEvolutionPaths(zmean, xold);\n+        updateCovariance(hsig, arx, arz, sortedFitnessIndices, xold);\n+        double[] fitnessRange = fitnessRange(fitnessValues, sortedFitnessIndices);\n+        sigma *= getSigmaFactor(normps / chiN - 1, cs, damps);\n+        double[] candidateBest = selectedOffspring[0];\n+        double candidateFitness = fitnessValues[(int)sortedFitnessIndices[0]];\n+        if (candidateFitness < bestFitness) {\n+            bestFitness = candidateFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitnessFunction.repair(candidateBest), minimize ? candidateFitness : -candidateFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break;\n                 }\n             }\n         }\n-        if (maxCosine <= orthoTolerance) {\n-            // Convergence has been reached.\n-            setCost(currentCost);\n-            return current;\n+        if (stopFitness != 0 && reachedStopFitness(minimize, bestFitness)) {\n+            break;\n         }\n-        // rescale if necessary\n-        for (int j = 0; j < nC; ++j) {\n-            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n+        if (reachedStopTolX(sigma)) {\n+            break;\n         }\n-        // Inner loop.\n-        for (double ratio = 0; ratio < 1.0e-4; ) {\n-            // save the state\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                oldX[pj] = currentPoint[pj];\n+        if (reachedStopTolUpX(sigma)) {\n+            break;\n+        }\n+        if (reachedStopTolFun(fitnessHistory, fitnessRange)) {\n+            break;\n+        }\n+        if (reachedStopTolHistFun(fitnessHistory, fitnessRange)) {\n+            break;\n+        }\n+        if (covarianceCondition() > 1e7) {\n+            break;\n+        }\n+        if (getConvergenceChecker() != null) {\n+            PointValuePair current = new PointValuePair(candidateBest, minimize ? candidateFitness : -candidateFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break;\n             }\n-            final double previousCost = currentCost;\n-            double[] tmpVec = weightedResidual;\n-            weightedResidual = oldRes;\n-            oldRes = tmpVec;\n-            tmpVec = currentObjective;\n-            currentObjective = oldObj;\n-            oldObj = tmpVec;\n-            // determine the Levenberg-Marquardt parameter\n-            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n-            // compute the new point and the norm of the evolution direction\n-            double lmNorm = 0;\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                lmDir[pj] = -lmDir[pj];\n-                currentPoint[pj] = oldX[pj] + lmDir[pj];\n-                double s = diag[pj] * lmDir[pj];\n-                lmNorm += s * s;\n-            }\n-            lmNorm = FastMath.sqrt(lmNorm);\n-            // on the first iteration, adjust the initial step bound.\n-            if (firstIteration) {\n-                delta = FastMath.min(delta, lmNorm);\n-            }\n-            // Evaluate the function at x + p and calculate its norm.\n-            currentObjective = computeObjectiveValue(currentPoint);\n-            currentResiduals = computeResiduals(currentObjective);\n-            current = new PointVectorValuePair(currentPoint, currentObjective);\n-            currentCost = computeCost(currentResiduals);\n-            // compute the scaled actual reduction\n-            double actRed = -1.0;\n-            if (0.1 * currentCost < previousCost) {\n-                double r = currentCost / previousCost;\n-                actRed = 1.0 - r * r;\n-            }\n-            // compute the scaled predicted reduction\n-            // and the scaled directional derivative\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                double dirJ = lmDir[pj];\n-                work1[j] = 0;\n-                for (int i = 0; i <= j; ++i) {\n-                    work1[i] += weightedJacobian[i][pj] * dirJ;\n-                }\n-            }\n-            double coeff1 = 0;\n-            for (int j = 0; j < solvedCols; ++j) {\n-                coeff1 += work1[j] * work1[j];\n-            }\n-            double pc2 = previousCost * previousCost;\n-            coeff1 = coeff1 / pc2;\n-            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n-            double preRed = coeff1 + 2 * coeff2;\n-            double dirDer = -(coeff1 + coeff2);\n-            // ratio of the actual to the predicted reduction\n-            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n-            // update the step bound\n-            if (ratio <= 0.25) {\n-                double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n-                if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n-                    tmp = 0.1;\n-                }\n-                delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n-                lmPar /= tmp;\n-            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n-                delta = 2 * lmNorm;\n-                lmPar *= 0.5;\n-            }\n-            // test for successful iteration.\n-            if (ratio >= 1.0e-4) {\n-                // successful iteration, update the norm\n-                firstIteration = false;\n-                xNorm = 0;\n-                for (int k = 0; k < nC; ++k) {\n-                    double xK = diag[k] * currentPoint[k];\n-                    xNorm += xK * xK;\n-                }\n-                xNorm = FastMath.sqrt(xNorm);\n-                // tests for convergence.\n-                if (checker != null) {\n-                    // we use the vectorial convergence checker\n-                    if (checker.converged(iter, previous, current)) {\n-                        setCost(currentCost);\n-                        return current;\n-                    }\n-                }\n-            } else {\n-                // failed iteration, reset the previous values\n-                currentCost = previousCost;\n-                for (int j = 0; j < solvedCols; ++j) {\n-                    int pj = permutation[j];\n-                    currentPoint[pj] = oldX[pj];\n-                }\n-                tmpVec = weightedResidual;\n-                weightedResidual = oldRes;\n-                oldRes = tmpVec;\n-                tmpVec = currentObjective;\n-                currentObjective = oldObj;\n-                oldObj = tmpVec;\n-                // Reset \"current\" to previous values.\n-                current = new PointVectorValuePair(currentPoint, currentObjective);\n-            }\n-            // Default convergence criteria.\n-            if ((FastMath.abs(actRed) <= costRelativeTolerance && preRed <= costRelativeTolerance && ratio <= 2.0) || delta <= parRelativeTolerance * xNorm) {\n-                setCost(currentCost);\n-                return current;\n-            }\n-            // tests for termination and stringent tolerances\n-            // (2.2204e-16 is the machine epsilon for IEEE754)\n-            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE, costRelativeTolerance);\n-            } else if (delta <= 2.2204e-16 * xNorm) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE, parRelativeTolerance);\n-            } else if (maxCosine <= 2.2204e-16) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE, orthoTolerance);\n-            }\n+            lastResult = current;\n         }\n+        sigma *= adjustStepSize(selectedOffspring, sortedFitnessIndices);\n+        push(fitnessHistory, candidateFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(candidateFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n+        iterations++;\n     }\n+    return optimum;\n }\n", "finish_reason": "stop", "valid": false, "num": 1}, {"output": "@Override\nprotected PointValuePair doOptimize() {\n    // -------------------- Initialization --------------------------------\n    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n    final FitnessFunction fitfun = new FitnessFunction();\n    final double[] guess = getStartPoint();\n    // number of objective variables/problem dimension\n    dimension = guess.length;\n    initializeCMA(guess);\n    iterations = 0;\n    double bestValue = fitfun.value(guess);\n    push(fitnessHistory, bestValue);\n    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n    PointValuePair lastResult = null;\n    // -------------------- Generation Loop --------------------------------\n    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n        // Generate and evaluate lambda offspring\n        final RealMatrix arz = randn(dimension, lambda); // Fixed\n        final RealMatrix arx = zeros(dimension, lambda);\n        final double[] fitness = new double[lambda];\n        // generate random offspring\n        for (int k = 0; k < lambda; k++) {\n            RealMatrix arxk = null;\n            for (int i = 0; i < checkFeasableCount + 1; i++) {\n                if (diagonalOnly <= 0) {\n                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n                    sigma));\n                } else {\n                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                }\n                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n                    break;\n                }\n                // regenerate random arguments for row\n                arz.setColumn(k, randn(dimension));\n            }\n            copyColumn(arxk, 0, arx, k);\n            try {\n                // compute fitness\n                fitness[k] = fitfun.value(arx.getColumn(k));\n            } catch (TooManyEvaluationsException e) {\n                break generationLoop;\n            }\n        }\n        // Sort by fitness and compute weighted mean into xmean\n        final int[] arindex = sortedIndices(fitness);\n        // Calculate new xmean, this is selection and recombination\n        // for speed up of Eq. (2) and (3)\n        final RealMatrix xold = xmean;\n        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n        xmean = bestArx.multiply(weights);\n        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n        final RealMatrix zmean = bestArz.multiply(weights);\n        final boolean hsig = updateEvolutionPaths(zmean, xold);\n        if (diagonalOnly <= 0) {\n            updateCovariance(hsig, bestArx, arz, arindex, xold);\n        } else {\n            updateCovarianceDiagonalOnly(hsig, bestArz);\n        }\n        // Adapt step size sigma - Eq. (5)\n        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n        final double bestFitness = fitness[arindex[0]];\n        final double worstFitness = fitness[arindex[arindex.length - 1]];\n        if (bestValue > bestFitness) {\n            bestValue = bestFitness;\n            lastResult = optimum;\n            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n            if (getConvergenceChecker() != null && lastResult != null) {\n                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                    break generationLoop;\n                }\n            }\n        }\n        // handle termination criteria\n        // Break, if fitness is good enough\n        if (stopFitness != 0) {\n            // only if stopFitness is defined\n            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                break generationLoop;\n            }\n        }\n        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n        final double[] pcCol = pc.getColumn(0);\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                break;\n            }\n            if (i >= dimension - 1) {\n                break generationLoop;\n            }\n        }\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                break generationLoop;\n            }\n        }\n        final double historyBest = min(fitnessHistory);\n        final double historyWorst = max(fitnessHistory);\n        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n            break generationLoop;\n        }\n        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n            break generationLoop;\n        }\n        // condition number of the covariance matrix exceeds 1e14\n        if (max(diagD) / min(diagD) > 1e7) {\n            break generationLoop;\n        }\n        // user defined termination\n        if (getConvergenceChecker() != null) {\n            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n                break generationLoop;\n            }\n            lastResult = current;\n        }\n        // Adjust step size in case of equal function values (flat fitness)\n        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        // store best in history\n        push(fitnessHistory, bestFitness);\n        fitfun.setValueRange(worstFitness - bestFitness);\n        if (generateStatistics) {\n            statisticsSigmaHistory.add(sigma);\n            statisticsFitnessHistory.add(bestFitness);\n            statisticsMeanHistory.add(xmean.transpose());\n            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n        }\n    }\n    return optimum;\n}", "diff": "--- org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java@doOptimize\n@@ -1,88 +1,135 @@\n @Override\n protected PointValuePair doOptimize() {\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    final double[] point = getStartPoint();\n-    final GoalType goal = getGoalType();\n-    final int n = point.length;\n-    double[] r = computeObjectiveGradient(point);\n-    if (goal == GoalType.MINIMIZE) {\n-        for (int i = 0; i < n; i++) {\n-            r[i] = -r[i];\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn(dimension, lambda); // Fixed\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n+            }\n+        }\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n+            }\n+        }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n+            }\n+        }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n         }\n     }\n-    // Initial search direction.\n-    double[] steepestDescent = preconditioner.precondition(point, r);\n-    double[] searchDirection = steepestDescent.clone();\n-    double delta = 0;\n-    for (int i = 0; i < n; ++i) {\n-        delta += r[i] * searchDirection[i];\n-    }\n-    PointValuePair current = null;\n-    int iter = 0;\n-    int maxEval = getMaxEvaluations();\n-    while (true) {\n-        ++iter;\n-        final double objective = computeObjectiveValue(point);\n-        PointValuePair previous = current;\n-        current = new PointValuePair(point, objective);\n-        if (previous != null) {\n-            if (checker.converged(iter, previous, current)) {\n-                // We have found an optimum.\n-                return current;\n-            }\n-        }\n-        // Find the optimal step in the search direction.\n-        final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n-        final double uB = findUpperBound(lsf, 0, initialStep);\n-        // XXX Last parameters is set to a value close to zero in order to\n-        // work around the divergence problem in the \"testCircleFitting\"\n-        // unit test (see MATH-439).\n-        final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n-        // Subtract used up evaluations.\n-        maxEval -= solver.getEvaluations();\n-        // Validate new point.\n-        for (int i = 0; i < point.length; ++i) {\n-            point[i] += step * searchDirection[i];\n-        }\n-        r = computeObjectiveGradient(point);\n-        if (goal == GoalType.MINIMIZE) {\n-            for (int i = 0; i < n; ++i) {\n-                r[i] = -r[i];\n-            }\n-        }\n-        // Compute beta.\n-        final double deltaOld = delta;\n-        final double[] newSteepestDescent = preconditioner.precondition(point, r);\n-        delta = 0;\n-        for (int i = 0; i < n; ++i) {\n-            delta += r[i] * newSteepestDescent[i];\n-        }\n-        final double beta;\n-        switch(updateFormula) {\n-            case FLETCHER_REEVES:\n-                beta = delta / deltaOld;\n-                break;\n-            case POLAK_RIBIERE:\n-                double deltaMid = 0;\n-                for (int i = 0; i < r.length; ++i) {\n-                    deltaMid += r[i] * steepestDescent[i];\n-                }\n-                beta = (delta - deltaMid) / deltaOld;\n-                break;\n-            default:\n-                // Should never happen.\n-                throw new MathInternalError();\n-        }\n-        steepestDescent = newSteepestDescent;\n-        // Compute conjugate search direction.\n-        if (iter % n == 0 || beta < 0) {\n-            // Break conjugation: reset search direction.\n-            searchDirection = steepestDescent.clone();\n-        } else {\n-            // Compute new conjugate search direction.\n-            for (int i = 0; i < n; ++i) {\n-                searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n-            }\n-        }\n-    }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java@doOptimize\n@@ -15,7 +15,7 @@\n     // -------------------- Generation Loop --------------------------------\n     generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n         // Generate and evaluate lambda offspring\n-        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arz = randn(dimension, lambda); // Fixed\n         final RealMatrix arx = zeros(dimension, lambda);\n         final double[] fitness = new double[lambda];\n         // generate random offspring\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java@doOptimize\n@@ -1,79 +1,135 @@\n @Override\n protected PointValuePair doOptimize() {\n-    checkParameters();\n-    final GoalType goal = getGoalType();\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n     final double[] guess = getStartPoint();\n-    final int n = guess.length;\n-    final double[][] direc = new double[n][n];\n-    for (int i = 0; i < n; i++) {\n-        direc[i][i] = 1;\n-    }\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    double[] x = guess;\n-    double fVal = computeObjectiveValue(x);\n-    double[] x1 = x.clone();\n-    int iter = 0;\n-    while (true) {\n-        ++iter;\n-        double fX = fVal;\n-        double fX2 = 0;\n-        double delta = 0;\n-        int bigInd = 0;\n-        double alphaMin = 0;\n-        for (int i = 0; i < n; i++) {\n-            final double[] d = MathArrays.copyOf(direc[i]);\n-            fX2 = fVal;\n-            final UnivariatePointValuePair optimum = line.search(x, d);\n-            fVal = optimum.getValue();\n-            alphaMin = optimum.getPoint();\n-            final double[][] result = newPointAndDirection(x, d, alphaMin);\n-            x = result[0];\n-            if ((fX2 - fVal) > delta) {\n-                delta = fX2 - fVal;\n-                bigInd = i;\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn(dimension, lambda); // Fixed\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n             }\n         }\n-        // Default convergence check.\n-        boolean stop = 2 * (fX - fVal) <= (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) + absoluteThreshold);\n-        final PointValuePair previous = new PointValuePair(x1, fX);\n-        final PointValuePair current = new PointValuePair(x, fVal);\n-        if (!stop) {\n-            // User-defined stopping criteria.\n-            if (checker != null) {\n-                stop = checker.converged(iter, previous, current);\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n             }\n         }\n-        if (stop) {\n-            if (goal == GoalType.MINIMIZE) {\n-                return (fVal < fX) ? current : previous;\n-            } else {\n-                return (fVal > fX) ? current : previous;\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n-        final double[] d = new double[n];\n-        final double[] x2 = new double[n];\n-        for (int i = 0; i < n; i++) {\n-            d[i] = x[i] - x1[i];\n-            x2[i] = 2 * x[i] - x1[i];\n-        }\n-        x1 = x.clone();\n-        fX2 = computeObjectiveValue(x2);\n-        if (fX > fX2) {\n-            double t = 2 * (fX + fX2 - 2 * fVal);\n-            double temp = fX - fVal - delta;\n-            t *= temp * temp;\n-            temp = fX - fX2;\n-            t -= delta * temp * temp;\n-            if (t < 0.0) {\n-                final UnivariatePointValuePair optimum = line.search(x, d);\n-                fVal = optimum.getValue();\n-                alphaMin = optimum.getPoint();\n-                final double[][] result = newPointAndDirection(x, d, alphaMin);\n-                x = result[0];\n-                final int lastInd = n - 1;\n-                direc[bigInd] = direc[lastInd];\n-                direc[lastInd] = result[1];\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n             }\n         }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java@doOptimize\n@@ -1,44 +1,135 @@\n @Override\n protected PointValuePair doOptimize() {\n-    checkParameters();\n-    // Indirect call to \"computeObjectiveValue\" in order to update the\n-    // evaluations counter.\n-    final MultivariateFunction evalFunc = new MultivariateFunction() {\n-\n-        public double value(double[] point) {\n-            return computeObjectiveValue(point);\n-        }\n-    };\n-    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n-    final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() {\n-\n-        public int compare(final PointValuePair o1, final PointValuePair o2) {\n-            final double v1 = o1.getValue();\n-            final double v2 = o2.getValue();\n-            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n-        }\n-    };\n-    // Initialize search.\n-    simplex.build(getStartPoint());\n-    simplex.evaluate(evalFunc, comparator);\n-    PointValuePair[] previous = null;\n-    int iteration = 0;\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    while (true) {\n-        if (iteration > 0) {\n-            boolean converged = true;\n-            for (int i = 0; i < simplex.getSize(); i++) {\n-                PointValuePair prev = previous[i];\n-                converged = converged && checker.converged(iteration, prev, simplex.getPoint(i));\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn(dimension, lambda); // Fixed\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n             }\n-            if (converged) {\n-                // We have found an optimum.\n-                return simplex.getPoint(0);\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n             }\n         }\n-        // We still need to search.\n-        previous = simplex.getPoints();\n-        simplex.iterate(evalFunc, comparator);\n-        ++iteration;\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n+            }\n+        }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n+            }\n+        }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java@doOptimize\n@@ -1,76 +1,135 @@\n @Override\n-public PointVectorValuePair doOptimize() {\n-    checkParameters();\n-    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n-    // Computation will be useless without a checker (see \"for-loop\").\n-    if (checker == null) {\n-        throw new NullArgumentException();\n-    }\n-    final double[] targetValues = getTarget();\n-    // Number of observed data.\n-    final int nR = targetValues.length;\n-    final RealMatrix weightMatrix = getWeight();\n-    // Diagonal of the weight matrix.\n-    final double[] residualsWeights = new double[nR];\n-    for (int i = 0; i < nR; i++) {\n-        residualsWeights[i] = weightMatrix.getEntry(i, i);\n-    }\n-    final double[] currentPoint = getStartPoint();\n-    final int nC = currentPoint.length;\n-    // iterate until convergence is reached\n-    PointVectorValuePair current = null;\n-    int iter = 0;\n-    for (boolean converged = false; !converged; ) {\n-        ++iter;\n-        // evaluate the objective function and its jacobian\n-        PointVectorValuePair previous = current;\n-        // Value of the objective function at \"currentPoint\".\n-        final double[] currentObjective = computeObjectiveValue(currentPoint);\n-        final double[] currentResiduals = computeResiduals(currentObjective);\n-        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n-        current = new PointVectorValuePair(currentPoint, currentObjective);\n-        // build the linear problem\n-        final double[] b = new double[nC];\n-        final double[][] a = new double[nC][nC];\n-        for (int i = 0; i < nR; ++i) {\n-            final double[] grad = weightedJacobian.getRow(i);\n-            final double weight = residualsWeights[i];\n-            final double residual = currentResiduals[i];\n-            // compute the normal equation\n-            final double wr = weight * residual;\n-            for (int j = 0; j < nC; ++j) {\n-                b[j] += wr * grad[j];\n+protected PointValuePair doOptimize() {\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn(dimension, lambda); // Fixed\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n             }\n-            // build the contribution matrix for measurement i\n-            for (int k = 0; k < nC; ++k) {\n-                double[] ak = a[k];\n-                double wgk = weight * grad[k];\n-                for (int l = 0; l < nC; ++l) {\n-                    ak[l] += wgk * grad[l];\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n+            }\n+        }\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n                 }\n             }\n         }\n-        try {\n-            // solve the linearized least squares problem\n-            RealMatrix mA = new BlockRealMatrix(a);\n-            DecompositionSolver solver = useLU ? new LUDecomposition(mA).getSolver() : new QRDecomposition(mA).getSolver();\n-            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n-            // update the estimated parameters\n-            for (int i = 0; i < nC; ++i) {\n-                currentPoint[i] += dX[i];\n-            }\n-        } catch (SingularMatrixException e) {\n-            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n-        }\n-        // Check convergence.\n-        if (previous != null) {\n-            converged = checker.converged(iter, previous, current);\n-            if (converged) {\n-                setCost(computeCost(currentResiduals));\n-                return current;\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n-    // Must never happen.\n-    throw new MathInternalError();\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java@doOptimize\n@@ -1,220 +1,135 @@\n @Override\n-protected PointVectorValuePair doOptimize() {\n-    checkParameters();\n-    // Number of observed data.\n-    final int nR = getTarget().length;\n-    final double[] currentPoint = getStartPoint();\n-    // Number of parameters.\n-    final int nC = currentPoint.length;\n-    // arrays shared with the other private methods\n-    solvedCols = FastMath.min(nR, nC);\n-    diagR = new double[nC];\n-    jacNorm = new double[nC];\n-    beta = new double[nC];\n-    permutation = new int[nC];\n-    lmDir = new double[nC];\n-    // local point\n-    double delta = 0;\n-    double xNorm = 0;\n-    double[] diag = new double[nC];\n-    double[] oldX = new double[nC];\n-    double[] oldRes = new double[nR];\n-    double[] oldObj = new double[nR];\n-    double[] qtf = new double[nR];\n-    double[] work1 = new double[nC];\n-    double[] work2 = new double[nC];\n-    double[] work3 = new double[nC];\n-    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n-    // Evaluate the function at the starting point and calculate its norm.\n-    double[] currentObjective = computeObjectiveValue(currentPoint);\n-    double[] currentResiduals = computeResiduals(currentObjective);\n-    PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n-    double currentCost = computeCost(currentResiduals);\n-    // Outer loop.\n-    lmPar = 0;\n-    boolean firstIteration = true;\n-    int iter = 0;\n-    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n-    while (true) {\n-        ++iter;\n-        final PointVectorValuePair previous = current;\n-        // QR decomposition of the jacobian matrix\n-        qrDecomposition(computeWeightedJacobian(currentPoint));\n-        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n-        for (int i = 0; i < nR; i++) {\n-            qtf[i] = weightedResidual[i];\n+protected PointValuePair doOptimize() {\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn(dimension, lambda); // Fixed\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n+            }\n         }\n-        // compute Qt.res\n-        qTy(qtf);\n-        // now we don't need Q anymore,\n-        // so let jacobian contain the R matrix with its diagonal elements\n-        for (int k = 0; k < solvedCols; ++k) {\n-            int pk = permutation[k];\n-            weightedJacobian[k][pk] = diagR[pk];\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n         }\n-        if (firstIteration) {\n-            // scale the point according to the norms of the columns\n-            // of the initial jacobian\n-            xNorm = 0;\n-            for (int k = 0; k < nC; ++k) {\n-                double dk = jacNorm[k];\n-                if (dk == 0) {\n-                    dk = 1.0;\n-                }\n-                double xk = dk * currentPoint[k];\n-                xNorm += xk * xk;\n-                diag[k] = dk;\n-            }\n-            xNorm = FastMath.sqrt(xNorm);\n-            // initialize the step bound delta\n-            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n-        }\n-        // check orthogonality between function vector and jacobian columns\n-        double maxCosine = 0;\n-        if (currentCost != 0) {\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                double s = jacNorm[pj];\n-                if (s != 0) {\n-                    double sum = 0;\n-                    for (int i = 0; i <= j; ++i) {\n-                        sum += weightedJacobian[i][pj] * qtf[i];\n-                    }\n-                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n                 }\n             }\n         }\n-        if (maxCosine <= orthoTolerance) {\n-            // Convergence has been reached.\n-            setCost(currentCost);\n-            return current;\n-        }\n-        // rescale if necessary\n-        for (int j = 0; j < nC; ++j) {\n-            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n-        }\n-        // Inner loop.\n-        for (double ratio = 0; ratio < 1.0e-4; ) {\n-            // save the state\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                oldX[pj] = currentPoint[pj];\n-            }\n-            final double previousCost = currentCost;\n-            double[] tmpVec = weightedResidual;\n-            weightedResidual = oldRes;\n-            oldRes = tmpVec;\n-            tmpVec = currentObjective;\n-            currentObjective = oldObj;\n-            oldObj = tmpVec;\n-            // determine the Levenberg-Marquardt parameter\n-            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n-            // compute the new point and the norm of the evolution direction\n-            double lmNorm = 0;\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                lmDir[pj] = -lmDir[pj];\n-                currentPoint[pj] = oldX[pj] + lmDir[pj];\n-                double s = diag[pj] * lmDir[pj];\n-                lmNorm += s * s;\n-            }\n-            lmNorm = FastMath.sqrt(lmNorm);\n-            // on the first iteration, adjust the initial step bound.\n-            if (firstIteration) {\n-                delta = FastMath.min(delta, lmNorm);\n-            }\n-            // Evaluate the function at x + p and calculate its norm.\n-            currentObjective = computeObjectiveValue(currentPoint);\n-            currentResiduals = computeResiduals(currentObjective);\n-            current = new PointVectorValuePair(currentPoint, currentObjective);\n-            currentCost = computeCost(currentResiduals);\n-            // compute the scaled actual reduction\n-            double actRed = -1.0;\n-            if (0.1 * currentCost < previousCost) {\n-                double r = currentCost / previousCost;\n-                actRed = 1.0 - r * r;\n-            }\n-            // compute the scaled predicted reduction\n-            // and the scaled directional derivative\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                double dirJ = lmDir[pj];\n-                work1[j] = 0;\n-                for (int i = 0; i <= j; ++i) {\n-                    work1[i] += weightedJacobian[i][pj] * dirJ;\n-                }\n-            }\n-            double coeff1 = 0;\n-            for (int j = 0; j < solvedCols; ++j) {\n-                coeff1 += work1[j] * work1[j];\n-            }\n-            double pc2 = previousCost * previousCost;\n-            coeff1 = coeff1 / pc2;\n-            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n-            double preRed = coeff1 + 2 * coeff2;\n-            double dirDer = -(coeff1 + coeff2);\n-            // ratio of the actual to the predicted reduction\n-            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n-            // update the step bound\n-            if (ratio <= 0.25) {\n-                double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n-                if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n-                    tmp = 0.1;\n-                }\n-                delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n-                lmPar /= tmp;\n-            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n-                delta = 2 * lmNorm;\n-                lmPar *= 0.5;\n-            }\n-            // test for successful iteration.\n-            if (ratio >= 1.0e-4) {\n-                // successful iteration, update the norm\n-                firstIteration = false;\n-                xNorm = 0;\n-                for (int k = 0; k < nC; ++k) {\n-                    double xK = diag[k] * currentPoint[k];\n-                    xNorm += xK * xK;\n-                }\n-                xNorm = FastMath.sqrt(xNorm);\n-                // tests for convergence.\n-                if (checker != null) {\n-                    // we use the vectorial convergence checker\n-                    if (checker.converged(iter, previous, current)) {\n-                        setCost(currentCost);\n-                        return current;\n-                    }\n-                }\n-            } else {\n-                // failed iteration, reset the previous values\n-                currentCost = previousCost;\n-                for (int j = 0; j < solvedCols; ++j) {\n-                    int pj = permutation[j];\n-                    currentPoint[pj] = oldX[pj];\n-                }\n-                tmpVec = weightedResidual;\n-                weightedResidual = oldRes;\n-                oldRes = tmpVec;\n-                tmpVec = currentObjective;\n-                currentObjective = oldObj;\n-                oldObj = tmpVec;\n-                // Reset \"current\" to previous values.\n-                current = new PointVectorValuePair(currentPoint, currentObjective);\n-            }\n-            // Default convergence criteria.\n-            if ((FastMath.abs(actRed) <= costRelativeTolerance && preRed <= costRelativeTolerance && ratio <= 2.0) || delta <= parRelativeTolerance * xNorm) {\n-                setCost(currentCost);\n-                return current;\n-            }\n-            // tests for termination and stringent tolerances\n-            // (2.2204e-16 is the machine epsilon for IEEE754)\n-            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE, costRelativeTolerance);\n-            } else if (delta <= 2.2204e-16 * xNorm) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE, parRelativeTolerance);\n-            } else if (maxCosine <= 2.2204e-16) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE, orthoTolerance);\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n", "finish_reason": "stop", "valid": false, "num": 1}, {"output": "@Override\nprotected PointValuePair doOptimize() {\n    // -------------------- Initialization --------------------------------\n    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n    final FitnessFunction fitfun = new FitnessFunction();\n    final double[] guess = getStartPoint();\n    // number of objective variables/problem dimension\n    dimension = guess.length;\n    initializeCMA(guess);\n    iterations = 0;\n    double bestValue = fitfun.value(guess);\n    push(fitnessHistory, bestValue);\n    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n    PointValuePair lastResult = null;\n    // -------------------- Generation Loop --------------------------------\n    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n        // Generate and evaluate lambda offspring\n        final RealMatrix arz = randn1(dimension, lambda);\n        final RealMatrix arx = zeros(dimension, lambda);\n        final double[] fitness = new double[lambda];\n        // generate random offspring\n        for (int k = 0; k < lambda; k++) {\n            RealMatrix arxk = null;\n            for (int i = 0; i < checkFeasableCount + 1; i++) {\n                if (diagonalOnly <= 0) {\n                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n                    sigma));\n                } else {\n                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                }\n                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n                    break;\n                }\n                // regenerate random arguments for row\n                arz.setColumn(k, randn(dimension));\n            }\n            copyColumn(arxk, 0, arx, k);\n            try {\n                // compute fitness\n                fitness[k] = fitfun.value(arx.getColumn(k));\n            } catch (TooManyEvaluationsException e) {\n                break generationLoop;\n            }\n        }\n        // Sort by fitness and compute weighted mean into xmean\n        final int[] arindex = sortedIndices(fitness);\n        // Calculate new xmean, this is selection and recombination\n        // for speed up of Eq. (2) and (3)\n        final RealMatrix xold = xmean;\n        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n        xmean = bestArx.multiply(weights);\n        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n        final RealMatrix zmean = bestArz.multiply(weights);\n        final boolean hsig = updateEvolutionPaths(zmean, xold);\n        if (diagonalOnly <= 0) {\n            updateCovariance(hsig, bestArx, arz, arindex, xold);\n        } else {\n            updateCovarianceDiagonalOnly(hsig, bestArz);\n        }\n        // Adapt step size sigma - Eq. (5)\n        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n        final double bestFitness = fitness[arindex[0]];\n        final double worstFitness = fitness[arindex[arindex.length - 1]];\n        if (bestValue > bestFitness) {\n            bestValue = bestFitness;\n            lastResult = optimum;\n            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n            if (getConvergenceChecker() != null && lastResult != null) {\n                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                    break generationLoop;\n                }\n            }\n        }\n        // handle termination criteria\n        // Break, if fitness is good enough\n        if (stopFitness != 0) {\n            // only if stopFitness is defined\n            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                break generationLoop;\n            }\n        }\n        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n        final double[] pcCol = pc.getColumn(0);\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                break;\n            }\n            if (i >= dimension - 1) {\n                break generationLoop;\n            }\n        }\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                break generationLoop;\n            }\n        }\n        final double historyBest = min(fitnessHistory);\n        final double historyWorst = max(fitnessHistory);\n        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n            break generationLoop;\n        }\n        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n            break generationLoop;\n        }\n        // condition number of the covariance matrix exceeds 1e14\n        if (max(diagD) / min(diagD) > 1e7) {\n            break generationLoop;\n        }\n        // user defined termination\n        if (getConvergenceChecker() != null) {\n            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n                break generationLoop;\n            }\n            lastResult = current;\n        }\n        // Adjust step size in case of equal function values (flat fitness)\n        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        // store best in history\n        push(fitnessHistory, bestFitness);\n        fitfun.setValueRange(worstFitness - bestFitness);\n        if (generateStatistics) {\n            statisticsSigmaHistory.add(sigma);\n            statisticsFitnessHistory.add(bestFitness);\n            statisticsMeanHistory.add(xmean.transpose());\n            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n        }\n    }\n    return optimum;\n}", "diff": "--- org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java@doOptimize\n@@ -1,88 +1,135 @@\n @Override\n protected PointValuePair doOptimize() {\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    final double[] point = getStartPoint();\n-    final GoalType goal = getGoalType();\n-    final int n = point.length;\n-    double[] r = computeObjectiveGradient(point);\n-    if (goal == GoalType.MINIMIZE) {\n-        for (int i = 0; i < n; i++) {\n-            r[i] = -r[i];\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n+            }\n+        }\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n+            }\n+        }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n+            }\n+        }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n         }\n     }\n-    // Initial search direction.\n-    double[] steepestDescent = preconditioner.precondition(point, r);\n-    double[] searchDirection = steepestDescent.clone();\n-    double delta = 0;\n-    for (int i = 0; i < n; ++i) {\n-        delta += r[i] * searchDirection[i];\n-    }\n-    PointValuePair current = null;\n-    int iter = 0;\n-    int maxEval = getMaxEvaluations();\n-    while (true) {\n-        ++iter;\n-        final double objective = computeObjectiveValue(point);\n-        PointValuePair previous = current;\n-        current = new PointValuePair(point, objective);\n-        if (previous != null) {\n-            if (checker.converged(iter, previous, current)) {\n-                // We have found an optimum.\n-                return current;\n-            }\n-        }\n-        // Find the optimal step in the search direction.\n-        final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n-        final double uB = findUpperBound(lsf, 0, initialStep);\n-        // XXX Last parameters is set to a value close to zero in order to\n-        // work around the divergence problem in the \"testCircleFitting\"\n-        // unit test (see MATH-439).\n-        final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n-        // Subtract used up evaluations.\n-        maxEval -= solver.getEvaluations();\n-        // Validate new point.\n-        for (int i = 0; i < point.length; ++i) {\n-            point[i] += step * searchDirection[i];\n-        }\n-        r = computeObjectiveGradient(point);\n-        if (goal == GoalType.MINIMIZE) {\n-            for (int i = 0; i < n; ++i) {\n-                r[i] = -r[i];\n-            }\n-        }\n-        // Compute beta.\n-        final double deltaOld = delta;\n-        final double[] newSteepestDescent = preconditioner.precondition(point, r);\n-        delta = 0;\n-        for (int i = 0; i < n; ++i) {\n-            delta += r[i] * newSteepestDescent[i];\n-        }\n-        final double beta;\n-        switch(updateFormula) {\n-            case FLETCHER_REEVES:\n-                beta = delta / deltaOld;\n-                break;\n-            case POLAK_RIBIERE:\n-                double deltaMid = 0;\n-                for (int i = 0; i < r.length; ++i) {\n-                    deltaMid += r[i] * steepestDescent[i];\n-                }\n-                beta = (delta - deltaMid) / deltaOld;\n-                break;\n-            default:\n-                // Should never happen.\n-                throw new MathInternalError();\n-        }\n-        steepestDescent = newSteepestDescent;\n-        // Compute conjugate search direction.\n-        if (iter % n == 0 || beta < 0) {\n-            // Break conjugation: reset search direction.\n-            searchDirection = steepestDescent.clone();\n-        } else {\n-            // Compute new conjugate search direction.\n-            for (int i = 0; i < n; ++i) {\n-                searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n-            }\n-        }\n-    }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java@doOptimize\n@@ -47,9 +47,9 @@\n         // Calculate new xmean, this is selection and recombination\n         // for speed up of Eq. (2) and (3)\n         final RealMatrix xold = xmean;\n-        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n         xmean = bestArx.multiply(weights);\n-        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n         final RealMatrix zmean = bestArz.multiply(weights);\n         final boolean hsig = updateEvolutionPaths(zmean, xold);\n         if (diagonalOnly <= 0) {\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java@doOptimize\n@@ -1,79 +1,135 @@\n @Override\n protected PointValuePair doOptimize() {\n-    checkParameters();\n-    final GoalType goal = getGoalType();\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n     final double[] guess = getStartPoint();\n-    final int n = guess.length;\n-    final double[][] direc = new double[n][n];\n-    for (int i = 0; i < n; i++) {\n-        direc[i][i] = 1;\n-    }\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    double[] x = guess;\n-    double fVal = computeObjectiveValue(x);\n-    double[] x1 = x.clone();\n-    int iter = 0;\n-    while (true) {\n-        ++iter;\n-        double fX = fVal;\n-        double fX2 = 0;\n-        double delta = 0;\n-        int bigInd = 0;\n-        double alphaMin = 0;\n-        for (int i = 0; i < n; i++) {\n-            final double[] d = MathArrays.copyOf(direc[i]);\n-            fX2 = fVal;\n-            final UnivariatePointValuePair optimum = line.search(x, d);\n-            fVal = optimum.getValue();\n-            alphaMin = optimum.getPoint();\n-            final double[][] result = newPointAndDirection(x, d, alphaMin);\n-            x = result[0];\n-            if ((fX2 - fVal) > delta) {\n-                delta = fX2 - fVal;\n-                bigInd = i;\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n             }\n         }\n-        // Default convergence check.\n-        boolean stop = 2 * (fX - fVal) <= (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) + absoluteThreshold);\n-        final PointValuePair previous = new PointValuePair(x1, fX);\n-        final PointValuePair current = new PointValuePair(x, fVal);\n-        if (!stop) {\n-            // User-defined stopping criteria.\n-            if (checker != null) {\n-                stop = checker.converged(iter, previous, current);\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n             }\n         }\n-        if (stop) {\n-            if (goal == GoalType.MINIMIZE) {\n-                return (fVal < fX) ? current : previous;\n-            } else {\n-                return (fVal > fX) ? current : previous;\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n-        final double[] d = new double[n];\n-        final double[] x2 = new double[n];\n-        for (int i = 0; i < n; i++) {\n-            d[i] = x[i] - x1[i];\n-            x2[i] = 2 * x[i] - x1[i];\n-        }\n-        x1 = x.clone();\n-        fX2 = computeObjectiveValue(x2);\n-        if (fX > fX2) {\n-            double t = 2 * (fX + fX2 - 2 * fVal);\n-            double temp = fX - fVal - delta;\n-            t *= temp * temp;\n-            temp = fX - fX2;\n-            t -= delta * temp * temp;\n-            if (t < 0.0) {\n-                final UnivariatePointValuePair optimum = line.search(x, d);\n-                fVal = optimum.getValue();\n-                alphaMin = optimum.getPoint();\n-                final double[][] result = newPointAndDirection(x, d, alphaMin);\n-                x = result[0];\n-                final int lastInd = n - 1;\n-                direc[bigInd] = direc[lastInd];\n-                direc[lastInd] = result[1];\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n             }\n         }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java@doOptimize\n@@ -1,44 +1,135 @@\n @Override\n protected PointValuePair doOptimize() {\n-    checkParameters();\n-    // Indirect call to \"computeObjectiveValue\" in order to update the\n-    // evaluations counter.\n-    final MultivariateFunction evalFunc = new MultivariateFunction() {\n-\n-        public double value(double[] point) {\n-            return computeObjectiveValue(point);\n-        }\n-    };\n-    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n-    final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() {\n-\n-        public int compare(final PointValuePair o1, final PointValuePair o2) {\n-            final double v1 = o1.getValue();\n-            final double v2 = o2.getValue();\n-            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n-        }\n-    };\n-    // Initialize search.\n-    simplex.build(getStartPoint());\n-    simplex.evaluate(evalFunc, comparator);\n-    PointValuePair[] previous = null;\n-    int iteration = 0;\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    while (true) {\n-        if (iteration > 0) {\n-            boolean converged = true;\n-            for (int i = 0; i < simplex.getSize(); i++) {\n-                PointValuePair prev = previous[i];\n-                converged = converged && checker.converged(iteration, prev, simplex.getPoint(i));\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n             }\n-            if (converged) {\n-                // We have found an optimum.\n-                return simplex.getPoint(0);\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n             }\n         }\n-        // We still need to search.\n-        previous = simplex.getPoints();\n-        simplex.iterate(evalFunc, comparator);\n-        ++iteration;\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n+            }\n+        }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n+            }\n+        }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java@doOptimize\n@@ -1,76 +1,135 @@\n @Override\n-public PointVectorValuePair doOptimize() {\n-    checkParameters();\n-    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n-    // Computation will be useless without a checker (see \"for-loop\").\n-    if (checker == null) {\n-        throw new NullArgumentException();\n-    }\n-    final double[] targetValues = getTarget();\n-    // Number of observed data.\n-    final int nR = targetValues.length;\n-    final RealMatrix weightMatrix = getWeight();\n-    // Diagonal of the weight matrix.\n-    final double[] residualsWeights = new double[nR];\n-    for (int i = 0; i < nR; i++) {\n-        residualsWeights[i] = weightMatrix.getEntry(i, i);\n-    }\n-    final double[] currentPoint = getStartPoint();\n-    final int nC = currentPoint.length;\n-    // iterate until convergence is reached\n-    PointVectorValuePair current = null;\n-    int iter = 0;\n-    for (boolean converged = false; !converged; ) {\n-        ++iter;\n-        // evaluate the objective function and its jacobian\n-        PointVectorValuePair previous = current;\n-        // Value of the objective function at \"currentPoint\".\n-        final double[] currentObjective = computeObjectiveValue(currentPoint);\n-        final double[] currentResiduals = computeResiduals(currentObjective);\n-        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n-        current = new PointVectorValuePair(currentPoint, currentObjective);\n-        // build the linear problem\n-        final double[] b = new double[nC];\n-        final double[][] a = new double[nC][nC];\n-        for (int i = 0; i < nR; ++i) {\n-            final double[] grad = weightedJacobian.getRow(i);\n-            final double weight = residualsWeights[i];\n-            final double residual = currentResiduals[i];\n-            // compute the normal equation\n-            final double wr = weight * residual;\n-            for (int j = 0; j < nC; ++j) {\n-                b[j] += wr * grad[j];\n+protected PointValuePair doOptimize() {\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n             }\n-            // build the contribution matrix for measurement i\n-            for (int k = 0; k < nC; ++k) {\n-                double[] ak = a[k];\n-                double wgk = weight * grad[k];\n-                for (int l = 0; l < nC; ++l) {\n-                    ak[l] += wgk * grad[l];\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n+            }\n+        }\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n                 }\n             }\n         }\n-        try {\n-            // solve the linearized least squares problem\n-            RealMatrix mA = new BlockRealMatrix(a);\n-            DecompositionSolver solver = useLU ? new LUDecomposition(mA).getSolver() : new QRDecomposition(mA).getSolver();\n-            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n-            // update the estimated parameters\n-            for (int i = 0; i < nC; ++i) {\n-                currentPoint[i] += dX[i];\n-            }\n-        } catch (SingularMatrixException e) {\n-            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n-        }\n-        // Check convergence.\n-        if (previous != null) {\n-            converged = checker.converged(iter, previous, current);\n-            if (converged) {\n-                setCost(computeCost(currentResiduals));\n-                return current;\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n-    // Must never happen.\n-    throw new MathInternalError();\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java@doOptimize\n@@ -1,220 +1,135 @@\n @Override\n-protected PointVectorValuePair doOptimize() {\n-    checkParameters();\n-    // Number of observed data.\n-    final int nR = getTarget().length;\n-    final double[] currentPoint = getStartPoint();\n-    // Number of parameters.\n-    final int nC = currentPoint.length;\n-    // arrays shared with the other private methods\n-    solvedCols = FastMath.min(nR, nC);\n-    diagR = new double[nC];\n-    jacNorm = new double[nC];\n-    beta = new double[nC];\n-    permutation = new int[nC];\n-    lmDir = new double[nC];\n-    // local point\n-    double delta = 0;\n-    double xNorm = 0;\n-    double[] diag = new double[nC];\n-    double[] oldX = new double[nC];\n-    double[] oldRes = new double[nR];\n-    double[] oldObj = new double[nR];\n-    double[] qtf = new double[nR];\n-    double[] work1 = new double[nC];\n-    double[] work2 = new double[nC];\n-    double[] work3 = new double[nC];\n-    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n-    // Evaluate the function at the starting point and calculate its norm.\n-    double[] currentObjective = computeObjectiveValue(currentPoint);\n-    double[] currentResiduals = computeResiduals(currentObjective);\n-    PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n-    double currentCost = computeCost(currentResiduals);\n-    // Outer loop.\n-    lmPar = 0;\n-    boolean firstIteration = true;\n-    int iter = 0;\n-    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n-    while (true) {\n-        ++iter;\n-        final PointVectorValuePair previous = current;\n-        // QR decomposition of the jacobian matrix\n-        qrDecomposition(computeWeightedJacobian(currentPoint));\n-        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n-        for (int i = 0; i < nR; i++) {\n-            qtf[i] = weightedResidual[i];\n+protected PointValuePair doOptimize() {\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n+            }\n         }\n-        // compute Qt.res\n-        qTy(qtf);\n-        // now we don't need Q anymore,\n-        // so let jacobian contain the R matrix with its diagonal elements\n-        for (int k = 0; k < solvedCols; ++k) {\n-            int pk = permutation[k];\n-            weightedJacobian[k][pk] = diagR[pk];\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n         }\n-        if (firstIteration) {\n-            // scale the point according to the norms of the columns\n-            // of the initial jacobian\n-            xNorm = 0;\n-            for (int k = 0; k < nC; ++k) {\n-                double dk = jacNorm[k];\n-                if (dk == 0) {\n-                    dk = 1.0;\n-                }\n-                double xk = dk * currentPoint[k];\n-                xNorm += xk * xk;\n-                diag[k] = dk;\n-            }\n-            xNorm = FastMath.sqrt(xNorm);\n-            // initialize the step bound delta\n-            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n-        }\n-        // check orthogonality between function vector and jacobian columns\n-        double maxCosine = 0;\n-        if (currentCost != 0) {\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                double s = jacNorm[pj];\n-                if (s != 0) {\n-                    double sum = 0;\n-                    for (int i = 0; i <= j; ++i) {\n-                        sum += weightedJacobian[i][pj] * qtf[i];\n-                    }\n-                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n                 }\n             }\n         }\n-        if (maxCosine <= orthoTolerance) {\n-            // Convergence has been reached.\n-            setCost(currentCost);\n-            return current;\n-        }\n-        // rescale if necessary\n-        for (int j = 0; j < nC; ++j) {\n-            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n-        }\n-        // Inner loop.\n-        for (double ratio = 0; ratio < 1.0e-4; ) {\n-            // save the state\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                oldX[pj] = currentPoint[pj];\n-            }\n-            final double previousCost = currentCost;\n-            double[] tmpVec = weightedResidual;\n-            weightedResidual = oldRes;\n-            oldRes = tmpVec;\n-            tmpVec = currentObjective;\n-            currentObjective = oldObj;\n-            oldObj = tmpVec;\n-            // determine the Levenberg-Marquardt parameter\n-            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n-            // compute the new point and the norm of the evolution direction\n-            double lmNorm = 0;\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                lmDir[pj] = -lmDir[pj];\n-                currentPoint[pj] = oldX[pj] + lmDir[pj];\n-                double s = diag[pj] * lmDir[pj];\n-                lmNorm += s * s;\n-            }\n-            lmNorm = FastMath.sqrt(lmNorm);\n-            // on the first iteration, adjust the initial step bound.\n-            if (firstIteration) {\n-                delta = FastMath.min(delta, lmNorm);\n-            }\n-            // Evaluate the function at x + p and calculate its norm.\n-            currentObjective = computeObjectiveValue(currentPoint);\n-            currentResiduals = computeResiduals(currentObjective);\n-            current = new PointVectorValuePair(currentPoint, currentObjective);\n-            currentCost = computeCost(currentResiduals);\n-            // compute the scaled actual reduction\n-            double actRed = -1.0;\n-            if (0.1 * currentCost < previousCost) {\n-                double r = currentCost / previousCost;\n-                actRed = 1.0 - r * r;\n-            }\n-            // compute the scaled predicted reduction\n-            // and the scaled directional derivative\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                double dirJ = lmDir[pj];\n-                work1[j] = 0;\n-                for (int i = 0; i <= j; ++i) {\n-                    work1[i] += weightedJacobian[i][pj] * dirJ;\n-                }\n-            }\n-            double coeff1 = 0;\n-            for (int j = 0; j < solvedCols; ++j) {\n-                coeff1 += work1[j] * work1[j];\n-            }\n-            double pc2 = previousCost * previousCost;\n-            coeff1 = coeff1 / pc2;\n-            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n-            double preRed = coeff1 + 2 * coeff2;\n-            double dirDer = -(coeff1 + coeff2);\n-            // ratio of the actual to the predicted reduction\n-            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n-            // update the step bound\n-            if (ratio <= 0.25) {\n-                double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n-                if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n-                    tmp = 0.1;\n-                }\n-                delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n-                lmPar /= tmp;\n-            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n-                delta = 2 * lmNorm;\n-                lmPar *= 0.5;\n-            }\n-            // test for successful iteration.\n-            if (ratio >= 1.0e-4) {\n-                // successful iteration, update the norm\n-                firstIteration = false;\n-                xNorm = 0;\n-                for (int k = 0; k < nC; ++k) {\n-                    double xK = diag[k] * currentPoint[k];\n-                    xNorm += xK * xK;\n-                }\n-                xNorm = FastMath.sqrt(xNorm);\n-                // tests for convergence.\n-                if (checker != null) {\n-                    // we use the vectorial convergence checker\n-                    if (checker.converged(iter, previous, current)) {\n-                        setCost(currentCost);\n-                        return current;\n-                    }\n-                }\n-            } else {\n-                // failed iteration, reset the previous values\n-                currentCost = previousCost;\n-                for (int j = 0; j < solvedCols; ++j) {\n-                    int pj = permutation[j];\n-                    currentPoint[pj] = oldX[pj];\n-                }\n-                tmpVec = weightedResidual;\n-                weightedResidual = oldRes;\n-                oldRes = tmpVec;\n-                tmpVec = currentObjective;\n-                currentObjective = oldObj;\n-                oldObj = tmpVec;\n-                // Reset \"current\" to previous values.\n-                current = new PointVectorValuePair(currentPoint, currentObjective);\n-            }\n-            // Default convergence criteria.\n-            if ((FastMath.abs(actRed) <= costRelativeTolerance && preRed <= costRelativeTolerance && ratio <= 2.0) || delta <= parRelativeTolerance * xNorm) {\n-                setCost(currentCost);\n-                return current;\n-            }\n-            // tests for termination and stringent tolerances\n-            // (2.2204e-16 is the machine epsilon for IEEE754)\n-            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE, costRelativeTolerance);\n-            } else if (delta <= 2.2204e-16 * xNorm) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE, parRelativeTolerance);\n-            } else if (maxCosine <= 2.2204e-16) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE, orthoTolerance);\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n", "finish_reason": "stop", "valid": false, "num": 2}, {"output": "@Override\nprotected PointValuePair doOptimize() {\n    // -------------------- Initialization --------------------------------\n    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n    final FitnessFunction fitfun = new FitnessFunction();\n    final double[] guess = getStartPoint();\n    // number of objective variables/problem dimension\n    dimension = guess.length;\n    initializeCMA(guess);\n    iterations = 0;\n    double bestValue = fitfun.value(guess);\n    push(fitnessHistory, bestValue);\n    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n    PointValuePair lastResult = null;\n    // -------------------- Generation Loop --------------------------------\n    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n        // Generate and evaluate lambda offspring\n        final RealMatrix arz = randn1(dimension, lambda);\n        final RealMatrix arx = zeros(dimension, lambda);\n        final double[] fitness = new double[lambda];\n        // generate random offspring\n        for (int k = 0; k < lambda; k++) {\n            RealMatrix arxk = null;\n            int i = 0;\n            for (; i < checkFeasableCount + 1; i++) {\n                if (diagonalOnly <= 0) {\n                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n                    sigma));\n                } else {\n                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                }\n                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n                    break;\n                }\n                // regenerate random arguments for row\n                arz.setColumn(k, randn(dimension));\n            }\n            // Check if the feasible point is found\n            if (i > checkFeasableCount) {\n                fitness[k] = Double.POSITIVE_INFINITY;\n            } else {\n                copyColumn(arxk, 0, arx, k);\n                try {\n                    // compute fitness\n                    fitness[k] = fitfun.value(arx.getColumn(k));\n                } catch (TooManyEvaluationsException e) {\n                    break generationLoop;\n                }\n            }\n        }\n        // Sort by fitness and compute weighted mean into xmean\n        final int[] arindex = sortedIndices(fitness);\n        // Calculate new xmean, this is selection and recombination\n        // for speed up of Eq. (2) and (3)\n        final RealMatrix xold = xmean;\n        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n        xmean = bestArx.multiply(weights);\n        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n        final RealMatrix zmean = bestArz.multiply(weights);\n        final boolean hsig = updateEvolutionPaths(zmean, xold);\n        if (diagonalOnly <= 0) {\n            updateCovariance(hsig, bestArx, arz, arindex, xold);\n        } else {\n            updateCovarianceDiagonalOnly(hsig, bestArz);\n        }\n        // Adapt step size sigma - Eq. (5)\n        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n        final double bestFitness = fitness[arindex[0]];\n        final double worstFitness = fitness[arindex[arindex.length - 1]];\n        if (bestValue > bestFitness) {\n            bestValue = bestFitness;\n            lastResult = optimum;\n            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n            if (getConvergenceChecker() != null && lastResult != null) {\n                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                    break generationLoop;\n                }\n            }\n        }\n        // handle termination criteria\n        // Break, if fitness is good enough\n        if (stopFitness != 0) {\n            // only if stopFitness is defined\n            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                break generationLoop;\n            }\n        }\n        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n        final double[] pcCol = pc.getColumn(0);\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                break;\n            }\n            if (i >= dimension - 1) {\n                break generationLoop;\n            }\n        }\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                break generationLoop;\n            }\n        }\n        final double historyBest = min(fitnessHistory);\n        final double historyWorst = max(fitnessHistory);\n        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n            break generationLoop;\n        }\n        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n            break generationLoop;\n        }\n        // condition number of the covariance matrix exceeds 1e14\n        if (max(diagD) / min(diagD) > 1e7) {\n            break generationLoop;\n        }\n        // user defined termination\n        if (getConvergenceChecker() != null) {\n            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n                break generationLoop;\n            }\n            lastResult = current;\n        }\n        // Adjust step size in case of equal function values (flat fitness)\n        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        // store best in history\n        push(fitnessHistory, bestFitness);\n        fitfun.setValueRange(worstFitness - bestFitness);\n        if (generateStatistics) {\n            statisticsSigmaHistory.add(sigma);\n            statisticsFitnessHistory.add(bestFitness);\n            statisticsMeanHistory.add(xmean.transpose());\n            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n        }\n    }\n    return optimum;\n}", "diff": "--- org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java@doOptimize\n@@ -1,88 +1,141 @@\n @Override\n protected PointValuePair doOptimize() {\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    final double[] point = getStartPoint();\n-    final GoalType goal = getGoalType();\n-    final int n = point.length;\n-    double[] r = computeObjectiveGradient(point);\n-    if (goal == GoalType.MINIMIZE) {\n-        for (int i = 0; i < n; i++) {\n-            r[i] = -r[i];\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            int i = 0;\n+            for (; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            // Check if the feasible point is found\n+            if (i > checkFeasableCount) {\n+                fitness[k] = Double.POSITIVE_INFINITY;\n+            } else {\n+                copyColumn(arxk, 0, arx, k);\n+                try {\n+                    // compute fitness\n+                    fitness[k] = fitfun.value(arx.getColumn(k));\n+                } catch (TooManyEvaluationsException e) {\n+                    break generationLoop;\n+                }\n+            }\n+        }\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n+            }\n+        }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n+            }\n+        }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n         }\n     }\n-    // Initial search direction.\n-    double[] steepestDescent = preconditioner.precondition(point, r);\n-    double[] searchDirection = steepestDescent.clone();\n-    double delta = 0;\n-    for (int i = 0; i < n; ++i) {\n-        delta += r[i] * searchDirection[i];\n-    }\n-    PointValuePair current = null;\n-    int iter = 0;\n-    int maxEval = getMaxEvaluations();\n-    while (true) {\n-        ++iter;\n-        final double objective = computeObjectiveValue(point);\n-        PointValuePair previous = current;\n-        current = new PointValuePair(point, objective);\n-        if (previous != null) {\n-            if (checker.converged(iter, previous, current)) {\n-                // We have found an optimum.\n-                return current;\n-            }\n-        }\n-        // Find the optimal step in the search direction.\n-        final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n-        final double uB = findUpperBound(lsf, 0, initialStep);\n-        // XXX Last parameters is set to a value close to zero in order to\n-        // work around the divergence problem in the \"testCircleFitting\"\n-        // unit test (see MATH-439).\n-        final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n-        // Subtract used up evaluations.\n-        maxEval -= solver.getEvaluations();\n-        // Validate new point.\n-        for (int i = 0; i < point.length; ++i) {\n-            point[i] += step * searchDirection[i];\n-        }\n-        r = computeObjectiveGradient(point);\n-        if (goal == GoalType.MINIMIZE) {\n-            for (int i = 0; i < n; ++i) {\n-                r[i] = -r[i];\n-            }\n-        }\n-        // Compute beta.\n-        final double deltaOld = delta;\n-        final double[] newSteepestDescent = preconditioner.precondition(point, r);\n-        delta = 0;\n-        for (int i = 0; i < n; ++i) {\n-            delta += r[i] * newSteepestDescent[i];\n-        }\n-        final double beta;\n-        switch(updateFormula) {\n-            case FLETCHER_REEVES:\n-                beta = delta / deltaOld;\n-                break;\n-            case POLAK_RIBIERE:\n-                double deltaMid = 0;\n-                for (int i = 0; i < r.length; ++i) {\n-                    deltaMid += r[i] * steepestDescent[i];\n-                }\n-                beta = (delta - deltaMid) / deltaOld;\n-                break;\n-            default:\n-                // Should never happen.\n-                throw new MathInternalError();\n-        }\n-        steepestDescent = newSteepestDescent;\n-        // Compute conjugate search direction.\n-        if (iter % n == 0 || beta < 0) {\n-            // Break conjugation: reset search direction.\n-            searchDirection = steepestDescent.clone();\n-        } else {\n-            // Compute new conjugate search direction.\n-            for (int i = 0; i < n; ++i) {\n-                searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n-            }\n-        }\n-    }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java@doOptimize\n@@ -21,7 +21,8 @@\n         // generate random offspring\n         for (int k = 0; k < lambda; k++) {\n             RealMatrix arxk = null;\n-            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+            int i = 0;\n+            for (; i < checkFeasableCount + 1; i++) {\n                 if (diagonalOnly <= 0) {\n                     arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n                     sigma));\n@@ -34,12 +35,17 @@\n                 // regenerate random arguments for row\n                 arz.setColumn(k, randn(dimension));\n             }\n-            copyColumn(arxk, 0, arx, k);\n-            try {\n-                // compute fitness\n-                fitness[k] = fitfun.value(arx.getColumn(k));\n-            } catch (TooManyEvaluationsException e) {\n-                break generationLoop;\n+            // Check if the feasible point is found\n+            if (i > checkFeasableCount) {\n+                fitness[k] = Double.POSITIVE_INFINITY;\n+            } else {\n+                copyColumn(arxk, 0, arx, k);\n+                try {\n+                    // compute fitness\n+                    fitness[k] = fitfun.value(arx.getColumn(k));\n+                } catch (TooManyEvaluationsException e) {\n+                    break generationLoop;\n+                }\n             }\n         }\n         // Sort by fitness and compute weighted mean into xmean\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java@doOptimize\n@@ -1,79 +1,141 @@\n @Override\n protected PointValuePair doOptimize() {\n-    checkParameters();\n-    final GoalType goal = getGoalType();\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n     final double[] guess = getStartPoint();\n-    final int n = guess.length;\n-    final double[][] direc = new double[n][n];\n-    for (int i = 0; i < n; i++) {\n-        direc[i][i] = 1;\n-    }\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    double[] x = guess;\n-    double fVal = computeObjectiveValue(x);\n-    double[] x1 = x.clone();\n-    int iter = 0;\n-    while (true) {\n-        ++iter;\n-        double fX = fVal;\n-        double fX2 = 0;\n-        double delta = 0;\n-        int bigInd = 0;\n-        double alphaMin = 0;\n-        for (int i = 0; i < n; i++) {\n-            final double[] d = MathArrays.copyOf(direc[i]);\n-            fX2 = fVal;\n-            final UnivariatePointValuePair optimum = line.search(x, d);\n-            fVal = optimum.getValue();\n-            alphaMin = optimum.getPoint();\n-            final double[][] result = newPointAndDirection(x, d, alphaMin);\n-            x = result[0];\n-            if ((fX2 - fVal) > delta) {\n-                delta = fX2 - fVal;\n-                bigInd = i;\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            int i = 0;\n+            for (; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            // Check if the feasible point is found\n+            if (i > checkFeasableCount) {\n+                fitness[k] = Double.POSITIVE_INFINITY;\n+            } else {\n+                copyColumn(arxk, 0, arx, k);\n+                try {\n+                    // compute fitness\n+                    fitness[k] = fitfun.value(arx.getColumn(k));\n+                } catch (TooManyEvaluationsException e) {\n+                    break generationLoop;\n+                }\n             }\n         }\n-        // Default convergence check.\n-        boolean stop = 2 * (fX - fVal) <= (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) + absoluteThreshold);\n-        final PointValuePair previous = new PointValuePair(x1, fX);\n-        final PointValuePair current = new PointValuePair(x, fVal);\n-        if (!stop) {\n-            // User-defined stopping criteria.\n-            if (checker != null) {\n-                stop = checker.converged(iter, previous, current);\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n             }\n         }\n-        if (stop) {\n-            if (goal == GoalType.MINIMIZE) {\n-                return (fVal < fX) ? current : previous;\n-            } else {\n-                return (fVal > fX) ? current : previous;\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n-        final double[] d = new double[n];\n-        final double[] x2 = new double[n];\n-        for (int i = 0; i < n; i++) {\n-            d[i] = x[i] - x1[i];\n-            x2[i] = 2 * x[i] - x1[i];\n-        }\n-        x1 = x.clone();\n-        fX2 = computeObjectiveValue(x2);\n-        if (fX > fX2) {\n-            double t = 2 * (fX + fX2 - 2 * fVal);\n-            double temp = fX - fVal - delta;\n-            t *= temp * temp;\n-            temp = fX - fX2;\n-            t -= delta * temp * temp;\n-            if (t < 0.0) {\n-                final UnivariatePointValuePair optimum = line.search(x, d);\n-                fVal = optimum.getValue();\n-                alphaMin = optimum.getPoint();\n-                final double[][] result = newPointAndDirection(x, d, alphaMin);\n-                x = result[0];\n-                final int lastInd = n - 1;\n-                direc[bigInd] = direc[lastInd];\n-                direc[lastInd] = result[1];\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n             }\n         }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java@doOptimize\n@@ -1,44 +1,141 @@\n @Override\n protected PointValuePair doOptimize() {\n-    checkParameters();\n-    // Indirect call to \"computeObjectiveValue\" in order to update the\n-    // evaluations counter.\n-    final MultivariateFunction evalFunc = new MultivariateFunction() {\n-\n-        public double value(double[] point) {\n-            return computeObjectiveValue(point);\n-        }\n-    };\n-    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n-    final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() {\n-\n-        public int compare(final PointValuePair o1, final PointValuePair o2) {\n-            final double v1 = o1.getValue();\n-            final double v2 = o2.getValue();\n-            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n-        }\n-    };\n-    // Initialize search.\n-    simplex.build(getStartPoint());\n-    simplex.evaluate(evalFunc, comparator);\n-    PointValuePair[] previous = null;\n-    int iteration = 0;\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    while (true) {\n-        if (iteration > 0) {\n-            boolean converged = true;\n-            for (int i = 0; i < simplex.getSize(); i++) {\n-                PointValuePair prev = previous[i];\n-                converged = converged && checker.converged(iteration, prev, simplex.getPoint(i));\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            int i = 0;\n+            for (; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n             }\n-            if (converged) {\n-                // We have found an optimum.\n-                return simplex.getPoint(0);\n+            // Check if the feasible point is found\n+            if (i > checkFeasableCount) {\n+                fitness[k] = Double.POSITIVE_INFINITY;\n+            } else {\n+                copyColumn(arxk, 0, arx, k);\n+                try {\n+                    // compute fitness\n+                    fitness[k] = fitfun.value(arx.getColumn(k));\n+                } catch (TooManyEvaluationsException e) {\n+                    break generationLoop;\n+                }\n             }\n         }\n-        // We still need to search.\n-        previous = simplex.getPoints();\n-        simplex.iterate(evalFunc, comparator);\n-        ++iteration;\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n+            }\n+        }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n+            }\n+        }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java@doOptimize\n@@ -1,76 +1,141 @@\n @Override\n-public PointVectorValuePair doOptimize() {\n-    checkParameters();\n-    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n-    // Computation will be useless without a checker (see \"for-loop\").\n-    if (checker == null) {\n-        throw new NullArgumentException();\n-    }\n-    final double[] targetValues = getTarget();\n-    // Number of observed data.\n-    final int nR = targetValues.length;\n-    final RealMatrix weightMatrix = getWeight();\n-    // Diagonal of the weight matrix.\n-    final double[] residualsWeights = new double[nR];\n-    for (int i = 0; i < nR; i++) {\n-        residualsWeights[i] = weightMatrix.getEntry(i, i);\n-    }\n-    final double[] currentPoint = getStartPoint();\n-    final int nC = currentPoint.length;\n-    // iterate until convergence is reached\n-    PointVectorValuePair current = null;\n-    int iter = 0;\n-    for (boolean converged = false; !converged; ) {\n-        ++iter;\n-        // evaluate the objective function and its jacobian\n-        PointVectorValuePair previous = current;\n-        // Value of the objective function at \"currentPoint\".\n-        final double[] currentObjective = computeObjectiveValue(currentPoint);\n-        final double[] currentResiduals = computeResiduals(currentObjective);\n-        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n-        current = new PointVectorValuePair(currentPoint, currentObjective);\n-        // build the linear problem\n-        final double[] b = new double[nC];\n-        final double[][] a = new double[nC][nC];\n-        for (int i = 0; i < nR; ++i) {\n-            final double[] grad = weightedJacobian.getRow(i);\n-            final double weight = residualsWeights[i];\n-            final double residual = currentResiduals[i];\n-            // compute the normal equation\n-            final double wr = weight * residual;\n-            for (int j = 0; j < nC; ++j) {\n-                b[j] += wr * grad[j];\n+protected PointValuePair doOptimize() {\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            int i = 0;\n+            for (; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n             }\n-            // build the contribution matrix for measurement i\n-            for (int k = 0; k < nC; ++k) {\n-                double[] ak = a[k];\n-                double wgk = weight * grad[k];\n-                for (int l = 0; l < nC; ++l) {\n-                    ak[l] += wgk * grad[l];\n+            // Check if the feasible point is found\n+            if (i > checkFeasableCount) {\n+                fitness[k] = Double.POSITIVE_INFINITY;\n+            } else {\n+                copyColumn(arxk, 0, arx, k);\n+                try {\n+                    // compute fitness\n+                    fitness[k] = fitfun.value(arx.getColumn(k));\n+                } catch (TooManyEvaluationsException e) {\n+                    break generationLoop;\n                 }\n             }\n         }\n-        try {\n-            // solve the linearized least squares problem\n-            RealMatrix mA = new BlockRealMatrix(a);\n-            DecompositionSolver solver = useLU ? new LUDecomposition(mA).getSolver() : new QRDecomposition(mA).getSolver();\n-            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n-            // update the estimated parameters\n-            for (int i = 0; i < nC; ++i) {\n-                currentPoint[i] += dX[i];\n-            }\n-        } catch (SingularMatrixException e) {\n-            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n         }\n-        // Check convergence.\n-        if (previous != null) {\n-            converged = checker.converged(iter, previous, current);\n-            if (converged) {\n-                setCost(computeCost(currentResiduals));\n-                return current;\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n             }\n         }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n+            }\n+        }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n-    // Must never happen.\n-    throw new MathInternalError();\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java@doOptimize\n@@ -1,220 +1,141 @@\n @Override\n-protected PointVectorValuePair doOptimize() {\n-    checkParameters();\n-    // Number of observed data.\n-    final int nR = getTarget().length;\n-    final double[] currentPoint = getStartPoint();\n-    // Number of parameters.\n-    final int nC = currentPoint.length;\n-    // arrays shared with the other private methods\n-    solvedCols = FastMath.min(nR, nC);\n-    diagR = new double[nC];\n-    jacNorm = new double[nC];\n-    beta = new double[nC];\n-    permutation = new int[nC];\n-    lmDir = new double[nC];\n-    // local point\n-    double delta = 0;\n-    double xNorm = 0;\n-    double[] diag = new double[nC];\n-    double[] oldX = new double[nC];\n-    double[] oldRes = new double[nR];\n-    double[] oldObj = new double[nR];\n-    double[] qtf = new double[nR];\n-    double[] work1 = new double[nC];\n-    double[] work2 = new double[nC];\n-    double[] work3 = new double[nC];\n-    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n-    // Evaluate the function at the starting point and calculate its norm.\n-    double[] currentObjective = computeObjectiveValue(currentPoint);\n-    double[] currentResiduals = computeResiduals(currentObjective);\n-    PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n-    double currentCost = computeCost(currentResiduals);\n-    // Outer loop.\n-    lmPar = 0;\n-    boolean firstIteration = true;\n-    int iter = 0;\n-    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n-    while (true) {\n-        ++iter;\n-        final PointVectorValuePair previous = current;\n-        // QR decomposition of the jacobian matrix\n-        qrDecomposition(computeWeightedJacobian(currentPoint));\n-        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n-        for (int i = 0; i < nR; i++) {\n-            qtf[i] = weightedResidual[i];\n-        }\n-        // compute Qt.res\n-        qTy(qtf);\n-        // now we don't need Q anymore,\n-        // so let jacobian contain the R matrix with its diagonal elements\n-        for (int k = 0; k < solvedCols; ++k) {\n-            int pk = permutation[k];\n-            weightedJacobian[k][pk] = diagR[pk];\n-        }\n-        if (firstIteration) {\n-            // scale the point according to the norms of the columns\n-            // of the initial jacobian\n-            xNorm = 0;\n-            for (int k = 0; k < nC; ++k) {\n-                double dk = jacNorm[k];\n-                if (dk == 0) {\n-                    dk = 1.0;\n+protected PointValuePair doOptimize() {\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            int i = 0;\n+            for (; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                    sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                 }\n-                double xk = dk * currentPoint[k];\n-                xNorm += xk * xk;\n-                diag[k] = dk;\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n             }\n-            xNorm = FastMath.sqrt(xNorm);\n-            // initialize the step bound delta\n-            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n-        }\n-        // check orthogonality between function vector and jacobian columns\n-        double maxCosine = 0;\n-        if (currentCost != 0) {\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                double s = jacNorm[pj];\n-                if (s != 0) {\n-                    double sum = 0;\n-                    for (int i = 0; i <= j; ++i) {\n-                        sum += weightedJacobian[i][pj] * qtf[i];\n-                    }\n-                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n+            // Check if the feasible point is found\n+            if (i > checkFeasableCount) {\n+                fitness[k] = Double.POSITIVE_INFINITY;\n+            } else {\n+                copyColumn(arxk, 0, arx, k);\n+                try {\n+                    // compute fitness\n+                    fitness[k] = fitfun.value(arx.getColumn(k));\n+                } catch (TooManyEvaluationsException e) {\n+                    break generationLoop;\n                 }\n             }\n         }\n-        if (maxCosine <= orthoTolerance) {\n-            // Convergence has been reached.\n-            setCost(currentCost);\n-            return current;\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n         }\n-        // rescale if necessary\n-        for (int j = 0; j < nC; ++j) {\n-            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n-        }\n-        // Inner loop.\n-        for (double ratio = 0; ratio < 1.0e-4; ) {\n-            // save the state\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                oldX[pj] = currentPoint[pj];\n-            }\n-            final double previousCost = currentCost;\n-            double[] tmpVec = weightedResidual;\n-            weightedResidual = oldRes;\n-            oldRes = tmpVec;\n-            tmpVec = currentObjective;\n-            currentObjective = oldObj;\n-            oldObj = tmpVec;\n-            // determine the Levenberg-Marquardt parameter\n-            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n-            // compute the new point and the norm of the evolution direction\n-            double lmNorm = 0;\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                lmDir[pj] = -lmDir[pj];\n-                currentPoint[pj] = oldX[pj] + lmDir[pj];\n-                double s = diag[pj] * lmDir[pj];\n-                lmNorm += s * s;\n-            }\n-            lmNorm = FastMath.sqrt(lmNorm);\n-            // on the first iteration, adjust the initial step bound.\n-            if (firstIteration) {\n-                delta = FastMath.min(delta, lmNorm);\n-            }\n-            // Evaluate the function at x + p and calculate its norm.\n-            currentObjective = computeObjectiveValue(currentPoint);\n-            currentResiduals = computeResiduals(currentObjective);\n-            current = new PointVectorValuePair(currentPoint, currentObjective);\n-            currentCost = computeCost(currentResiduals);\n-            // compute the scaled actual reduction\n-            double actRed = -1.0;\n-            if (0.1 * currentCost < previousCost) {\n-                double r = currentCost / previousCost;\n-                actRed = 1.0 - r * r;\n-            }\n-            // compute the scaled predicted reduction\n-            // and the scaled directional derivative\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                double dirJ = lmDir[pj];\n-                work1[j] = 0;\n-                for (int i = 0; i <= j; ++i) {\n-                    work1[i] += weightedJacobian[i][pj] * dirJ;\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n                 }\n             }\n-            double coeff1 = 0;\n-            for (int j = 0; j < solvedCols; ++j) {\n-                coeff1 += work1[j] * work1[j];\n-            }\n-            double pc2 = previousCost * previousCost;\n-            coeff1 = coeff1 / pc2;\n-            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n-            double preRed = coeff1 + 2 * coeff2;\n-            double dirDer = -(coeff1 + coeff2);\n-            // ratio of the actual to the predicted reduction\n-            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n-            // update the step bound\n-            if (ratio <= 0.25) {\n-                double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n-                if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n-                    tmp = 0.1;\n-                }\n-                delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n-                lmPar /= tmp;\n-            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n-                delta = 2 * lmNorm;\n-                lmPar *= 0.5;\n-            }\n-            // test for successful iteration.\n-            if (ratio >= 1.0e-4) {\n-                // successful iteration, update the norm\n-                firstIteration = false;\n-                xNorm = 0;\n-                for (int k = 0; k < nC; ++k) {\n-                    double xK = diag[k] * currentPoint[k];\n-                    xNorm += xK * xK;\n-                }\n-                xNorm = FastMath.sqrt(xNorm);\n-                // tests for convergence.\n-                if (checker != null) {\n-                    // we use the vectorial convergence checker\n-                    if (checker.converged(iter, previous, current)) {\n-                        setCost(currentCost);\n-                        return current;\n-                    }\n-                }\n-            } else {\n-                // failed iteration, reset the previous values\n-                currentCost = previousCost;\n-                for (int j = 0; j < solvedCols; ++j) {\n-                    int pj = permutation[j];\n-                    currentPoint[pj] = oldX[pj];\n-                }\n-                tmpVec = weightedResidual;\n-                weightedResidual = oldRes;\n-                oldRes = tmpVec;\n-                tmpVec = currentObjective;\n-                currentObjective = oldObj;\n-                oldObj = tmpVec;\n-                // Reset \"current\" to previous values.\n-                current = new PointVectorValuePair(currentPoint, currentObjective);\n-            }\n-            // Default convergence criteria.\n-            if ((FastMath.abs(actRed) <= costRelativeTolerance && preRed <= costRelativeTolerance && ratio <= 2.0) || delta <= parRelativeTolerance * xNorm) {\n-                setCost(currentCost);\n-                return current;\n-            }\n-            // tests for termination and stringent tolerances\n-            // (2.2204e-16 is the machine epsilon for IEEE754)\n-            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE, costRelativeTolerance);\n-            } else if (delta <= 2.2204e-16 * xNorm) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE, parRelativeTolerance);\n-            } else if (maxCosine <= 2.2204e-16) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE, orthoTolerance);\n+        }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n", "finish_reason": "stop", "valid": false, "num": 1}, {"output": "@Override\nprotected PointValuePair doOptimize() {\n    // -------------------- Initialization --------------------------------\n    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n    final FitnessFunction fitfun = new FitnessFunction();\n    final double[] guess = getStartPoint();\n    // number of objective variables/problem dimension\n    dimension = guess.length;\n    initializeCMA(guess);\n    iterations = 0;\n    double bestValue = fitfun.value(guess);\n    push(fitnessHistory, bestValue);\n    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n    PointValuePair lastResult = null;\n    // -------------------- Generation Loop --------------------------------\n    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n        // Generate and evaluate lambda offspring\n        final RealMatrix arz = randn1(dimension, lambda);\n        final RealMatrix arx = zeros(dimension, lambda);\n        final double[] fitness = new double[lambda];\n        // generate random offspring\n        for (int k = 0; k < lambda; k++) {\n            RealMatrix arxk = null;\n            for (int i = 0; i < checkFeasableCount + 1; i++) {\n                if (diagonalOnly <= 0) {\n                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n                            sigma));\n                } else {\n                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                }\n                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n                    break;\n                }\n                // regenerate random arguments for row\n                arz.setColumn(k, randn(dimension));\n            }\n            copyColumn(arxk, 0, arx, k);\n            try {\n                // compute fitness\n                fitness[k] = fitfun.value(arx.getColumn(k));\n            } catch (TooManyEvaluationsException e) {\n                break generationLoop;\n            }\n        }\n        // Sort by fitness and compute weighted mean into xmean\n        final int[] arindex = sortedIndices(fitness);\n        // Calculate new xmean, this is selection and recombination\n        // for speed up of Eq. (2) and (3)\n        final RealMatrix xold = xmean;\n        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n        xmean = bestArx.multiply(weights);\n        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n        final RealMatrix zmean = bestArz.multiply(weights);\n        final boolean hsig = updateEvolutionPaths(zmean, xold);\n        if (diagonalOnly <= 0) {\n            updateCovariance(hsig, bestArx, arz, arindex, xold);\n        } else {\n            updateCovarianceDiagonalOnly(hsig, bestArz);\n        }\n        // Adapt step size sigma - Eq. (5)\n        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n        final double bestFitness = fitness[arindex[0]];\n        final double worstFitness = fitness[arindex[arindex.length - 1]];\n        if (bestValue > bestFitness) {\n            bestValue = bestFitness;\n            lastResult = optimum;\n            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n            if (getConvergenceChecker() != null && lastResult != null) {\n                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                    break generationLoop;\n                }\n            }\n        }\n        // handle termination criteria\n        // Break, if fitness is good enough\n        if (stopFitness != 0) {\n            // only if stopFitness is defined\n            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                break generationLoop;\n            }\n        }\n        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n        final double[] pcCol = pc.getColumn(0);\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n                break;\n            }\n            if (i >= dimension - 1) {\n                break generationLoop;\n            }\n        }\n        for (int i = 0; i < dimension; i++) {\n            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n                break generationLoop;\n            }\n        }\n        final double historyBest = min(fitnessHistory);\n        final double historyWorst = max(fitnessHistory);\n        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n            break generationLoop;\n        }\n        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n            break generationLoop;\n        }\n        // condition number of the covariance matrix exceeds 1e14\n        if (max(diagD) / min(diagD) > 1e7) {\n            break generationLoop;\n        }\n        // user defined termination\n        if (getConvergenceChecker() != null) {\n            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n                break generationLoop;\n            }\n            lastResult = current;\n        }\n        // Adjust step size in case of equal function values (flat fitness)\n        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n            sigma = sigma * Math.exp(0.2 + cs / damps);\n        }\n        // store best in history\n        push(fitnessHistory, bestFitness);\n        fitfun.setValueRange(worstFitness - bestFitness);\n        if (generateStatistics) {\n            statisticsSigmaHistory.add(sigma);\n            statisticsFitnessHistory.add(bestFitness);\n            statisticsMeanHistory.add(xmean.transpose());\n            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n        }\n    }\n    return optimum;\n}", "diff": "--- org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java@doOptimize\n@@ -1,88 +1,135 @@\n @Override\n protected PointValuePair doOptimize() {\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    final double[] point = getStartPoint();\n-    final GoalType goal = getGoalType();\n-    final int n = point.length;\n-    double[] r = computeObjectiveGradient(point);\n-    if (goal == GoalType.MINIMIZE) {\n-        for (int i = 0; i < n; i++) {\n-            r[i] = -r[i];\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                            sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n+            }\n+        }\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n+            }\n+        }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n+            }\n+        }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n         }\n     }\n-    // Initial search direction.\n-    double[] steepestDescent = preconditioner.precondition(point, r);\n-    double[] searchDirection = steepestDescent.clone();\n-    double delta = 0;\n-    for (int i = 0; i < n; ++i) {\n-        delta += r[i] * searchDirection[i];\n-    }\n-    PointValuePair current = null;\n-    int iter = 0;\n-    int maxEval = getMaxEvaluations();\n-    while (true) {\n-        ++iter;\n-        final double objective = computeObjectiveValue(point);\n-        PointValuePair previous = current;\n-        current = new PointValuePair(point, objective);\n-        if (previous != null) {\n-            if (checker.converged(iter, previous, current)) {\n-                // We have found an optimum.\n-                return current;\n-            }\n-        }\n-        // Find the optimal step in the search direction.\n-        final UnivariateFunction lsf = new LineSearchFunction(point, searchDirection);\n-        final double uB = findUpperBound(lsf, 0, initialStep);\n-        // XXX Last parameters is set to a value close to zero in order to\n-        // work around the divergence problem in the \"testCircleFitting\"\n-        // unit test (see MATH-439).\n-        final double step = solver.solve(maxEval, lsf, 0, uB, 1e-15);\n-        // Subtract used up evaluations.\n-        maxEval -= solver.getEvaluations();\n-        // Validate new point.\n-        for (int i = 0; i < point.length; ++i) {\n-            point[i] += step * searchDirection[i];\n-        }\n-        r = computeObjectiveGradient(point);\n-        if (goal == GoalType.MINIMIZE) {\n-            for (int i = 0; i < n; ++i) {\n-                r[i] = -r[i];\n-            }\n-        }\n-        // Compute beta.\n-        final double deltaOld = delta;\n-        final double[] newSteepestDescent = preconditioner.precondition(point, r);\n-        delta = 0;\n-        for (int i = 0; i < n; ++i) {\n-            delta += r[i] * newSteepestDescent[i];\n-        }\n-        final double beta;\n-        switch(updateFormula) {\n-            case FLETCHER_REEVES:\n-                beta = delta / deltaOld;\n-                break;\n-            case POLAK_RIBIERE:\n-                double deltaMid = 0;\n-                for (int i = 0; i < r.length; ++i) {\n-                    deltaMid += r[i] * steepestDescent[i];\n-                }\n-                beta = (delta - deltaMid) / deltaOld;\n-                break;\n-            default:\n-                // Should never happen.\n-                throw new MathInternalError();\n-        }\n-        steepestDescent = newSteepestDescent;\n-        // Compute conjugate search direction.\n-        if (iter % n == 0 || beta < 0) {\n-            // Break conjugation: reset search direction.\n-            searchDirection = steepestDescent.clone();\n-        } else {\n-            // Compute new conjugate search direction.\n-            for (int i = 0; i < n; ++i) {\n-                searchDirection[i] = steepestDescent[i] + beta * searchDirection[i];\n-            }\n-        }\n-    }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java@doOptimize\n@@ -24,7 +24,7 @@\n             for (int i = 0; i < checkFeasableCount + 1; i++) {\n                 if (diagonalOnly <= 0) {\n                     arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n-                    sigma));\n+                            sigma));\n                 } else {\n                     arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n                 }\n@@ -47,9 +47,9 @@\n         // Calculate new xmean, this is selection and recombination\n         // for speed up of Eq. (2) and (3)\n         final RealMatrix xold = xmean;\n-        final RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n         xmean = bestArx.multiply(weights);\n-        final RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n         final RealMatrix zmean = bestArz.multiply(weights);\n         final boolean hsig = updateEvolutionPaths(zmean, xold);\n         if (diagonalOnly <= 0) {\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java@doOptimize\n@@ -1,79 +1,135 @@\n @Override\n protected PointValuePair doOptimize() {\n-    checkParameters();\n-    final GoalType goal = getGoalType();\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n     final double[] guess = getStartPoint();\n-    final int n = guess.length;\n-    final double[][] direc = new double[n][n];\n-    for (int i = 0; i < n; i++) {\n-        direc[i][i] = 1;\n-    }\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    double[] x = guess;\n-    double fVal = computeObjectiveValue(x);\n-    double[] x1 = x.clone();\n-    int iter = 0;\n-    while (true) {\n-        ++iter;\n-        double fX = fVal;\n-        double fX2 = 0;\n-        double delta = 0;\n-        int bigInd = 0;\n-        double alphaMin = 0;\n-        for (int i = 0; i < n; i++) {\n-            final double[] d = MathArrays.copyOf(direc[i]);\n-            fX2 = fVal;\n-            final UnivariatePointValuePair optimum = line.search(x, d);\n-            fVal = optimum.getValue();\n-            alphaMin = optimum.getPoint();\n-            final double[][] result = newPointAndDirection(x, d, alphaMin);\n-            x = result[0];\n-            if ((fX2 - fVal) > delta) {\n-                delta = fX2 - fVal;\n-                bigInd = i;\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                            sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n             }\n         }\n-        // Default convergence check.\n-        boolean stop = 2 * (fX - fVal) <= (relativeThreshold * (FastMath.abs(fX) + FastMath.abs(fVal)) + absoluteThreshold);\n-        final PointValuePair previous = new PointValuePair(x1, fX);\n-        final PointValuePair current = new PointValuePair(x, fVal);\n-        if (!stop) {\n-            // User-defined stopping criteria.\n-            if (checker != null) {\n-                stop = checker.converged(iter, previous, current);\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n             }\n         }\n-        if (stop) {\n-            if (goal == GoalType.MINIMIZE) {\n-                return (fVal < fX) ? current : previous;\n-            } else {\n-                return (fVal > fX) ? current : previous;\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n-        final double[] d = new double[n];\n-        final double[] x2 = new double[n];\n-        for (int i = 0; i < n; i++) {\n-            d[i] = x[i] - x1[i];\n-            x2[i] = 2 * x[i] - x1[i];\n-        }\n-        x1 = x.clone();\n-        fX2 = computeObjectiveValue(x2);\n-        if (fX > fX2) {\n-            double t = 2 * (fX + fX2 - 2 * fVal);\n-            double temp = fX - fVal - delta;\n-            t *= temp * temp;\n-            temp = fX - fX2;\n-            t -= delta * temp * temp;\n-            if (t < 0.0) {\n-                final UnivariatePointValuePair optimum = line.search(x, d);\n-                fVal = optimum.getValue();\n-                alphaMin = optimum.getPoint();\n-                final double[][] result = newPointAndDirection(x, d, alphaMin);\n-                x = result[0];\n-                final int lastInd = n - 1;\n-                direc[bigInd] = direc[lastInd];\n-                direc[lastInd] = result[1];\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n             }\n         }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java@doOptimize\n@@ -1,44 +1,135 @@\n @Override\n protected PointValuePair doOptimize() {\n-    checkParameters();\n-    // Indirect call to \"computeObjectiveValue\" in order to update the\n-    // evaluations counter.\n-    final MultivariateFunction evalFunc = new MultivariateFunction() {\n-\n-        public double value(double[] point) {\n-            return computeObjectiveValue(point);\n-        }\n-    };\n-    final boolean isMinim = getGoalType() == GoalType.MINIMIZE;\n-    final Comparator<PointValuePair> comparator = new Comparator<PointValuePair>() {\n-\n-        public int compare(final PointValuePair o1, final PointValuePair o2) {\n-            final double v1 = o1.getValue();\n-            final double v2 = o2.getValue();\n-            return isMinim ? Double.compare(v1, v2) : Double.compare(v2, v1);\n-        }\n-    };\n-    // Initialize search.\n-    simplex.build(getStartPoint());\n-    simplex.evaluate(evalFunc, comparator);\n-    PointValuePair[] previous = null;\n-    int iteration = 0;\n-    final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();\n-    while (true) {\n-        if (iteration > 0) {\n-            boolean converged = true;\n-            for (int i = 0; i < simplex.getSize(); i++) {\n-                PointValuePair prev = previous[i];\n-                converged = converged && checker.converged(iteration, prev, simplex.getPoint(i));\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                            sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n             }\n-            if (converged) {\n-                // We have found an optimum.\n-                return simplex.getPoint(0);\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n             }\n         }\n-        // We still need to search.\n-        previous = simplex.getPoints();\n-        simplex.iterate(evalFunc, comparator);\n-        ++iteration;\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n+                }\n+            }\n+        }\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n+            }\n+        }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java@doOptimize\n@@ -1,76 +1,135 @@\n @Override\n-public PointVectorValuePair doOptimize() {\n-    checkParameters();\n-    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n-    // Computation will be useless without a checker (see \"for-loop\").\n-    if (checker == null) {\n-        throw new NullArgumentException();\n-    }\n-    final double[] targetValues = getTarget();\n-    // Number of observed data.\n-    final int nR = targetValues.length;\n-    final RealMatrix weightMatrix = getWeight();\n-    // Diagonal of the weight matrix.\n-    final double[] residualsWeights = new double[nR];\n-    for (int i = 0; i < nR; i++) {\n-        residualsWeights[i] = weightMatrix.getEntry(i, i);\n-    }\n-    final double[] currentPoint = getStartPoint();\n-    final int nC = currentPoint.length;\n-    // iterate until convergence is reached\n-    PointVectorValuePair current = null;\n-    int iter = 0;\n-    for (boolean converged = false; !converged; ) {\n-        ++iter;\n-        // evaluate the objective function and its jacobian\n-        PointVectorValuePair previous = current;\n-        // Value of the objective function at \"currentPoint\".\n-        final double[] currentObjective = computeObjectiveValue(currentPoint);\n-        final double[] currentResiduals = computeResiduals(currentObjective);\n-        final RealMatrix weightedJacobian = computeWeightedJacobian(currentPoint);\n-        current = new PointVectorValuePair(currentPoint, currentObjective);\n-        // build the linear problem\n-        final double[] b = new double[nC];\n-        final double[][] a = new double[nC][nC];\n-        for (int i = 0; i < nR; ++i) {\n-            final double[] grad = weightedJacobian.getRow(i);\n-            final double weight = residualsWeights[i];\n-            final double residual = currentResiduals[i];\n-            // compute the normal equation\n-            final double wr = weight * residual;\n-            for (int j = 0; j < nC; ++j) {\n-                b[j] += wr * grad[j];\n+protected PointValuePair doOptimize() {\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                            sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n             }\n-            // build the contribution matrix for measurement i\n-            for (int k = 0; k < nC; ++k) {\n-                double[] ak = a[k];\n-                double wgk = weight * grad[k];\n-                for (int l = 0; l < nC; ++l) {\n-                    ak[l] += wgk * grad[l];\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n+            }\n+        }\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n+        }\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n                 }\n             }\n         }\n-        try {\n-            // solve the linearized least squares problem\n-            RealMatrix mA = new BlockRealMatrix(a);\n-            DecompositionSolver solver = useLU ? new LUDecomposition(mA).getSolver() : new QRDecomposition(mA).getSolver();\n-            final double[] dX = solver.solve(new ArrayRealVector(b, false)).toArray();\n-            // update the estimated parameters\n-            for (int i = 0; i < nC; ++i) {\n-                currentPoint[i] += dX[i];\n-            }\n-        } catch (SingularMatrixException e) {\n-            throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);\n-        }\n-        // Check convergence.\n-        if (previous != null) {\n-            converged = checker.converged(iter, previous, current);\n-            if (converged) {\n-                setCost(computeCost(currentResiduals));\n-                return current;\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n-    // Must never happen.\n-    throw new MathInternalError();\n+    return optimum;\n }\n--- org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java@doOptimize\n+++ org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java@doOptimize\n@@ -1,220 +1,135 @@\n @Override\n-protected PointVectorValuePair doOptimize() {\n-    checkParameters();\n-    // Number of observed data.\n-    final int nR = getTarget().length;\n-    final double[] currentPoint = getStartPoint();\n-    // Number of parameters.\n-    final int nC = currentPoint.length;\n-    // arrays shared with the other private methods\n-    solvedCols = FastMath.min(nR, nC);\n-    diagR = new double[nC];\n-    jacNorm = new double[nC];\n-    beta = new double[nC];\n-    permutation = new int[nC];\n-    lmDir = new double[nC];\n-    // local point\n-    double delta = 0;\n-    double xNorm = 0;\n-    double[] diag = new double[nC];\n-    double[] oldX = new double[nC];\n-    double[] oldRes = new double[nR];\n-    double[] oldObj = new double[nR];\n-    double[] qtf = new double[nR];\n-    double[] work1 = new double[nC];\n-    double[] work2 = new double[nC];\n-    double[] work3 = new double[nC];\n-    final RealMatrix weightMatrixSqrt = getWeightSquareRoot();\n-    // Evaluate the function at the starting point and calculate its norm.\n-    double[] currentObjective = computeObjectiveValue(currentPoint);\n-    double[] currentResiduals = computeResiduals(currentObjective);\n-    PointVectorValuePair current = new PointVectorValuePair(currentPoint, currentObjective);\n-    double currentCost = computeCost(currentResiduals);\n-    // Outer loop.\n-    lmPar = 0;\n-    boolean firstIteration = true;\n-    int iter = 0;\n-    final ConvergenceChecker<PointVectorValuePair> checker = getConvergenceChecker();\n-    while (true) {\n-        ++iter;\n-        final PointVectorValuePair previous = current;\n-        // QR decomposition of the jacobian matrix\n-        qrDecomposition(computeWeightedJacobian(currentPoint));\n-        weightedResidual = weightMatrixSqrt.operate(currentResiduals);\n-        for (int i = 0; i < nR; i++) {\n-            qtf[i] = weightedResidual[i];\n+protected PointValuePair doOptimize() {\n+    // -------------------- Initialization --------------------------------\n+    isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n+    final FitnessFunction fitfun = new FitnessFunction();\n+    final double[] guess = getStartPoint();\n+    // number of objective variables/problem dimension\n+    dimension = guess.length;\n+    initializeCMA(guess);\n+    iterations = 0;\n+    double bestValue = fitfun.value(guess);\n+    push(fitnessHistory, bestValue);\n+    PointValuePair optimum = new PointValuePair(getStartPoint(), isMinimize ? bestValue : -bestValue);\n+    PointValuePair lastResult = null;\n+    // -------------------- Generation Loop --------------------------------\n+    generationLoop: for (iterations = 1; iterations <= maxIterations; iterations++) {\n+        // Generate and evaluate lambda offspring\n+        final RealMatrix arz = randn1(dimension, lambda);\n+        final RealMatrix arx = zeros(dimension, lambda);\n+        final double[] fitness = new double[lambda];\n+        // generate random offspring\n+        for (int k = 0; k < lambda; k++) {\n+            RealMatrix arxk = null;\n+            for (int i = 0; i < checkFeasableCount + 1; i++) {\n+                if (diagonalOnly <= 0) {\n+                    arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k)).scalarMultiply(// m + sig * Normal(0,C)\n+                            sigma));\n+                } else {\n+                    arxk = xmean.add(times(diagD, arz.getColumnMatrix(k)).scalarMultiply(sigma));\n+                }\n+                if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n+                    break;\n+                }\n+                // regenerate random arguments for row\n+                arz.setColumn(k, randn(dimension));\n+            }\n+            copyColumn(arxk, 0, arx, k);\n+            try {\n+                // compute fitness\n+                fitness[k] = fitfun.value(arx.getColumn(k));\n+            } catch (TooManyEvaluationsException e) {\n+                break generationLoop;\n+            }\n         }\n-        // compute Qt.res\n-        qTy(qtf);\n-        // now we don't need Q anymore,\n-        // so let jacobian contain the R matrix with its diagonal elements\n-        for (int k = 0; k < solvedCols; ++k) {\n-            int pk = permutation[k];\n-            weightedJacobian[k][pk] = diagR[pk];\n+        // Sort by fitness and compute weighted mean into xmean\n+        final int[] arindex = sortedIndices(fitness);\n+        // Calculate new xmean, this is selection and recombination\n+        // for speed up of Eq. (2) and (3)\n+        final RealMatrix xold = xmean;\n+        final RealMatrix bestArx = selectColumns(arx, Arrays.copyOf(arindex, mu));\n+        xmean = bestArx.multiply(weights);\n+        final RealMatrix bestArz = selectColumns(arz, Arrays.copyOf(arindex, mu));\n+        final RealMatrix zmean = bestArz.multiply(weights);\n+        final boolean hsig = updateEvolutionPaths(zmean, xold);\n+        if (diagonalOnly <= 0) {\n+            updateCovariance(hsig, bestArx, arz, arindex, xold);\n+        } else {\n+            updateCovarianceDiagonalOnly(hsig, bestArz);\n         }\n-        if (firstIteration) {\n-            // scale the point according to the norms of the columns\n-            // of the initial jacobian\n-            xNorm = 0;\n-            for (int k = 0; k < nC; ++k) {\n-                double dk = jacNorm[k];\n-                if (dk == 0) {\n-                    dk = 1.0;\n-                }\n-                double xk = dk * currentPoint[k];\n-                xNorm += xk * xk;\n-                diag[k] = dk;\n-            }\n-            xNorm = FastMath.sqrt(xNorm);\n-            // initialize the step bound delta\n-            delta = (xNorm == 0) ? initialStepBoundFactor : (initialStepBoundFactor * xNorm);\n-        }\n-        // check orthogonality between function vector and jacobian columns\n-        double maxCosine = 0;\n-        if (currentCost != 0) {\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                double s = jacNorm[pj];\n-                if (s != 0) {\n-                    double sum = 0;\n-                    for (int i = 0; i <= j; ++i) {\n-                        sum += weightedJacobian[i][pj] * qtf[i];\n-                    }\n-                    maxCosine = FastMath.max(maxCosine, FastMath.abs(sum) / (s * currentCost));\n+        // Adapt step size sigma - Eq. (5)\n+        sigma *= Math.exp(Math.min(1, (normps / chiN - 1) * cs / damps));\n+        final double bestFitness = fitness[arindex[0]];\n+        final double worstFitness = fitness[arindex[arindex.length - 1]];\n+        if (bestValue > bestFitness) {\n+            bestValue = bestFitness;\n+            lastResult = optimum;\n+            optimum = new PointValuePair(fitfun.repair(bestArx.getColumn(0)), isMinimize ? bestFitness : -bestFitness);\n+            if (getConvergenceChecker() != null && lastResult != null) {\n+                if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n+                    break generationLoop;\n                 }\n             }\n         }\n-        if (maxCosine <= orthoTolerance) {\n-            // Convergence has been reached.\n-            setCost(currentCost);\n-            return current;\n-        }\n-        // rescale if necessary\n-        for (int j = 0; j < nC; ++j) {\n-            diag[j] = FastMath.max(diag[j], jacNorm[j]);\n-        }\n-        // Inner loop.\n-        for (double ratio = 0; ratio < 1.0e-4; ) {\n-            // save the state\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                oldX[pj] = currentPoint[pj];\n-            }\n-            final double previousCost = currentCost;\n-            double[] tmpVec = weightedResidual;\n-            weightedResidual = oldRes;\n-            oldRes = tmpVec;\n-            tmpVec = currentObjective;\n-            currentObjective = oldObj;\n-            oldObj = tmpVec;\n-            // determine the Levenberg-Marquardt parameter\n-            determineLMParameter(qtf, delta, diag, work1, work2, work3);\n-            // compute the new point and the norm of the evolution direction\n-            double lmNorm = 0;\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                lmDir[pj] = -lmDir[pj];\n-                currentPoint[pj] = oldX[pj] + lmDir[pj];\n-                double s = diag[pj] * lmDir[pj];\n-                lmNorm += s * s;\n-            }\n-            lmNorm = FastMath.sqrt(lmNorm);\n-            // on the first iteration, adjust the initial step bound.\n-            if (firstIteration) {\n-                delta = FastMath.min(delta, lmNorm);\n-            }\n-            // Evaluate the function at x + p and calculate its norm.\n-            currentObjective = computeObjectiveValue(currentPoint);\n-            currentResiduals = computeResiduals(currentObjective);\n-            current = new PointVectorValuePair(currentPoint, currentObjective);\n-            currentCost = computeCost(currentResiduals);\n-            // compute the scaled actual reduction\n-            double actRed = -1.0;\n-            if (0.1 * currentCost < previousCost) {\n-                double r = currentCost / previousCost;\n-                actRed = 1.0 - r * r;\n-            }\n-            // compute the scaled predicted reduction\n-            // and the scaled directional derivative\n-            for (int j = 0; j < solvedCols; ++j) {\n-                int pj = permutation[j];\n-                double dirJ = lmDir[pj];\n-                work1[j] = 0;\n-                for (int i = 0; i <= j; ++i) {\n-                    work1[i] += weightedJacobian[i][pj] * dirJ;\n-                }\n-            }\n-            double coeff1 = 0;\n-            for (int j = 0; j < solvedCols; ++j) {\n-                coeff1 += work1[j] * work1[j];\n-            }\n-            double pc2 = previousCost * previousCost;\n-            coeff1 = coeff1 / pc2;\n-            double coeff2 = lmPar * lmNorm * lmNorm / pc2;\n-            double preRed = coeff1 + 2 * coeff2;\n-            double dirDer = -(coeff1 + coeff2);\n-            // ratio of the actual to the predicted reduction\n-            ratio = (preRed == 0) ? 0 : (actRed / preRed);\n-            // update the step bound\n-            if (ratio <= 0.25) {\n-                double tmp = (actRed < 0) ? (0.5 * dirDer / (dirDer + 0.5 * actRed)) : 0.5;\n-                if ((0.1 * currentCost >= previousCost) || (tmp < 0.1)) {\n-                    tmp = 0.1;\n-                }\n-                delta = tmp * FastMath.min(delta, 10.0 * lmNorm);\n-                lmPar /= tmp;\n-            } else if ((lmPar == 0) || (ratio >= 0.75)) {\n-                delta = 2 * lmNorm;\n-                lmPar *= 0.5;\n-            }\n-            // test for successful iteration.\n-            if (ratio >= 1.0e-4) {\n-                // successful iteration, update the norm\n-                firstIteration = false;\n-                xNorm = 0;\n-                for (int k = 0; k < nC; ++k) {\n-                    double xK = diag[k] * currentPoint[k];\n-                    xNorm += xK * xK;\n-                }\n-                xNorm = FastMath.sqrt(xNorm);\n-                // tests for convergence.\n-                if (checker != null) {\n-                    // we use the vectorial convergence checker\n-                    if (checker.converged(iter, previous, current)) {\n-                        setCost(currentCost);\n-                        return current;\n-                    }\n-                }\n-            } else {\n-                // failed iteration, reset the previous values\n-                currentCost = previousCost;\n-                for (int j = 0; j < solvedCols; ++j) {\n-                    int pj = permutation[j];\n-                    currentPoint[pj] = oldX[pj];\n-                }\n-                tmpVec = weightedResidual;\n-                weightedResidual = oldRes;\n-                oldRes = tmpVec;\n-                tmpVec = currentObjective;\n-                currentObjective = oldObj;\n-                oldObj = tmpVec;\n-                // Reset \"current\" to previous values.\n-                current = new PointVectorValuePair(currentPoint, currentObjective);\n-            }\n-            // Default convergence criteria.\n-            if ((FastMath.abs(actRed) <= costRelativeTolerance && preRed <= costRelativeTolerance && ratio <= 2.0) || delta <= parRelativeTolerance * xNorm) {\n-                setCost(currentCost);\n-                return current;\n-            }\n-            // tests for termination and stringent tolerances\n-            // (2.2204e-16 is the machine epsilon for IEEE754)\n-            if ((FastMath.abs(actRed) <= 2.2204e-16) && (preRed <= 2.2204e-16) && (ratio <= 2.0)) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_COST_RELATIVE_TOLERANCE, costRelativeTolerance);\n-            } else if (delta <= 2.2204e-16 * xNorm) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_PARAMETERS_RELATIVE_TOLERANCE, parRelativeTolerance);\n-            } else if (maxCosine <= 2.2204e-16) {\n-                throw new ConvergenceException(LocalizedFormats.TOO_SMALL_ORTHOGONALITY_TOLERANCE, orthoTolerance);\n+        // handle termination criteria\n+        // Break, if fitness is good enough\n+        if (stopFitness != 0) {\n+            // only if stopFitness is defined\n+            if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n+                break generationLoop;\n             }\n         }\n+        final double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n+        final double[] pcCol = pc.getColumn(0);\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * Math.max(Math.abs(pcCol[i]), sqrtDiagC[i]) > stopTolX) {\n+                break;\n+            }\n+            if (i >= dimension - 1) {\n+                break generationLoop;\n+            }\n+        }\n+        for (int i = 0; i < dimension; i++) {\n+            if (sigma * sqrtDiagC[i] > stopTolUpX) {\n+                break generationLoop;\n+            }\n+        }\n+        final double historyBest = min(fitnessHistory);\n+        final double historyWorst = max(fitnessHistory);\n+        if (iterations > 2 && Math.max(historyWorst, worstFitness) - Math.min(historyBest, bestFitness) < stopTolFun) {\n+            break generationLoop;\n+        }\n+        if (iterations > fitnessHistory.length && historyWorst - historyBest < stopTolHistFun) {\n+            break generationLoop;\n+        }\n+        // condition number of the covariance matrix exceeds 1e14\n+        if (max(diagD) / min(diagD) > 1e7) {\n+            break generationLoop;\n+        }\n+        // user defined termination\n+        if (getConvergenceChecker() != null) {\n+            final PointValuePair current = new PointValuePair(bestArx.getColumn(0), isMinimize ? bestFitness : -bestFitness);\n+            if (lastResult != null && getConvergenceChecker().converged(iterations, current, lastResult)) {\n+                break generationLoop;\n+            }\n+            lastResult = current;\n+        }\n+        // Adjust step size in case of equal function values (flat fitness)\n+        if (bestValue == fitness[arindex[(int) (0.1 + lambda / 4.)]]) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        if (iterations > 2 && Math.max(historyWorst, bestFitness) - Math.min(historyBest, bestFitness) == 0) {\n+            sigma = sigma * Math.exp(0.2 + cs / damps);\n+        }\n+        // store best in history\n+        push(fitnessHistory, bestFitness);\n+        fitfun.setValueRange(worstFitness - bestFitness);\n+        if (generateStatistics) {\n+            statisticsSigmaHistory.add(sigma);\n+            statisticsFitnessHistory.add(bestFitness);\n+            statisticsMeanHistory.add(xmean.transpose());\n+            statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n+        }\n     }\n+    return optimum;\n }\n", "finish_reason": "stop", "valid": false, "num": 1}]